{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Equilibrium Maximazation of Yield #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# make sure jupyter path is correct for loading local moudules\n",
    "import sys\n",
    "# path to steric_simulator module relative to notebook\n",
    "sys.path.append(\"../../../\")\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from KineticAssembly_AD import ReactionNetwork, VectorizedRxnNetExp, VecSim, Optimizer, EquilibriumSolver,OptimizerExp\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch import DoubleTensor as Tensor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with the AP2 complex that we've worked with before. Pairwise $\\Delta Gs$ were derived from the PDB structures via Rossetta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../input_files/ap2_dG_trap.pwr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4308\\3334798069.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbase_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'../../input_files/ap2_dG_trap.pwr'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mReactionNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mrn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AMGEN\\KineticAssembly_AD\\reaction_network.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, bngl_path, one_step, seed)\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;31m# resolve graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initial_copies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_bngl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbngl_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m  \u001b[1;31m# gradient params\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_energy_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../input_files/ap2_dG_trap.pwr'"
     ]
    }
   ],
   "source": [
    "base_input = '../../input_files/ap2_dG_trap.pwr'\n",
    "rn = ReactionNetwork(base_input, one_step=True)\n",
    "rn.resolve_tree()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rn.dG_is_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_dict = {}\n",
    "sys.path.append(\"../../\")\n",
    "import numpy as np\n",
    "from reaction_network import gtostr\n",
    "for n in rn.network.nodes():\n",
    "    #print(n)\n",
    "    #print(rn.network.nodes()[n])\n",
    "    for k,v in rn.network[n].items():\n",
    "        uid = v['uid']\n",
    "        r1 = set(gtostr(rn.network.nodes[n]['struct']))\n",
    "        p = set(gtostr(rn.network.nodes[k]['struct']))\n",
    "        r2 = p-r1\n",
    "        reactants = (r1,r2)\n",
    "        uid_dict[(n,k)] = uid\n",
    "\n",
    "print(uid_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do modifications here\n",
    "#Changing Initial Conditions\n",
    "import networkx as nx\n",
    "#Changin k_on\n",
    "new_kon = torch.zeros([rn._rxn_count], requires_grad=True).double()\n",
    "new_kon = new_kon + Tensor([2]*np.array(1e0))\n",
    "# new_kon = new_kon +torch.rand(len(new_kon))*4\n",
    "\n",
    "#Random initial condition\n",
    "#Latest\n",
    "# new_kon = new_kon + Tensor([1.0784531126282055, 1.029458105931473, 1.1269561454873085, 1.1016504619786531, 1.1962540960808816, 12.733516534843671, 13.127248934123227, 1.0685841067754172, 12.268238406610628, 11.377777346624722, 3.567418136298749, 5.936616399850786, 13.945665881026535, 13.370150974891361, 11.895765670366242, 4.970355439491927, 11.479711161807957, 2.391746953140913, 13.204855105632266, 11.736181219089554, 4.7102150063254795, 7.89012439713287])\n",
    "\n",
    "\n",
    "# new_kon = new_kon + Tensor([0.5742, 0.5904, 0.5740, 0.5455, 0.5514, 7.5604, 7.9476, 0.5958, 7.0857,\n",
    "#         6.1911, 5.7815, 4.7904, 8.7658, 8.1895, 6.7626, 3.8649, 6.3190, 1.3492,\n",
    "#         8.0216, 6.5788, 3.7168, 6.6780])\n",
    "\n",
    "# new_kon = new_kon + Tensor([3.875147930751502, 3.633157460430131, 3.07085193445574, 0.9518820841788759, 1.0494634790517816, 3.875147930751502, 3.875147930751502, 1.0627547697426531, 3.633157460430131, 3.633157460430131, 1.0627547697426531, 3.633157460430131, 3.07085193445574, 3.07085193445574, 1.0627547697426531, 3.07085193445574, 3.875147930751502, 3.875147930751502, 3.633157460430131, 1.0494634790517816, 3.875147930751502, 3.875147930751502])\n",
    "\n",
    "\n",
    "#Starting condition for HomoRates\n",
    "# rates = [0.2498580000880769, 1.5458277554618909, 1.3286185272052629]\n",
    "# rates=[5,5,5]\n",
    "# rates=[0.8469241906100582, 2.0549138907600244, 7.962836081464261]\n",
    "# rates=[0.6910176821168917, 1.893485325644041, 8.187479054602493]\n",
    "\n",
    "# rates = [0.7638575677886703, 1.2275762212163108, 1.2292556662570815] # Current\n",
    "# rates = [0.3667160530555285, 4.056637284043357, 4.197374196441568]\n",
    "# rates= [0.6090738370299691, 2.0618771938391056, 8.36260499321765]\n",
    "# rates = [0.31993640267233014, 3.439671796780782, 3.669072000528544]\n",
    "# rates = [0.7754529778993657, 2.7661023329487895, 5.638537667297305]\n",
    "# rates=[0.1668, 2.8192, 2.8560]\n",
    "rates=[0.29646059227830807, 3.8221716863515653, 3.5559205371359406]\n",
    "counter=0\n",
    "for k,v in rn.rxn_class.items():\n",
    "    for rid in v:\n",
    "        new_kon[v] = rates[counter]\n",
    "    counter+=1\n",
    "\n",
    "\n",
    "\n",
    "update_kon_dict = {}\n",
    "for edge in rn.network.edges:\n",
    "    print(rn.network.get_edge_data(edge[0],edge[1]))\n",
    "    update_kon_dict[edge] = new_kon[uid_dict[edge]]\n",
    "\n",
    "nx.set_edge_attributes(rn.network,update_kon_dict,'k_on')\n",
    "\n",
    "# for edge in rn.network.edges:\n",
    "#     print(rn.network.get_edge_data(edge[0],edge[1]))\n",
    "vec_rn = VectorizedRxnNet_Exp(rn, dev='cpu')\n",
    "print(vec_rn.kon)\n",
    "\n",
    "#Changing initial concentrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The Equilibrium Solution ##\n",
    "First we will find the equilibrium solution for this system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(rn.dG_is_param)\n",
    "print(vec_rn.dG_is_param)\n",
    "# print(rn.ddG_fluc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Using the optimizer with a 1 second simulation runtime ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn_rate=[1e-3,1e-3]\n",
    "learn_rate=1e-3\n",
    "momentum=0.2\n",
    "runtime=10\n",
    "\n",
    "files_range=[100,500,1000,5000,10000]\n",
    "yield_thresh=0.8\n",
    "yield_min=0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "vec_rn.reset(reset_params=True)\n",
    "optim = OptimizerExp(reaction_network=vec_rn,\n",
    "                  sim_runtime=runtime,\n",
    "                  optim_iterations=100,\n",
    "                  learning_rate=learn_rate,\n",
    "                  device='cpu',method=\"Adam\",reg_penalty=1000000,mom=momentum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conc based optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files_range=[50,80,100,120,150]\n",
    "# optim.rn.update_reaction_net(rn)\n",
    "# optim.optimize_wrt_conc(conc_scale=1e-1,conc_thresh=1e-1,mod_bool=True,mod_factor=10,max_thresh=1e2,max_yield=0,yield_species=14,conc_files_pref=\"ConcProfile_Time_HomoRates_\",conc_files_range=files_range,time_threshmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TIme based optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optim.rn.update_reaction_net(rn)\n",
    "# optim.optimize_wrt_time(conc_scale=1e-1,conc_thresh=1e-1,mod_bool=True,mod_factor=10,max_thresh=1e2,max_yield=0,yield_species=14,\n",
    "#                         conc_file=\"ConcProfile_Time_1000.0uM\",time_threshmin=1e-3,time_threshmax=2e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.optimizer.load_state_dict(torch.load(\"Opt_state_01\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files_range=[10,50,100,500,1000,5000]\n",
    "# files_range=[10,50,100,1000]\n",
    "\n",
    "optim.rn.update_reaction_net(rn)\n",
    "optim.optimize_wrt_conc_beta(conc_scale=1e-1,conc_thresh=1e-1,mod_bool=True,mod_factor=10,max_thresh=1e2,max_yield=0,yield_species=14,\n",
    "                        conc_files_pref=\"dG_trap/ConcProfile_Time_HomoRates_\",conc_files_range=files_range,yield_threshmin=yield_min,yield_threshmax=yield_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_state = \"Opt_state_01\"\n",
    "\n",
    "print(optim.optimizer.state_dict())\n",
    "torch.save(optim.optimizer.state_dict(),path_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # files_range=[10,50,100,500,1000,5000]\n",
    "# # files_range=[10,50,100,1000]\n",
    "\n",
    "# # optim.rn.update_reaction_net(rn)\n",
    "# optim.optimize_wrt_conc_beta(conc_scale=1e-1,conc_thresh=1e-1,mod_bool=True,mod_factor=10,max_thresh=1e2,max_yield=0,yield_species=14,\n",
    "#                         conc_files_pref=\"dG_param/ConcProfile_Time_HomoRates_\",conc_files_range=files_range,yield_threshmin=yield_min,yield_threshmax=yield_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "ax.plot(optim.mse_error)\n",
    "\n",
    "ax.tick_params(labelsize='xx-large')\n",
    "\n",
    "ax.set_xlabel(\"Iterations\",fontsize=25)\n",
    "ax.set_ylabel(\"MSE\",fontsize=25)\n",
    "\n",
    "\n",
    "# ax.legend(fontsize='large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optim.parameter_history[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yields= []\n",
    "final_params=[]\n",
    "mse_error = []\n",
    "for i in range(len(optim.final_yields)):\n",
    "    yields.append(optim.final_yields[i].item())\n",
    "#     print(optim.final_solns[i].numpy())\n",
    "    final_params.append(optim.final_solns[i].numpy())\n",
    "    mse_error.append(optim.mse_error[i])\n",
    "    \n",
    "sort_indx=np.argsort(np.array(yields))\n",
    "sorted_yields=np.array(yields)#[sort_indx]\n",
    "sorted_params = np.array(final_params)#[sort_indx]\n",
    "mse_error = np.array(mse_error)\n",
    "\n",
    "\n",
    "\n",
    "print(list(sorted_params[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(optim.mse_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yields= []\n",
    "final_params=[]\n",
    "mse_error = []\n",
    "\n",
    "for i in range(len(optim.yield_per_iter)):\n",
    "    yields.append(optim.yield_per_iter[i])\n",
    "    params=[]\n",
    "    for j in range(len(optim.parameter_history[i])):\n",
    "        params.append(np.array(optim.parameter_history[i][j]))\n",
    "    final_params.append(params)\n",
    "    mse_error.append(optim.mse_error[i])\n",
    "    \n",
    "sorted_yields=np.array(yields)#[sort_indx]\n",
    "sorted_params = np.array(final_params)#[sort_indx]\n",
    "mse_error = np.array(mse_error)\n",
    "\n",
    "print(sorted_params[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_indx = np.argmin(mse_error)\n",
    "\n",
    "min_rates = list(sorted_params[min_indx])\n",
    "min_error = mse_error[min_indx]\n",
    "\n",
    "# dG = -1*torch.log(min_rates[0][0]*vec_rn._C0/min_rates[1][0])\n",
    "print(\"Params: \",min_rates)\n",
    "# print(\"dG: \",dG)\n",
    "print(\"Min SSE: \",min_error )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing when koff is also a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Writing all solutions to a file\n",
    "\n",
    "# klabels=['k'+str(i) for i in range(len(vec_rn.kon))]\n",
    "# header = '#Yield\\t' + \"\\t\".join(klabels) + \"\\tt50\\tt85\\tt95\\n\"\n",
    "\n",
    "# files_range = [str(f) for f in files_range]\n",
    "# filestr = \",\".join(files_range)\n",
    "\n",
    "\n",
    "# with open(\"Solutions_Conc_Homorates_dGparam_07_part2\",'a') as fl:\n",
    "#     fl.write(header)\n",
    "#     fl.write(\"# Range of Concentrations: %s\\n\" %filestr)\n",
    "#     fl.write(\"# Learning rate: %s\\n\" %(\",\".join(str(lr) for lr in learn_rate)))\n",
    "#     fl.write(\"# Momentum: %f\\n\" %(momentum))\n",
    "#     fl.write(\"# Yield thresh: %f\\n\" %(yield_thresh))\n",
    "#     for i in range(len(sorted_yields)):\n",
    "#         fl.write(\"%f\" %(sorted_yields[i]))\n",
    "#         fl.write(\"\\t%f\" %(mse_error[i]))\n",
    "#         for j in range((sorted_params[i].shape[0])):\n",
    "#             for k in range(len(sorted_params[i][j])):\n",
    "#                 fl.write(\"\\t%f\" %(sorted_params[i][j][k]))\n",
    "#         fl.write(\"\\n\")\n",
    "        \n",
    "                 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing outpout when only kon is parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing all solutions to a file\n",
    "\n",
    "klabels=['k'+str(i) for i in range(len(vec_rn.kon))]\n",
    "header = '#Yield\\t' + \"\\t\".join(klabels) + \"\\tt50\\tt85\\tt95\\n\"\n",
    "\n",
    "files_range = [str(f) for f in files_range]\n",
    "filestr = \",\".join(files_range)\n",
    "\n",
    "\n",
    "with open(\"Solutions_Conc_Homorates_dGNotrap_02_part1\",'a') as fl:\n",
    "    fl.write(header)\n",
    "    fl.write(\"# Range of Concentrations: %s\\n\" %filestr)\n",
    "    fl.write(\"# Learning rate: %s\\n\" %(str(learn_rate)))\n",
    "    fl.write(\"# Momentum: %f\\n\" %(momentum))\n",
    "    fl.write(\"# Yield thresh: %f\\n\" %(yield_thresh))\n",
    "    for i in range(len(sorted_yields)):\n",
    "        fl.write(\"%f\" %(sorted_yields[i]))\n",
    "        fl.write(\"\\t%f\" %(mse_error[i]))\n",
    "        for j in range((sorted_params[i].shape[0])):\n",
    "            fl.write(\"\\t%f\" %(sorted_params[i][j]))\n",
    "        fl.write(\"\\n\")\n",
    "                 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "\n",
    "k1=[]\n",
    "for i in range(len(sorted_params)):\n",
    "    k1.append(sorted_params[i][0])\n",
    "ax.plot(k1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_dict = {}\n",
    "sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "from reaction_network import gtostr\n",
    "for n in rn.network.nodes():\n",
    "    #print(n)\n",
    "    #print(rn.network.nodes()[n])\n",
    "    for k,v in rn.network[n].items():\n",
    "        uid = v['uid']\n",
    "        r1 = set(gtostr(rn.network.nodes[n]['struct']))\n",
    "        p = set(gtostr(rn.network.nodes[k]['struct']))\n",
    "        r2 = p-r1\n",
    "        reactants = (r1,r2)\n",
    "        uid_val = {'reactants':reactants,'kon':v['k_on'],'score':v['rxn_score'],'koff':v['k_off']}\n",
    "        if uid not in uid_dict.keys():\n",
    "            uid_dict[uid] = uid_val\n",
    "    print(gtostr(rn.network.nodes[n]['struct']))\n",
    "    #for r_set in rn.get_reactant_sets(n):\n",
    "    #    print(tuple(r_set))\n",
    "    #print(rn.network[n]['struct'])\n",
    "ind_sort = np.argsort(vec_rn.kon.detach().numpy())\n",
    "for i in ind_sort:\n",
    "    print(vec_rn.kon[i])\n",
    "    print(uid_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uid_dict = {}\n",
    "sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "from reaction_network import gtostr\n",
    "from torch import DoubleTensor as Tensor\n",
    "\n",
    "def get_max_edge(n):\n",
    "    \"\"\"\n",
    "    Calculates the max rate (k_on) for a given node\n",
    "    To find out the maximum flow path to the final complex starting from the current node.\n",
    "    \n",
    "    Can also calculate the total rate of consumption of a node by summing up all rates. \n",
    "    Can tell which component is used quickly.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        edges = rn.network.out_edges(n)\n",
    "        #Loop over all edges\n",
    "        #Get attributes\n",
    "        if len(edges)==0:\n",
    "            return(False)\n",
    "        kon_max = -1\n",
    "        next_node = -1\n",
    "        \n",
    "        kon_sum = 0\n",
    "        for edge in edges:\n",
    "            data = rn.network.get_edge_data(edge[0],edge[1])\n",
    "            #print(data)\n",
    "            #Get uid\n",
    "            uid = data['uid']\n",
    "            #Get updated kon\n",
    "            temp_kon = vec_rn.kon[uid]\n",
    "            kon_sum+=temp_kon\n",
    "            \n",
    "#             #Calculate k_off also\n",
    "#             std_c = Tensor([1.])\n",
    "#             l_kon = torch.log(temp_kon)\n",
    "#             l_koff = (vec_rn.rxn_score_vec[uid] * 1. / (self._R * self._T)) + l_kon + torch.log(std_c)\n",
    "            if temp_kon > kon_max:\n",
    "                kon_max = temp_kon\n",
    "                next_node=edge[1]\n",
    "        return(kon_max,next_node,kon_sum)\n",
    "    except Exception as err:\n",
    "        raise(err)\n",
    "\n",
    "pathway = []\n",
    "kon_sumarray = []\n",
    "total_con_rate = {}\n",
    "for n in rn.network.nodes():\n",
    "    \n",
    "    n_str = gtostr(rn.network.nodes[n]['struct']) \n",
    "    \n",
    "    paths = [n_str]\n",
    "    kon_sum = 0\n",
    "    temp_node = n\n",
    "    max_edge = True\n",
    "    consumption_rate = 0\n",
    "    if n < len(rn.network.nodes()):#num_monomers:\n",
    "#         print(\"Current node: \")\n",
    "#         print(n_str)\n",
    "        while max_edge:\n",
    "            max_edge = get_max_edge(temp_node)\n",
    "            if max_edge:\n",
    "                total_con_rate[gtostr(rn.network.nodes[temp_node]['struct'])] = max_edge[2]\n",
    "                temp_node = max_edge[1]\n",
    "                kon_sum += max_edge[0].item()\n",
    "                \n",
    "#                 print(\"Next node: \")\n",
    "#                 print(temp_node)\n",
    "\n",
    "                paths.append(gtostr(rn.network.nodes[temp_node]['struct']))\n",
    "            else:\n",
    "                break\n",
    "        pathway.append(paths)\n",
    "        kon_sumarray.append(kon_sum)\n",
    "        paths=[]\n",
    "\n",
    "print(pathway)\n",
    "print(kon_sumarray)\n",
    "#print(total_con_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in sorted(total_con_rate.items(),key=lambda x : x[1]):\n",
    "    print(k,\" : \", v.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's first visualize some of the data.\n",
    "\n",
    "**Without any optimization**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nodes_list = ['A','B','S','M','AB','BMS','ABS','AMS','ABMS','AM','AS']\n",
    "#nodes_list = ['A','B','ABMS']\n",
    "optim.plot_observable(0,nodes_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**After 750 optimization iterations**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optim.plot_observable(-1,nodes_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.plot_yield()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It seems like we've found a stable solution that produces greater yield than equilibrium. This should be thermodynamically\n",
    "impossible. Let's try to find an explanation. We'll run simulations using the learned optimal parameters at a few different\n",
    "timescales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "optim_rn = optim.rn\n",
    "for i, runtime in enumerate([1, 8, 64]):\n",
    "    optim_rn.reset()\n",
    "    sim = VecSim(optim_rn, runtime, device='cpu')\n",
    "    y = sim.simulate()\n",
    "    sim.plot_observable(nodes_list,ax=ax[i],)\n",
    "    ax[i].set_title(\"runtime: \" + str(runtime) + \" seconds\")\n",
    "fig.set_size_inches(18, 6)\n",
    "node_map = {}\n",
    "for node in rn.network.nodes():\n",
    "    node_map[gtostr(rn.network.nodes[node]['struct'])] = node\n",
    "\n",
    "print(node_map)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_map = {}\n",
    "for node in rn.network.nodes():\n",
    "    node_map[gtostr(rn.network.nodes[node]['struct'])] = node\n",
    "\n",
    "print(node_map)\n",
    "def get_max_edge(n):\n",
    "    \"\"\"\n",
    "    Calculates the max rate (k_on) for a given node\n",
    "    To find out the maximum flow path to the final complex starting from the current node.\n",
    "    \n",
    "    Can also calculate the total rate of consumption of a node by summing up all rates. \n",
    "    Can tell which component is used quickly.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        edges = rn.network.out_edges(n)\n",
    "        #Loop over all edges\n",
    "        #Get attributes\n",
    "        kon_max = -1\n",
    "        next_node = -1\n",
    "\n",
    "        kon_sum = 0\n",
    "        total_flux_outedges = 0\n",
    "        total_flux_inedges = 0\n",
    "        if len(edges)==0:\n",
    "            return(False)\n",
    "            \n",
    "        for edge in edges:\n",
    "            data = rn.network.get_edge_data(edge[0],edge[1])\n",
    "            #print(data)\n",
    "            #Get uid\n",
    "            uid = data['uid']\n",
    "\n",
    "            #Get updated kon\n",
    "            temp_kon = vec_rn.kon[uid]\n",
    "            kon_sum+=temp_kon\n",
    "            \n",
    "            if temp_kon > kon_max:\n",
    "                kon_max = temp_kon\n",
    "                next_node=edge[1]\n",
    "             \n",
    "        return(kon_max,next_node,kon_sum)\n",
    "    except Exception as err:\n",
    "        raise(err)\n",
    "\n",
    "        \n",
    "def get_node_flux(n):\n",
    "    total_flux_outedges = 0\n",
    "    total_flux_inedges = 0\n",
    "    #Go over all the out edges\n",
    "    edges_out = rn.network.out_edges(n)\n",
    "    if len(edges_out)>0:\n",
    "\n",
    "        for edge in edges_out:\n",
    "            data = rn.network.get_edge_data(edge[0],edge[1])\n",
    "            #print(data)\n",
    "            #Get uid\n",
    "            uid = data['uid']\n",
    "\n",
    "            #Get updated kon\n",
    "            temp_kon = vec_rn.kon[uid]\n",
    "\n",
    "            #Calculate k_off also\n",
    "            std_c = Tensor([1.])\n",
    "            l_kon = torch.log(temp_kon)\n",
    "            l_koff = (vec_rn.rxn_score_vec[uid] * 1. / (vec_rn._R * vec_rn._T)) + l_kon + torch.log(std_c)\n",
    "            koff = torch.exp(l_koff)\n",
    "\n",
    "            #Getting conc. of reactants and products\n",
    "            #Get product\n",
    "            prod = gtostr(rn.network.nodes[edge[1]]['struct']) \n",
    "            #Get other reactant\n",
    "            react = \"\".join(sorted(list(set(prod) - set(gtostr(rn.network.nodes[edge[0]]['struct']) ))))\n",
    "\n",
    "            #Net flux from this edge = Generation - consumption\n",
    "            edge_flux = koff*vec_rn.copies_vec[edge[1]] - temp_kon*(vec_rn.copies_vec[edge[0]])*(vec_rn.copies_vec[node_map[react]])\n",
    "            #edge_flux = koff*vec_rn.copies_vec[edge[1]] \n",
    "\n",
    "            print(\"Reaction: \", gtostr(rn.network.nodes[edge[0]]['struct']), \"+\",react,\" -> \",prod)\n",
    "            print(\"Net flux: \",edge_flux)\n",
    "            print(\"kon : \",temp_kon)\n",
    "            print(\"koff: \",koff)\n",
    "            print(\"Reaction data OUTWARD: \")\n",
    "            print(data)\n",
    "\n",
    "            total_flux_outedges+=edge_flux\n",
    "    \n",
    "    #Now go over all the in edges\n",
    "    edges_in = rn.network.in_edges(n)\n",
    "    react_list = []\n",
    "    if len(edges_in) > 0:\n",
    "        for edge in edges_in:\n",
    "            if edge[0] in react_list:\n",
    "                continue\n",
    "            data = rn.network.get_edge_data(edge[0],edge[1])\n",
    "            uid = data['uid']\n",
    "\n",
    "\n",
    "            #Get generation rates; which would be kon\n",
    "            temp_kon = vec_rn.kon[uid]\n",
    "\n",
    "            #Get consumption rates; which is k_off\n",
    "            std_c = Tensor([1.])\n",
    "            l_kon = torch.log(temp_kon)\n",
    "            l_koff = (vec_rn.rxn_score_vec[uid] * 1. / (vec_rn._R * vec_rn._T)) + l_kon + torch.log(std_c)\n",
    "            koff = torch.exp(l_koff)\n",
    "\n",
    "            #Get conc. of reactants and products\n",
    "            prod = gtostr(rn.network.nodes[edge[1]]['struct'])\n",
    "            #Get other reactant\n",
    "            react = \"\".join(sorted(list(set(prod) - set(gtostr(rn.network.nodes[edge[0]]['struct']) ))))\n",
    "            react_list.append(node_map[react])\n",
    "            #Net flux from this edge = Generation - consumption\n",
    "            edge_flux_in = temp_kon*(vec_rn.copies_vec[edge[0]])*(vec_rn.copies_vec[node_map[react]])- koff*vec_rn.copies_vec[edge[1]]\n",
    "            #edge_flux_in = koff*vec_rn.copies_vec[edge[1]]\n",
    "            \n",
    "\n",
    "\n",
    "            print(\"Reaction: \", prod ,\" -> \",gtostr(rn.network.nodes[edge[0]]['struct']), \"+\",react)\n",
    "            print(\"Net flux: \",edge_flux_in)\n",
    "            print(\"kon : \",temp_kon)\n",
    "            print(\"koff: \",koff)\n",
    "            print(\"Raction data INWARD: \")\n",
    "            print(data)\n",
    "\n",
    "            total_flux_inedges+=edge_flux_in\n",
    "    net_node_flux = total_flux_outedges + total_flux_inedges\n",
    "    \n",
    "    return(net_node_flux)\n",
    "    \n",
    "pathway = []\n",
    "kon_sumarray = []\n",
    "total_con_rate = {}\n",
    "net_flux = {}\n",
    "for n in rn.network.nodes():\n",
    "    \n",
    "    n_str = gtostr(rn.network.nodes[n]['struct']) \n",
    "    \n",
    "    paths = [n_str]\n",
    "    kon_sum = 0\n",
    "    temp_node = n\n",
    "    max_edge = True\n",
    "    consumption_rate = 0\n",
    "    if n < len(rn.network.nodes()):#num_monomers:\n",
    "#         print(\"Current node: \")\n",
    "#         print(n_str)\n",
    "        while max_edge:\n",
    "            max_edge = get_max_edge(temp_node)\n",
    "            if max_edge:\n",
    "                total_con_rate[gtostr(rn.network.nodes[temp_node]['struct'])] = max_edge[2]\n",
    "                \n",
    "                temp_node = max_edge[1]\n",
    "                kon_sum += max_edge[0].item()\n",
    "                \n",
    "                \n",
    "#                 print(\"Next node: \")\n",
    "#                 print(temp_node)\n",
    "\n",
    "                paths.append(gtostr(rn.network.nodes[temp_node]['struct']))\n",
    "            else:\n",
    "                break\n",
    "        pathway.append(paths)\n",
    "        kon_sumarray.append(kon_sum)\n",
    "        paths=[]\n",
    "    print(\"-------------------------------------------------------------------------------\")\n",
    "    print(\"-------------------------------------------------------------------------------\")\n",
    "    print(\"|                                                                             |\")\n",
    "    node_flux = get_node_flux(n)\n",
    "    net_flux[gtostr(rn.network.nodes[n]['struct'])] = node_flux\n",
    "    print(\"|                                                                             |\")\n",
    "    print(\"-------------------------------------------------------------------------------\")\n",
    "    print(\"-------------------------------------------------------------------------------\")\n",
    "\n",
    "print(pathway)\n",
    "print(kon_sumarray)\n",
    "\n",
    "#print(total_con_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in sorted(net_flux.items(),key=lambda x : x[1]):\n",
    "    print(k,\" : \", v)\n",
    "\n",
    "print(vec_rn.copies_vec)\n",
    "print(vec_rn.kon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the equilibrium reached by the system still matches the equilibrium solution. We have however found a set of parameters that can increase available complete AP2 at some point before equilibrium to levels significantly higher than at equilibrium. We don't observe any trapping, but have uncovered an interesting effect. \n",
    "\n",
    "Now we'll move on to looking at ARP23. This is 7 subunits, which drastically increases the number of possible reactions. Expect longer runtimes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "bconc=Tensor([10,12,14,20,25])\n",
    "b.requires_grad=True\n",
    "a = Tensor([3,5,6,7,8,9,10])\n",
    "a.requires_grad=True\n",
    "b = Tensor([4,5,6,7,8])\n",
    "b.requires_grad=True\n",
    "\n",
    "c = a.expand(b.shape[0], a.shape[0])\n",
    "c2 = b.expand(a.shape[0], b.shape[0])\n",
    "\n",
    "d = torch.abs(c-c2.T)\n",
    "\n",
    "print(d.shape)\n",
    "\n",
    "indx = d.argmin(dim=1)\n",
    "\n",
    "e = a[indx]\n",
    "print(e)\n",
    "f = 2*e\n",
    "\n",
    "cost=loss(f,bconc)\n",
    "print(cost)\n",
    "\n",
    "cost.backward()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tensor([0])\n",
    "a.requires_grad=True\n",
    "b = Tensor([10])\n",
    "b.requires_grad=True\n",
    "\n",
    "for i in range(10):\n",
    "    a=torch.cat((a,b))\n",
    "    \n",
    "print(a)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
