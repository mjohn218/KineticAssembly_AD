{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Equilibrium Maximazation of Yield #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# make sure jupyter path is correct for loading local moudules\n",
    "import sys\n",
    "# path to steric_simulator module relative to notebook\n",
    "sys.path.append(\"../../../\")\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnergyExplorer Module is not available. Check Rosetta installation. <ipykernel.iostream.OutStream object at 0x7f49a97fae90>\n"
     ]
    }
   ],
   "source": [
    "from KineticAssembly_AD import ReactionNetwork, VectorizedRxnNet, VecSim, Optimizer, EquilibriumSolver\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch import DoubleTensor as Tensor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with the AP2 complex that we've worked with before. Pairwise $\\Delta Gs$ were derived from the PDB structures via Rossetta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['default_assoc', 1.0]\n",
      "['monomer_add_only', False]\n",
      "['chaperone', True]\n",
      "New node added - Node index: 5 ; Node label: AM \n",
      "New node added - Node index: 6 ; Node label: AB \n",
      "New node added - Node index: 7 ; Node label: AS \n",
      "Trying internal bonds\n",
      "New node added - Node index: 8 ; Node label: BM \n",
      "New node added - Node index: 9 ; Node label: MS \n",
      "New node added - Node index: 10 ; Node label: ABM \n",
      "New node added - Node index: 11 ; Node label: AMS \n",
      "Trying internal bonds\n",
      "New node added - Node index: 12 ; Node label: BS \n",
      "New node added - Node index: 13 ; Node label: ABS \n",
      "New node added - Node index: 14 ; Node label: BMS \n",
      "New node added - Node index: 15 ; Node label: ABMS \n",
      "The number of bonds formed are not compensated by the number of edges\n",
      "This could be possible due to presence of a repeating subunit\n",
      "SOurce1:  2 11\n",
      "The common reactant is:  B\n",
      "Edge added between:  2 15\n",
      "Trying internal bonds\n",
      "The number of bonds formed are not compensated by the number of edges\n",
      "This could be possible due to presence of a repeating subunit\n",
      "SOurce1:  3 10\n",
      "The common reactant is:  S\n",
      "Edge added between:  3 15\n",
      "Trying internal bonds\n",
      "*******Chaperone Reaction**********\n",
      "[4, 6] ['A', 'B', 'X']\n",
      "New node added - Node index: 16 ; Node label: ABX \n",
      "*******Chaperone Reaction**********\n",
      "[4, 10] ['A', 'B', 'M', 'X']\n",
      "New node added - Node index: 17 ; Node label: ABMX \n",
      "Trying internal bonds\n",
      "The number of bonds formed are not compensated by the number of edges\n",
      "This could be possible due to presence of a repeating subunit\n",
      "SOurce1:  5 12\n",
      "Trying internal bonds\n",
      "*******Chaperone Reaction**********\n",
      "[4, 6] ['A', 'B', 'X']\n",
      "The number of bonds formed are not compensated by the number of edges\n",
      "This could be possible due to presence of a repeating subunit\n",
      "SOurce1:  6 9\n",
      "Trying internal bonds\n",
      "The number of bonds formed are not compensated by the number of edges\n",
      "This could be possible due to presence of a repeating subunit\n",
      "SOurce1:  7 8\n",
      "Trying internal bonds\n",
      "Trying internal bonds\n",
      "Trying internal bonds\n",
      "*******Chaperone Reaction**********\n",
      "[4, 10] ['A', 'B', 'M', 'X']\n",
      "Trying internal bonds\n",
      "Trying internal bonds\n",
      "Trying internal bonds\n",
      "The number of bonds formed are not compensated by the number of edges\n",
      "This could be possible due to presence of a repeating subunit\n",
      "SOurce1:  13 1\n",
      "The common reactant is:  M\n",
      "Edge added between:  1 15\n",
      "Trying internal bonds\n",
      "The number of bonds formed are not compensated by the number of edges\n",
      "This could be possible due to presence of a repeating subunit\n",
      "SOurce1:  14 0\n",
      "The common reactant is:  A\n",
      "Edge added between:  0 15\n",
      "Trying internal bonds\n",
      "Trying internal bonds\n",
      "Trying internal bonds\n",
      "Trying internal bonds\n",
      "Resolving Chaperone Rxns::\n",
      "[([4, 6], ['A', 'B', 'X']), ([4, 10], ['A', 'B', 'M', 'X'])]\n",
      "Reaction Network Completed\n"
     ]
    }
   ],
   "source": [
    "base_input = '../input_files/tetramer_chaperone.pwr'\n",
    "rn = ReactionNetwork(base_input, one_step=True)\n",
    "rn.resolve_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(1, 1): [0, 1, 2, 3, 4, 7], (2, 1): [5, 6, 8, 9, 10, 12, 13, 14, 16, 21, 22, 23, 24], (3, 1): [11, 15, 17, 25, 26], (2, 2): [18, 19, 20]}\n",
      "{4: [27, 16, 28, 17]}\n"
     ]
    }
   ],
   "source": [
    "print(rn.rxn_class)\n",
    "print(rn.chap_uid_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -- A\n",
      "1 -- M\n",
      "2 -- B\n",
      "3 -- S\n",
      "4 -- X\n",
      "5 -- AM\n",
      "6 -- AB\n",
      "7 -- AS\n",
      "8 -- BM\n",
      "9 -- MS\n",
      "10 -- ABM\n",
      "11 -- AMS\n",
      "12 -- BS\n",
      "13 -- ABS\n",
      "14 -- BMS\n",
      "15 -- ABMS\n",
      "16 -- ABX\n",
      "17 -- ABMX\n",
      "{(0, 5): 0, (0, 6): 1, (0, 7): 2, (0, 10): 21, (0, 11): 22, (0, 13): 23, (0, 15): 26, (1, 5): 0, (1, 8): 3, (1, 9): 4, (1, 10): 5, (1, 11): 6, (1, 14): 24, (1, 15): 25, (2, 6): 1, (2, 8): 3, (2, 12): 7, (2, 10): 8, (2, 13): 9, (2, 14): 10, (2, 15): 11, (3, 7): 2, (3, 9): 4, (3, 12): 7, (3, 11): 12, (3, 13): 13, (3, 14): 14, (3, 15): 15, (4, 16): 16, (4, 17): 17, (5, 10): 8, (5, 11): 12, (5, 15): 18, (6, 10): 5, (6, 13): 13, (6, 16): 16, (6, 15): 19, (7, 11): 6, (7, 13): 9, (7, 15): 20, (8, 14): 14, (8, 15): 20, (8, 10): 21, (9, 14): 10, (9, 15): 19, (9, 11): 22, (10, 15): 15, (10, 17): 17, (11, 15): 11, (12, 15): 18, (12, 13): 23, (12, 14): 24, (13, 15): 25, (14, 15): 26, (16, 0): 27, (16, 2): 27, (16, 4): 27, (17, 0): 28, (17, 1): 28, (17, 2): 28, (17, 4): 28}\n"
     ]
    }
   ],
   "source": [
    "uid_dict = {}\n",
    "sys.path.append(\"../../\")\n",
    "import numpy as np\n",
    "from reaction_network import gtostr\n",
    "for n in rn.network.nodes():\n",
    "    #print(n)\n",
    "    #print(rn.network.nodes()[n])\n",
    "    print(n,\"--\",gtostr(rn.network.nodes[n]['struct']))\n",
    "    for k,v in rn.network[n].items():\n",
    "        uid = v['uid']\n",
    "        r1 = set(gtostr(rn.network.nodes[n]['struct']))\n",
    "        p = set(gtostr(rn.network.nodes[k]['struct']))\n",
    "        r2 = p-r1\n",
    "        reactants = (r1,r2)\n",
    "        uid_dict[(n,k)] = uid\n",
    "\n",
    "print(uid_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 0}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 1}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 2}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 21}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 22}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 23}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-60.], dtype=torch.float64), 'uid': 26}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 0}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 3}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 4}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 5}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 6}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 24}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-60.], dtype=torch.float64), 'uid': 25}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 1}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 3}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 7}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 8}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 9}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 10}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-60.], dtype=torch.float64), 'uid': 11}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 2}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 4}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 7}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 12}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 13}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 14}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-60.], dtype=torch.float64), 'uid': 15}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 16}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 17}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 8}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 12}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-80.], dtype=torch.float64), 'uid': 18}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 5}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 13}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 16}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-80.], dtype=torch.float64), 'uid': 19}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 6}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 9}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-80.], dtype=torch.float64), 'uid': 20}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 14}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-80.], dtype=torch.float64), 'uid': 20}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 21}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 10}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-80.], dtype=torch.float64), 'uid': 19}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 22}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-60.], dtype=torch.float64), 'uid': 15}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 17}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-60.], dtype=torch.float64), 'uid': 11}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-80.], dtype=torch.float64), 'uid': 18}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 23}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 24}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-60.], dtype=torch.float64), 'uid': 25}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-60.], dtype=torch.float64), 'uid': 26}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-100.]), 'uid': 27}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-100.]), 'uid': 27}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-100.]), 'uid': 27}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-100.]), 'uid': 28}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-100.]), 'uid': 28}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-100.]), 'uid': 28}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-100.]), 'uid': 28}\n",
      "Reaction rates:  tensor([ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         6.4000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000, 10.0000,  1.0000,  1.0000], dtype=torch.float64,\n",
      "       grad_fn=<CopySlices>)\n",
      "dGs:  tensor([ -20.,  -20.,  -20.,  -20.,  -20.,  -40.,  -40.,  -20.,  -40.,  -40.,\n",
      "         -40.,  -60.,  -40.,  -40.,  -40.,  -60.,  -20.,  -20.,  -80.,  -80.,\n",
      "         -80.,  -40.,  -40.,  -40.,  -40.,  -60.,  -60., -100., -100.],\n",
      "       dtype=torch.float64)\n",
      "Species Concentrations:  tensor([100., 100., 100., 100., 300.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.], dtype=torch.float64)\n",
      "Shifting to device:  cpu\n",
      "tensor([ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         6.4000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000, 10.0000,  1.0000,  1.0000], dtype=torch.float64,\n",
      "       grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "#Do modifications here\n",
    "#Changing Initial Conditions\n",
    "import networkx as nx\n",
    "#Changin k_on\n",
    "new_kon = torch.zeros([rn._rxn_count], requires_grad=True).double()\n",
    "new_kon = new_kon + Tensor([1.]*np.array(1e0))\n",
    "new_kon[16] = 6.4\n",
    "new_kon[26] = 10\n",
    "update_kon_dict = {}\n",
    "for edge in rn.network.edges:\n",
    "    print(rn.network.get_edge_data(edge[0],edge[1]))\n",
    "    update_kon_dict[edge] = new_kon[uid_dict[edge]]\n",
    "\n",
    "nx.set_edge_attributes(rn.network,update_kon_dict,'k_on')\n",
    "\n",
    "# for edge in rn.network.edges:\n",
    "#     print(rn.network.get_edge_data(edge[0],edge[1]))\n",
    "vec_rn = VectorizedRxnNet(rn, dev='cpu',assoc_is_param=False,chap_is_param=True)\n",
    "print(vec_rn.kon)\n",
    "\n",
    "#Changing initial concentrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The Equilibrium Solution ##\n",
    "First we will find the equilibrium solution for this system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# vec_rn.update_reaction_net(rn)\n",
    "# poly_system = EquilibriumSolver(rn)\n",
    "# solution = poly_system.solve()\n",
    "# print(solution)\n",
    "# if solution == None:\n",
    "#     print(\"No Equilibrium solution\")\n",
    "# else:\n",
    "#     print(solution)\n",
    "#     print(\"Equilibrium expected yield: \", 100 * solution[-1] / min(vec_rn.initial_copies[:vec_rn.num_monomers]), '%')\n",
    "# print(vec_rn.kon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uid_dict = {}\n",
    "# sys.path.append(\"../\")\n",
    "# import numpy as np\n",
    "# from reaction_network import gtostr\n",
    "# from torch import DoubleTensor as Tensor\n",
    "\n",
    "# def get_max_edge(n):\n",
    "#     \"\"\"\n",
    "#     Calculates the max rate (k_on) for a given node\n",
    "#     To find out the maximum flow path to the final complex starting from the current node.\n",
    "    \n",
    "#     Can also calculate the total rate of consumption of a node by summing up all rates. \n",
    "#     Can tell which component is used quickly.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         edges = rn.network.out_edges(n)\n",
    "#         #Loop over all edges\n",
    "#         #Get attributes\n",
    "#         if len(edges)==0:\n",
    "#             return(False)\n",
    "#         kon_max = -1\n",
    "#         next_node = -1\n",
    "        \n",
    "#         kon_sum = 0\n",
    "#         for edge in edges:\n",
    "#             data = rn.network.get_edge_data(edge[0],edge[1])\n",
    "#             #print(data)\n",
    "#             #Get uid\n",
    "#             uid = data['uid']\n",
    "#             #Get updated kon\n",
    "#             temp_kon = vec_rn.kon[uid]\n",
    "#             kon_sum+=temp_kon\n",
    "            \n",
    "# #             #Calculate k_off also\n",
    "# #             std_c = Tensor([1.])\n",
    "# #             l_kon = torch.log(temp_kon)\n",
    "# #             l_koff = (vec_rn.rxn_score_vec[uid] * 1. / (self._R * self._T)) + l_kon + torch.log(std_c)\n",
    "#             if temp_kon > kon_max:\n",
    "#                 kon_max = temp_kon\n",
    "#                 next_node=edge[1]\n",
    "#         return(kon_max,next_node,kon_sum)\n",
    "#     except Exception as err:\n",
    "#         raise(err)\n",
    "\n",
    "# pathway = []\n",
    "# kon_sumarray = []\n",
    "# total_con_rate = {}\n",
    "# for n in rn.network.nodes():\n",
    "    \n",
    "#     n_str = gtostr(rn.network.nodes[n]['struct']) \n",
    "    \n",
    "#     paths = [n_str]\n",
    "#     kon_sum = 0\n",
    "#     temp_node = n\n",
    "#     max_edge = True\n",
    "#     consumption_rate = 0\n",
    "#     if n < len(rn.network.nodes()):#num_monomers:\n",
    "# #         print(\"Current node: \")\n",
    "# #         print(n_str)\n",
    "#         while max_edge:\n",
    "#             max_edge = get_max_edge(temp_node)\n",
    "#             if max_edge:\n",
    "#                 total_con_rate[gtostr(rn.network.nodes[temp_node]['struct'])] = max_edge[2]\n",
    "#                 temp_node = max_edge[1]\n",
    "#                 kon_sum += max_edge[0].item()\n",
    "                \n",
    "# #                 print(\"Next node: \")\n",
    "# #                 print(temp_node)\n",
    "\n",
    "#                 paths.append(gtostr(rn.network.nodes[temp_node]['struct']))\n",
    "#             else:\n",
    "#                 break\n",
    "#         pathway.append(paths)\n",
    "#         kon_sumarray.append(kon_sum)\n",
    "#         paths=[]\n",
    "\n",
    "# print(pathway)\n",
    "# print(kon_sumarray)\n",
    "# #print(total_con_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if solution != None:\n",
    "#     for k,v in sorted(total_con_rate.items(),key=lambda x : x[1]):\n",
    "#         print(k,\" : \", v.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if solution !=None:\n",
    "#     for k,v in sorted(net_flux.items(),key=lambda x : x[1]):\n",
    "#         print(k,\" : \", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Using the optimizer with a 1 second simulation runtime ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5c9f8aad40f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                   \u001b[0moptim_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                   \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2e-3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1e-8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2e-3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                   device='cpu',method=\"RMSprop\",mom=0.5)\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_reaction_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconc_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmod_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconc_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmod_bool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myield_species\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_yield\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchap_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mjohn218_KineticAssembly/KineticAssembly_AD/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, reaction_network, sim_runtime, optim_iterations, learning_rate, device, method, lr_change_step, gamma, mom, random_lr)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_itr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                     \u001b[0mlr_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_itr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlr_val\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_itr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                         \u001b[0mlr_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_itr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "runtime=10\n",
    "vec_rn.reset(reset_params=True)\n",
    "optim = Optimizer(reaction_network=vec_rn,\n",
    "                  sim_runtime=runtime,\n",
    "                  optim_iterations=100,\n",
    "                  learning_rate=[2e-3,1e-8,2e-3],\n",
    "                  device='cpu',method=\"RMSprop\",mom=0.5)\n",
    "optim.rn.update_reaction_net(rn)\n",
    "optim.optimize(conc_scale=1e-1,mod_factor=1.0,conc_thresh=1e-1,mod_bool=True,yield_species=15,max_yield=0,chap_mode=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optim.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optim.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optim.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_var(v1,v2):\n",
    "    sq_sum=0\n",
    "    for i in range(len(v1)):\n",
    "        sq_sum=(v1[i]-v2[i])**2+sq_sum\n",
    "    \n",
    "    sq_sum = ((sq_sum)**0.5)/(len(v1)-1)\n",
    "    return(sq_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters with Max yield: \n",
      "Max Yield:  0.5989112369170285 \n",
      "Params:  [0.74211134] [9.99997067 0.01662728]\n",
      "Iteration:  60\n"
     ]
    }
   ],
   "source": [
    "yields= []\n",
    "ab_yields=[]\n",
    "abt_yields=[]\n",
    "final_copies=[]\n",
    "final_rates = []\n",
    "final_t50 = []\n",
    "final_t85 = []\n",
    "final_t95 = []\n",
    "final_t99 = []\n",
    "times = []\n",
    "n_copies=len(rn.chap_uid_map.keys())\n",
    "n_rates = 2*n_copies\n",
    "for i in range(len(optim.final_yields)):\n",
    "    yields.append(optim.final_yields[i][0].item())\n",
    "    ab_yields.append(optim.dimer_max[i].item())\n",
    "    abt_yields.append(optim.chap_max[i].item())\n",
    "    times.append(optim.endtimes[i])\n",
    "    rate_params = []\n",
    "    copy_params = []\n",
    "    for j in range(n_copies):\n",
    "#     print(optim.final_solns[i])\n",
    "        copy_params.append(optim.final_solns[i][j].numpy())\n",
    "    for r in range(n_rates):\n",
    "        rate_params.append(optim.final_solns[i][r+n_copies].numpy())\n",
    "    \n",
    "    final_copies.append(copy_params)\n",
    "    final_rates.append(rate_params)\n",
    "    \n",
    "    if type(optim.final_t50[i])==int:\n",
    "        final_t50.append(1) \n",
    "    else:\n",
    "        final_t50.append(optim.final_t50[i].item()) \n",
    "    if type(optim.final_t85[i])==int:\n",
    "        final_t85.append(1) \n",
    "    else:\n",
    "        final_t85.append(optim.final_t85[i].item()) \n",
    "    if type(optim.final_t95[i])==int:\n",
    "        final_t95.append(1)\n",
    "    else:\n",
    "        final_t95.append(optim.final_t95[i].item())\n",
    "    if type(optim.final_t99[i])==int:\n",
    "        final_t99.append(1)\n",
    "    else:\n",
    "        final_t99.append(optim.final_t99[i].item())\n",
    "\n",
    "sort_indx=np.argsort(np.array(yields))\n",
    "\n",
    "\n",
    "# unsorted_yields=np.array(yields)\n",
    "# unsorted_copies = np.array(final_copies)\n",
    "# unsorted_rates = np.array(final_rates)\n",
    "\n",
    "# mask = unsorted_yields>0.2\n",
    "\n",
    "sorted_yields=np.array(yields)#[sort_indx]\n",
    "sorted_ab_yields=np.array(ab_yields)\n",
    "sorted_abt_yields=np.array(abt_yields)\n",
    "sorted_times = np.array(times)\n",
    "sorted_copies = np.array(final_copies)#[sort_indx]\n",
    "sorted_rates = np.array(final_rates)#[sort_indx]\n",
    "sorted_t50 = np.array(final_t50)\n",
    "sorted_t85 = np.array(final_t85)\n",
    "sorted_t95 = np.array(final_t95)\n",
    "sorted_t99 = np.array(final_t99)\n",
    "\n",
    "print(\"Parameters with Max yield: \")\n",
    "print(\"Max Yield: \",sorted_yields[-1],\"\\nParams: \",sorted_copies[-1], sorted_rates[-1])\n",
    "print(\"Iteration: \", sort_indx[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Optimizer' object has no attribute 'final_yielssds'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_46076/725158597.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_yielssds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Optimizer' object has no attribute 'final_yielssds'"
     ]
    }
   ],
   "source": [
    "print(len(optim.final_yielssds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Writing all solutions to a file\n",
    "# filename=\"Tetramer_Solutions_Chaperone_20kT_Mode2\"\n",
    "# with open(filename,'a') as fl:\n",
    "#     fl.write(\"#! Yield\\tAB_Yield\\tABT_Yield\\tTimes\\tC_x\\tk1\\tk2\\tt50\\tt85\\tt90\\n\")\n",
    "#     for i in range(1,len(sorted_yields)):\n",
    "#         fl.write(\"%f\" %(sorted_yields[i]))\n",
    "#         fl.write(\"\\t%f\" %(sorted_ab_yields[i]))\n",
    "#         fl.write(\"\\t%f\" %(sorted_abt_yields[i]))\n",
    "#         fl.write(\"\\t%f\" %(sorted_times[i]))\n",
    "        \n",
    "#         for j in range(sorted_copies[0].shape[0]):         \n",
    "#             fl.write(\"\\t%f\" %(sorted_copies[i][j]))\n",
    "#         for r1 in range(sorted_rates[0].shape[0]):\n",
    "#             fl.write(\"\\t%f\" %(sorted_rates[i][r1]))\n",
    "#         fl.write(\"\\t%f\\t%f\\t%f\\n\" %(sorted_t50[i],sorted_t85[i],sorted_t95[i]))\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_dict = {}\n",
    "sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "from reaction_network import gtostr\n",
    "for n in rn.network.nodes():\n",
    "    #print(n)\n",
    "    #print(rn.network.nodes()[n])\n",
    "    for k,v in rn.network[n].items():\n",
    "        uid = v['uid']\n",
    "        r1 = set(gtostr(rn.network.nodes[n]['struct']))\n",
    "        p = set(gtostr(rn.network.nodes[k]['struct']))\n",
    "        r2 = p-r1\n",
    "        reactants = (r1,r2)\n",
    "        uid_val = {'reactants':reactants,'kon':v['k_on'],'score':v['rxn_score'],'koff':v['k_off']}\n",
    "        if uid not in uid_dict.keys():\n",
    "            uid_dict[uid] = uid_val\n",
    "    print(gtostr(rn.network.nodes[n]['struct']))\n",
    "    #for r_set in rn.get_reactant_sets(n):\n",
    "    #    print(tuple(r_set))\n",
    "    #print(rn.network[n]['struct'])\n",
    "ind_sort = np.argsort(vec_rn.kon.detach().numpy())\n",
    "for i in ind_sort:\n",
    "    print(vec_rn.kon[i])\n",
    "    print(uid_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uid_dict = {}\n",
    "sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "from reaction_network import gtostr\n",
    "from torch import DoubleTensor as Tensor\n",
    "\n",
    "def get_max_edge(n):\n",
    "    \"\"\"\n",
    "    Calculates the max rate (k_on) for a given node\n",
    "    To find out the maximum flow path to the final complex starting from the current node.\n",
    "    \n",
    "    Can also calculate the total rate of consumption of a node by summing up all rates. \n",
    "    Can tell which component is used quickly.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        edges = rn.network.out_edges(n)\n",
    "        #Loop over all edges\n",
    "        #Get attributes\n",
    "        if len(edges)==0:\n",
    "            return(False)\n",
    "        kon_max = -1\n",
    "        next_node = -1\n",
    "        \n",
    "        kon_sum = 0\n",
    "        for edge in edges:\n",
    "            data = rn.network.get_edge_data(edge[0],edge[1])\n",
    "            #print(data)\n",
    "            #Get uid\n",
    "            uid = data['uid']\n",
    "            #Get updated kon\n",
    "            temp_kon = vec_rn.kon[uid]\n",
    "            kon_sum+=temp_kon\n",
    "            \n",
    "#             #Calculate k_off also\n",
    "#             std_c = Tensor([1.])\n",
    "#             l_kon = torch.log(temp_kon)\n",
    "#             l_koff = (vec_rn.rxn_score_vec[uid] * 1. / (self._R * self._T)) + l_kon + torch.log(std_c)\n",
    "            if temp_kon > kon_max:\n",
    "                kon_max = temp_kon\n",
    "                next_node=edge[1]\n",
    "        return(kon_max,next_node,kon_sum)\n",
    "    except Exception as err:\n",
    "        raise(err)\n",
    "\n",
    "pathway = []\n",
    "kon_sumarray = []\n",
    "total_con_rate = {}\n",
    "for n in rn.network.nodes():\n",
    "    \n",
    "    n_str = gtostr(rn.network.nodes[n]['struct']) \n",
    "    \n",
    "    paths = [n_str]\n",
    "    kon_sum = 0\n",
    "    temp_node = n\n",
    "    max_edge = True\n",
    "    consumption_rate = 0\n",
    "    if n < len(rn.network.nodes()):#num_monomers:\n",
    "#         print(\"Current node: \")\n",
    "#         print(n_str)\n",
    "        while max_edge:\n",
    "            max_edge = get_max_edge(temp_node)\n",
    "            if max_edge:\n",
    "                total_con_rate[gtostr(rn.network.nodes[temp_node]['struct'])] = max_edge[2]\n",
    "                temp_node = max_edge[1]\n",
    "                kon_sum += max_edge[0].item()\n",
    "                \n",
    "#                 print(\"Next node: \")\n",
    "#                 print(temp_node)\n",
    "\n",
    "                paths.append(gtostr(rn.network.nodes[temp_node]['struct']))\n",
    "            else:\n",
    "                break\n",
    "        pathway.append(paths)\n",
    "        kon_sumarray.append(kon_sum)\n",
    "        paths=[]\n",
    "\n",
    "print(pathway)\n",
    "print(kon_sumarray)\n",
    "#print(total_con_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in sorted(total_con_rate.items(),key=lambda x : x[1]):\n",
    "    print(k,\" : \", v.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's first visualize some of the data.\n",
    "\n",
    "**Without any optimization**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nodes_list = ['A','B','S','M','AB','BMS','ABS','AMS','ABMS','AM','AS']\n",
    "#nodes_list = ['A','B','ABMS']\n",
    "optim.plot_observable(0,nodes_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**After 750 optimization iterations**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optim.plot_observable(-1,nodes_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.plot_yield()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It seems like we've found a stable solution that produces greater yield than equilibrium. This should be thermodynamically\n",
    "impossible. Let's try to find an explanation. We'll run simulations using the learned optimal parameters at a few different\n",
    "timescales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "optim_rn = optim.rn\n",
    "for i, runtime in enumerate([1, 8, 64]):\n",
    "    optim_rn.reset()\n",
    "    sim = VecSim(optim_rn, runtime, device='cpu')\n",
    "    y = sim.simulate()\n",
    "    sim.plot_observable(nodes_list,ax=ax[i],)\n",
    "    ax[i].set_title(\"runtime: \" + str(runtime) + \" seconds\")\n",
    "fig.set_size_inches(18, 6)\n",
    "node_map = {}\n",
    "for node in rn.network.nodes():\n",
    "    node_map[gtostr(rn.network.nodes[node]['struct'])] = node\n",
    "\n",
    "print(node_map)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_map = {}\n",
    "for node in rn.network.nodes():\n",
    "    node_map[gtostr(rn.network.nodes[node]['struct'])] = node\n",
    "\n",
    "print(node_map)\n",
    "def get_max_edge(n):\n",
    "    \"\"\"\n",
    "    Calculates the max rate (k_on) for a given node\n",
    "    To find out the maximum flow path to the final complex starting from the current node.\n",
    "    \n",
    "    Can also calculate the total rate of consumption of a node by summing up all rates. \n",
    "    Can tell which component is used quickly.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        edges = rn.network.out_edges(n)\n",
    "        #Loop over all edges\n",
    "        #Get attributes\n",
    "        kon_max = -1\n",
    "        next_node = -1\n",
    "\n",
    "        kon_sum = 0\n",
    "        total_flux_outedges = 0\n",
    "        total_flux_inedges = 0\n",
    "        if len(edges)==0:\n",
    "            return(False)\n",
    "            \n",
    "        for edge in edges:\n",
    "            data = rn.network.get_edge_data(edge[0],edge[1])\n",
    "            #print(data)\n",
    "            #Get uid\n",
    "            uid = data['uid']\n",
    "\n",
    "            #Get updated kon\n",
    "            temp_kon = vec_rn.kon[uid]\n",
    "            kon_sum+=temp_kon\n",
    "            \n",
    "            if temp_kon > kon_max:\n",
    "                kon_max = temp_kon\n",
    "                next_node=edge[1]\n",
    "             \n",
    "        return(kon_max,next_node,kon_sum)\n",
    "    except Exception as err:\n",
    "        raise(err)\n",
    "\n",
    "        \n",
    "def get_node_flux(n):\n",
    "    total_flux_outedges = 0\n",
    "    total_flux_inedges = 0\n",
    "    #Go over all the out edges\n",
    "    edges_out = rn.network.out_edges(n)\n",
    "    if len(edges_out)>0:\n",
    "\n",
    "        for edge in edges_out:\n",
    "            data = rn.network.get_edge_data(edge[0],edge[1])\n",
    "            #print(data)\n",
    "            #Get uid\n",
    "            uid = data['uid']\n",
    "\n",
    "            #Get updated kon\n",
    "            temp_kon = vec_rn.kon[uid]\n",
    "\n",
    "            #Calculate k_off also\n",
    "            std_c = Tensor([1.])\n",
    "            l_kon = torch.log(temp_kon)\n",
    "            l_koff = (vec_rn.rxn_score_vec[uid] * 1. / (vec_rn._R * vec_rn._T)) + l_kon + torch.log(std_c)\n",
    "            koff = torch.exp(l_koff)\n",
    "\n",
    "            #Getting conc. of reactants and products\n",
    "            #Get product\n",
    "            prod = gtostr(rn.network.nodes[edge[1]]['struct']) \n",
    "            #Get other reactant\n",
    "            react = \"\".join(sorted(list(set(prod) - set(gtostr(rn.network.nodes[edge[0]]['struct']) ))))\n",
    "\n",
    "            #Net flux from this edge = Generation - consumption\n",
    "            edge_flux = koff*vec_rn.copies_vec[edge[1]] - temp_kon*(vec_rn.copies_vec[edge[0]])*(vec_rn.copies_vec[node_map[react]])\n",
    "            #edge_flux = koff*vec_rn.copies_vec[edge[1]] \n",
    "\n",
    "            print(\"Reaction: \", gtostr(rn.network.nodes[edge[0]]['struct']), \"+\",react,\" -> \",prod)\n",
    "            print(\"Net flux: \",edge_flux)\n",
    "            print(\"kon : \",temp_kon)\n",
    "            print(\"koff: \",koff)\n",
    "            print(\"Reaction data OUTWARD: \")\n",
    "            print(data)\n",
    "\n",
    "            total_flux_outedges+=edge_flux\n",
    "    \n",
    "    #Now go over all the in edges\n",
    "    edges_in = rn.network.in_edges(n)\n",
    "    react_list = []\n",
    "    if len(edges_in) > 0:\n",
    "        for edge in edges_in:\n",
    "            if edge[0] in react_list:\n",
    "                continue\n",
    "            data = rn.network.get_edge_data(edge[0],edge[1])\n",
    "            uid = data['uid']\n",
    "\n",
    "\n",
    "            #Get generation rates; which would be kon\n",
    "            temp_kon = vec_rn.kon[uid]\n",
    "\n",
    "            #Get consumption rates; which is k_off\n",
    "            std_c = Tensor([1.])\n",
    "            l_kon = torch.log(temp_kon)\n",
    "            l_koff = (vec_rn.rxn_score_vec[uid] * 1. / (vec_rn._R * vec_rn._T)) + l_kon + torch.log(std_c)\n",
    "            koff = torch.exp(l_koff)\n",
    "\n",
    "            #Get conc. of reactants and products\n",
    "            prod = gtostr(rn.network.nodes[edge[1]]['struct'])\n",
    "            #Get other reactant\n",
    "            react = \"\".join(sorted(list(set(prod) - set(gtostr(rn.network.nodes[edge[0]]['struct']) ))))\n",
    "            react_list.append(node_map[react])\n",
    "            #Net flux from this edge = Generation - consumption\n",
    "            edge_flux_in = temp_kon*(vec_rn.copies_vec[edge[0]])*(vec_rn.copies_vec[node_map[react]])- koff*vec_rn.copies_vec[edge[1]]\n",
    "            #edge_flux_in = koff*vec_rn.copies_vec[edge[1]]\n",
    "            \n",
    "\n",
    "\n",
    "            print(\"Reaction: \", prod ,\" -> \",gtostr(rn.network.nodes[edge[0]]['struct']), \"+\",react)\n",
    "            print(\"Net flux: \",edge_flux_in)\n",
    "            print(\"kon : \",temp_kon)\n",
    "            print(\"koff: \",koff)\n",
    "            print(\"Raction data INWARD: \")\n",
    "            print(data)\n",
    "\n",
    "            total_flux_inedges+=edge_flux_in\n",
    "    net_node_flux = total_flux_outedges + total_flux_inedges\n",
    "    \n",
    "    return(net_node_flux)\n",
    "    \n",
    "pathway = []\n",
    "kon_sumarray = []\n",
    "total_con_rate = {}\n",
    "net_flux = {}\n",
    "for n in rn.network.nodes():\n",
    "    \n",
    "    n_str = gtostr(rn.network.nodes[n]['struct']) \n",
    "    \n",
    "    paths = [n_str]\n",
    "    kon_sum = 0\n",
    "    temp_node = n\n",
    "    max_edge = True\n",
    "    consumption_rate = 0\n",
    "    if n < len(rn.network.nodes()):#num_monomers:\n",
    "#         print(\"Current node: \")\n",
    "#         print(n_str)\n",
    "        while max_edge:\n",
    "            max_edge = get_max_edge(temp_node)\n",
    "            if max_edge:\n",
    "                total_con_rate[gtostr(rn.network.nodes[temp_node]['struct'])] = max_edge[2]\n",
    "                \n",
    "                temp_node = max_edge[1]\n",
    "                kon_sum += max_edge[0].item()\n",
    "                \n",
    "                \n",
    "#                 print(\"Next node: \")\n",
    "#                 print(temp_node)\n",
    "\n",
    "                paths.append(gtostr(rn.network.nodes[temp_node]['struct']))\n",
    "            else:\n",
    "                break\n",
    "        pathway.append(paths)\n",
    "        kon_sumarray.append(kon_sum)\n",
    "        paths=[]\n",
    "    print(\"-------------------------------------------------------------------------------\")\n",
    "    print(\"-------------------------------------------------------------------------------\")\n",
    "    print(\"|                                                                             |\")\n",
    "    node_flux = get_node_flux(n)\n",
    "    net_flux[gtostr(rn.network.nodes[n]['struct'])] = node_flux\n",
    "    print(\"|                                                                             |\")\n",
    "    print(\"-------------------------------------------------------------------------------\")\n",
    "    print(\"-------------------------------------------------------------------------------\")\n",
    "\n",
    "print(pathway)\n",
    "print(kon_sumarray)\n",
    "\n",
    "#print(total_con_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in sorted(net_flux.items(),key=lambda x : x[1]):\n",
    "    print(k,\" : \", v)\n",
    "\n",
    "print(vec_rn.copies_vec)\n",
    "print(vec_rn.kon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(solution)\n",
    "poly_system = EquilibriumSolver(rn)\n",
    "solution = poly_system.solve(init_val=vec_rn.copies_vec.detach().numpy().tolist())\n",
    "#solution = poly_system.solve(verifyBool = False)\n",
    "if solution == None:\n",
    "    print(\"No Equilibrium solution\")\n",
    "else:\n",
    "    print(solution)\n",
    "    print(\"Equilibrium expected yield: \", 100 * solution[-1] / min(vec_rn.initial_copies[:vec_rn.num_monomers]), '%')\n",
    "print(vec_rn.kon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the equilibrium reached by the system still matches the equilibrium solution. We have however found a set of parameters that can increase available complete AP2 at some point before equilibrium to levels significantly higher than at equilibrium. We don't observe any trapping, but have uncovered an interesting effect. \n",
    "\n",
    "Now we'll move on to looking at ARP23. This is 7 subunits, which drastically increases the number of possible reactions. Expect longer runtimes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
