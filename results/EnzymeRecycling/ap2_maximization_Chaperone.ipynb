{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Equilibrium Maximazation of Yield #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# make sure jupyter path is correct for loading local moudules\n",
    "import sys\n",
    "# path to steric_simulator module relative to notebook\n",
    "sys.path.append(\"../../../\")\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from KineticAssembly_AD import ReactionNetwork, VectorizedRxnNet, VecSim, Optimizer, EquilibriumSolver\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch import DoubleTensor as Tensor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with the AP2 complex that we've worked with before. Pairwise $\\Delta Gs$ were derived from the PDB structures via Rossetta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['default_assoc', 1.0]\n",
      "['monomer_add_only', False]\n",
      "['chaperone', True]\n",
      "[(0, {'struct': <networkx.classes.graph.Graph object at 0x7f673abaccf8>, 'copies': tensor([100.], dtype=torch.float64), 'subunits': 1}), (1, {'struct': <networkx.classes.graph.Graph object at 0x7f673aa64a20>, 'copies': tensor([100.], dtype=torch.float64), 'subunits': 1}), (2, {'struct': <networkx.classes.graph.Graph object at 0x7f67c02c48d0>, 'copies': tensor([100.], dtype=torch.float64), 'subunits': 1}), (3, {'struct': <networkx.classes.graph.Graph object at 0x7f67c02c4160>, 'copies': tensor([100.], dtype=torch.float64), 'subunits': 1}), (4, {'struct': <networkx.classes.graph.Graph object at 0x7f67c02c4208>, 'copies': tensor([300.], dtype=torch.float64), 'subunits': 1})]\n",
      "New node added - Node index: 5 ; Node label: AM \n",
      "New node added - Node index: 6 ; Node label: AB \n",
      "New node added - Node index: 7 ; Node label: AS \n",
      "New node added - Node index: 8 ; Node label: BM \n",
      "New node added - Node index: 9 ; Node label: MS \n",
      "New node added - Node index: 10 ; Node label: ABM \n",
      "New node added - Node index: 11 ; Node label: AMS \n",
      "New node added - Node index: 12 ; Node label: BS \n",
      "New node added - Node index: 13 ; Node label: ABS \n",
      "New node added - Node index: 14 ; Node label: BMS \n",
      "New node added - Node index: 15 ; Node label: ABMS \n",
      "The number of bonds formed are not compensated by the number of edges\n",
      "This could be possible due to presence of a repeating subunit\n",
      "SOurce1:  2 11\n",
      "The common reactant is:  B\n",
      "Edge added between:  2 15\n",
      "The number of bonds formed are not compensated by the number of edges\n",
      "This could be possible due to presence of a repeating subunit\n",
      "SOurce1:  3 10\n",
      "The common reactant is:  S\n",
      "Edge added between:  3 15\n",
      "*******Chaperone Reaction**********\n",
      "[4, 6] ['A', 'B', 'X']\n",
      "New node added - Node index: 16 ; Node label: ABX \n",
      "*******Chaperone Reaction**********\n",
      "[4, 10] ['A', 'B', 'M', 'X']\n",
      "New node added - Node index: 17 ; Node label: ABMX \n",
      "The number of bonds formed are not compensated by the number of edges\n",
      "This could be possible due to presence of a repeating subunit\n",
      "SOurce1:  5 12\n",
      "*******Chaperone Reaction**********\n",
      "[4, 6] ['A', 'B', 'X']\n",
      "The number of bonds formed are not compensated by the number of edges\n",
      "This could be possible due to presence of a repeating subunit\n",
      "SOurce1:  6 9\n",
      "The number of bonds formed are not compensated by the number of edges\n",
      "This could be possible due to presence of a repeating subunit\n",
      "SOurce1:  7 8\n",
      "*******Chaperone Reaction**********\n",
      "[4, 10] ['A', 'B', 'M', 'X']\n",
      "The number of bonds formed are not compensated by the number of edges\n",
      "This could be possible due to presence of a repeating subunit\n",
      "SOurce1:  13 1\n",
      "The common reactant is:  M\n",
      "Edge added between:  1 15\n",
      "The number of bonds formed are not compensated by the number of edges\n",
      "This could be possible due to presence of a repeating subunit\n",
      "SOurce1:  14 0\n",
      "The common reactant is:  A\n",
      "Edge added between:  0 15\n",
      "Resolving Chaperone Rxns::\n",
      "[([4, 6], ['A', 'B', 'X']), ([4, 10], ['A', 'B', 'M', 'X'])]\n",
      "Reaction Network Completed\n"
     ]
    }
   ],
   "source": [
    "base_input = '../input_files/tetramer_chaperone.pwr'\n",
    "rn = ReactionNetwork(base_input, one_step=True)\n",
    "rn.resolve_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(1, 1): [0, 1, 2, 3, 4, 7], (2, 1): [5, 6, 8, 9, 10, 12, 13, 14, 16, 21, 22, 23, 24], (3, 1): [11, 15, 17, 25, 26], (2, 2): [18, 19, 20]}\n",
      "{4: [27, 16, 28, 17]}\n"
     ]
    }
   ],
   "source": [
    "print(rn.rxn_class)\n",
    "print(rn.chap_uid_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -- A\n",
      "1 -- M\n",
      "2 -- B\n",
      "3 -- S\n",
      "4 -- X\n",
      "5 -- AM\n",
      "6 -- AB\n",
      "7 -- AS\n",
      "8 -- BM\n",
      "9 -- MS\n",
      "10 -- ABM\n",
      "11 -- AMS\n",
      "12 -- BS\n",
      "13 -- ABS\n",
      "14 -- BMS\n",
      "15 -- ABMS\n",
      "16 -- ABX\n",
      "17 -- ABMX\n",
      "{(0, 5): 0, (0, 6): 1, (0, 7): 2, (0, 10): 21, (0, 11): 22, (0, 13): 23, (0, 15): 26, (1, 5): 0, (1, 8): 3, (1, 9): 4, (1, 10): 5, (1, 11): 6, (1, 14): 24, (1, 15): 25, (2, 6): 1, (2, 8): 3, (2, 12): 7, (2, 10): 8, (2, 13): 9, (2, 14): 10, (2, 15): 11, (3, 7): 2, (3, 9): 4, (3, 12): 7, (3, 11): 12, (3, 13): 13, (3, 14): 14, (3, 15): 15, (4, 16): 16, (4, 17): 17, (5, 10): 8, (5, 11): 12, (5, 15): 18, (6, 10): 5, (6, 13): 13, (6, 16): 16, (6, 15): 19, (7, 11): 6, (7, 13): 9, (7, 15): 20, (8, 14): 14, (8, 15): 20, (8, 10): 21, (9, 14): 10, (9, 15): 19, (9, 11): 22, (10, 15): 15, (10, 17): 17, (11, 15): 11, (12, 15): 18, (12, 13): 23, (12, 14): 24, (13, 15): 25, (14, 15): 26, (16, 0): 27, (16, 2): 27, (16, 4): 27, (17, 0): 28, (17, 1): 28, (17, 2): 28, (17, 4): 28}\n"
     ]
    }
   ],
   "source": [
    "uid_dict = {}\n",
    "sys.path.append(\"../../\")\n",
    "import numpy as np\n",
    "from reaction_network import gtostr\n",
    "for n in rn.network.nodes():\n",
    "    #print(n)\n",
    "    #print(rn.network.nodes()[n])\n",
    "    print(n,\"--\",gtostr(rn.network.nodes[n]['struct']))\n",
    "    for k,v in rn.network[n].items():\n",
    "        uid = v['uid']\n",
    "        r1 = set(gtostr(rn.network.nodes[n]['struct']))\n",
    "        p = set(gtostr(rn.network.nodes[k]['struct']))\n",
    "        r2 = p-r1\n",
    "        reactants = (r1,r2)\n",
    "        uid_dict[(n,k)] = uid\n",
    "\n",
    "print(uid_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17])\n"
     ]
    }
   ],
   "source": [
    "print(rn.observables.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 0}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 1}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 2}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 21}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 22}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 23}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-60.], dtype=torch.float64), 'uid': 26}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 0}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 3}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 4}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 5}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 6}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 24}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-60.], dtype=torch.float64), 'uid': 25}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 1}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 3}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 7}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 8}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 9}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 10}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-60.], dtype=torch.float64), 'uid': 11}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 2}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 4}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 7}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 12}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 13}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 14}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-60.], dtype=torch.float64), 'uid': 15}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 16}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 17}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 8}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 12}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-80.], dtype=torch.float64), 'uid': 18}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 5}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 13}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 16}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-80.], dtype=torch.float64), 'uid': 19}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 6}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 9}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-80.], dtype=torch.float64), 'uid': 20}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 14}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-80.], dtype=torch.float64), 'uid': 20}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 21}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 10}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-80.], dtype=torch.float64), 'uid': 19}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 22}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-60.], dtype=torch.float64), 'uid': 15}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 17}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-60.], dtype=torch.float64), 'uid': 11}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-80.], dtype=torch.float64), 'uid': 18}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 23}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 24}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-60.], dtype=torch.float64), 'uid': 25}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-60.], dtype=torch.float64), 'uid': 26}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-100.]), 'uid': 27}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-100.]), 'uid': 27}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-100.]), 'uid': 27}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-100.]), 'uid': 28}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-100.]), 'uid': 28}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-100.]), 'uid': 28}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-100.]), 'uid': 28}\n",
      "Reaction rates:  tensor([ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         6.4000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000, 10.0000,  1.0000,  1.0000], dtype=torch.float64,\n",
      "       grad_fn=<CopySlices>)\n",
      "dGs:  tensor([ -20.,  -20.,  -20.,  -20.,  -20.,  -40.,  -40.,  -20.,  -40.,  -40.,\n",
      "         -40.,  -60.,  -40.,  -40.,  -40.,  -60.,  -20.,  -20.,  -80.,  -80.,\n",
      "         -80.,  -40.,  -40.,  -40.,  -40.,  -60.,  -60., -100., -100.],\n",
      "       dtype=torch.float64)\n",
      "Species Concentrations:  tensor([100., 100., 100., 100., 300.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.], dtype=torch.float64)\n",
      "Shifting to device:  cpu\n",
      "tensor([ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         6.4000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000, 10.0000,  1.0000,  1.0000], dtype=torch.float64,\n",
      "       grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "#Do modifications here\n",
    "#Changing Initial Conditions\n",
    "import networkx as nx\n",
    "#Changin k_on\n",
    "new_kon = torch.zeros([rn._rxn_count], requires_grad=True).double()\n",
    "new_kon = new_kon + Tensor([1.]*np.array(1e0))\n",
    "new_kon[16] = 6.4\n",
    "new_kon[26] = 10\n",
    "update_kon_dict = {}\n",
    "for edge in rn.network.edges:\n",
    "    print(rn.network.get_edge_data(edge[0],edge[1]))\n",
    "    update_kon_dict[edge] = new_kon[uid_dict[edge]]\n",
    "\n",
    "nx.set_edge_attributes(rn.network,update_kon_dict,'k_on')\n",
    "\n",
    "# for edge in rn.network.edges:\n",
    "#     print(rn.network.get_edge_data(edge[0],edge[1]))\n",
    "vec_rn = VectorizedRxnNet(rn, dev='cpu',assoc_is_param=False,chap_is_param=True)\n",
    "print(vec_rn.kon)\n",
    "\n",
    "#Changing initial concentrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The Equilibrium Solution ##\n",
    "First we will find the equilibrium solution for this system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# vec_rn.update_reaction_net(rn)\n",
    "# poly_system = EquilibriumSolver(rn)\n",
    "# solution = poly_system.solve()\n",
    "# print(solution)\n",
    "# if solution == None:\n",
    "#     print(\"No Equilibrium solution\")\n",
    "# else:\n",
    "#     print(solution)\n",
    "#     print(\"Equilibrium expected yield: \", 100 * solution[-1] / min(vec_rn.initial_copies[:vec_rn.num_monomers]), '%')\n",
    "# print(vec_rn.kon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uid_dict = {}\n",
    "# sys.path.append(\"../\")\n",
    "# import numpy as np\n",
    "# from reaction_network import gtostr\n",
    "# from torch import DoubleTensor as Tensor\n",
    "\n",
    "# def get_max_edge(n):\n",
    "#     \"\"\"\n",
    "#     Calculates the max rate (k_on) for a given node\n",
    "#     To find out the maximum flow path to the final complex starting from the current node.\n",
    "    \n",
    "#     Can also calculate the total rate of consumption of a node by summing up all rates. \n",
    "#     Can tell which component is used quickly.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         edges = rn.network.out_edges(n)\n",
    "#         #Loop over all edges\n",
    "#         #Get attributes\n",
    "#         if len(edges)==0:\n",
    "#             return(False)\n",
    "#         kon_max = -1\n",
    "#         next_node = -1\n",
    "        \n",
    "#         kon_sum = 0\n",
    "#         for edge in edges:\n",
    "#             data = rn.network.get_edge_data(edge[0],edge[1])\n",
    "#             #print(data)\n",
    "#             #Get uid\n",
    "#             uid = data['uid']\n",
    "#             #Get updated kon\n",
    "#             temp_kon = vec_rn.kon[uid]\n",
    "#             kon_sum+=temp_kon\n",
    "            \n",
    "# #             #Calculate k_off also\n",
    "# #             std_c = Tensor([1.])\n",
    "# #             l_kon = torch.log(temp_kon)\n",
    "# #             l_koff = (vec_rn.rxn_score_vec[uid] * 1. / (self._R * self._T)) + l_kon + torch.log(std_c)\n",
    "#             if temp_kon > kon_max:\n",
    "#                 kon_max = temp_kon\n",
    "#                 next_node=edge[1]\n",
    "#         return(kon_max,next_node,kon_sum)\n",
    "#     except Exception as err:\n",
    "#         raise(err)\n",
    "\n",
    "# pathway = []\n",
    "# kon_sumarray = []\n",
    "# total_con_rate = {}\n",
    "# for n in rn.network.nodes():\n",
    "    \n",
    "#     n_str = gtostr(rn.network.nodes[n]['struct']) \n",
    "    \n",
    "#     paths = [n_str]\n",
    "#     kon_sum = 0\n",
    "#     temp_node = n\n",
    "#     max_edge = True\n",
    "#     consumption_rate = 0\n",
    "#     if n < len(rn.network.nodes()):#num_monomers:\n",
    "# #         print(\"Current node: \")\n",
    "# #         print(n_str)\n",
    "#         while max_edge:\n",
    "#             max_edge = get_max_edge(temp_node)\n",
    "#             if max_edge:\n",
    "#                 total_con_rate[gtostr(rn.network.nodes[temp_node]['struct'])] = max_edge[2]\n",
    "#                 temp_node = max_edge[1]\n",
    "#                 kon_sum += max_edge[0].item()\n",
    "                \n",
    "# #                 print(\"Next node: \")\n",
    "# #                 print(temp_node)\n",
    "\n",
    "#                 paths.append(gtostr(rn.network.nodes[temp_node]['struct']))\n",
    "#             else:\n",
    "#                 break\n",
    "#         pathway.append(paths)\n",
    "#         kon_sumarray.append(kon_sum)\n",
    "#         paths=[]\n",
    "\n",
    "# print(pathway)\n",
    "# print(kon_sumarray)\n",
    "# #print(total_con_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if solution != None:\n",
    "#     for k,v in sorted(total_con_rate.items(),key=lambda x : x[1]):\n",
    "#         print(k,\" : \", v.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if solution !=None:\n",
    "#     for k,v in sorted(net_flux.items(),key=lambda x : x[1]):\n",
    "#         print(k,\" : \", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Using the optimizer with a 1 second simulation runtime ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor(300., dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor(6.4000, dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor(1., dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor(1., dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor(1., dtype=torch.float64, requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(vec_rn.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 10]\n"
     ]
    }
   ],
   "source": [
    "print(vec_rn.optimize_species['substrate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Be careful about choosing yield_species; It defaults to the largest complex\n",
      "Reaction Parameters before optimization: \n",
      "[Parameter containing:\n",
      "tensor(300., dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor(6.4000, dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor(1., dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor(1., dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor(1., dtype=torch.float64, requires_grad=True)]\n",
      "Optimizer State: <bound method Optimizer.state_dict of RMSprop (\n",
      "Parameter Group 0\n",
      "    alpha: 0.99\n",
      "    centered: False\n",
      "    eps: 1e-08\n",
      "    lr: 0.6\n",
      "    momentum: 0.5\n",
      "    weight_decay: 0\n",
      "\n",
      "Parameter Group 1\n",
      "    alpha: 0.99\n",
      "    centered: False\n",
      "    eps: 1e-08\n",
      "    lr: 6.4e-08\n",
      "    momentum: 0.5\n",
      "    weight_decay: 0\n",
      "\n",
      "Parameter Group 2\n",
      "    alpha: 0.99\n",
      "    centered: False\n",
      "    eps: 1e-08\n",
      "    lr: 0.002\n",
      "    momentum: 0.5\n",
      "    weight_decay: 0\n",
      "\n",
      "Parameter Group 3\n",
      "    alpha: 0.99\n",
      "    centered: False\n",
      "    eps: 1e-08\n",
      "    lr: 1e-05\n",
      "    momentum: 0.5\n",
      "    weight_decay: 0\n",
      "\n",
      "Parameter Group 4\n",
      "    alpha: 0.99\n",
      "    centered: False\n",
      "    eps: 1e-08\n",
      "    lr: 1e-05\n",
      "    momentum: 0.5\n",
      "    weight_decay: 0\n",
      ")>\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 0 was 98.0%.\n",
      "current params: [tensor(300., dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0296, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0274, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9808, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(9.5241e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0022, dtype=torch.float64)Grad:  tensor(9.2311, dtype=torch.float64)Grad:  tensor(2.5916, dtype=torch.float64)Grad:  tensor(0.0906, dtype=torch.float64)Grad:  tensor(0.0609, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 1 was 98.1%.\n",
      "current params: [tensor(294.0003, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.9800, dtype=torch.float64), tensor(0.9999, dtype=torch.float64), tensor(0.9999, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0300, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0281, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9811, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(9.5501e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0015, dtype=torch.float64)Grad:  tensor(6.4337, dtype=torch.float64)Grad:  tensor(4.1901, dtype=torch.float64)Grad:  tensor(0.0897, dtype=torch.float64)Grad:  tensor(0.0600, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 2 was 98.1%.\n",
      "current params: [tensor(287.5425, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.9530, dtype=torch.float64), tensor(0.9998, dtype=torch.float64), tensor(0.9998, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0304, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0290, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9811, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(9.6939e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0015, dtype=torch.float64)Grad:  tensor(6.3864, dtype=torch.float64)Grad:  tensor(2.1570, dtype=torch.float64)Grad:  tensor(0.0933, dtype=torch.float64)Grad:  tensor(0.0617, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 3 was 98.0%.\n",
      "current params: [tensor(281.3088, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.9314, dtype=torch.float64), tensor(0.9997, dtype=torch.float64), tensor(0.9997, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0309, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0297, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9809, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(9.8968e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0014, dtype=torch.float64)Grad:  tensor(5.6838, dtype=torch.float64)Grad:  tensor(1.6014, dtype=torch.float64)Grad:  tensor(0.0947, dtype=torch.float64)Grad:  tensor(0.0622, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 4 was 98.0%.\n",
      "current params: [tensor(275.7255, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.9148, dtype=torch.float64), tensor(0.9995, dtype=torch.float64), tensor(0.9995, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0313, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0304, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9808, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0011, dtype=torch.float64)Grad:  tensor(4.6322, dtype=torch.float64)Grad:  tensor(1.0512, dtype=torch.float64)Grad:  tensor(0.0966, dtype=torch.float64)Grad:  tensor(0.0629, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 5 was 98.0%.\n",
      "current params: [tensor(271.0093, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.9028, dtype=torch.float64), tensor(0.9994, dtype=torch.float64), tensor(0.9994, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0316, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0309, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9807, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0010, dtype=torch.float64)Grad:  tensor(4.1117, dtype=torch.float64)Grad:  tensor(1.9003, dtype=torch.float64)Grad:  tensor(0.0951, dtype=torch.float64)Grad:  tensor(0.0621, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 6 was 98.0%.\n",
      "current params: [tensor(266.9926, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.8904, dtype=torch.float64), tensor(0.9994, dtype=torch.float64), tensor(0.9994, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0319, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0315, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9808, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0017, dtype=torch.float64)Grad:  tensor(7.0282, dtype=torch.float64)Grad:  tensor(0.4825, dtype=torch.float64)Grad:  tensor(0.0949, dtype=torch.float64)Grad:  tensor(0.0627, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 7 was 98.0%.\n",
      "current params: [tensor(262.3975, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.8825, dtype=torch.float64), tensor(0.9993, dtype=torch.float64), tensor(0.9993, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0323, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0319, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9808, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad:  tensor(0.0016, dtype=torch.float64)Grad:  tensor(6.4263, dtype=torch.float64)Grad:  tensor(0.9949, dtype=torch.float64)Grad:  tensor(0.0941, dtype=torch.float64)Grad:  tensor(0.0623, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 8 was 98.0%.\n",
      "current params: [tensor(257.8802, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.8753, dtype=torch.float64), tensor(0.9992, dtype=torch.float64), tensor(0.9992, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0327, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0324, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9809, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0009, dtype=torch.float64)Grad:  tensor(3.6635, dtype=torch.float64)Grad:  tensor(2.5283, dtype=torch.float64)Grad:  tensor(0.0933, dtype=torch.float64)Grad:  tensor(0.0613, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 9 was 98.0%.\n",
      "current params: [tensor(254.3729, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.8638, dtype=torch.float64), tensor(0.9991, dtype=torch.float64), tensor(0.9991, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0330, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0329, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9808, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0012, dtype=torch.float64)Grad:  tensor(4.8128, dtype=torch.float64)Grad:  tensor(0.6091, dtype=torch.float64)Grad:  tensor(0.0963, dtype=torch.float64)Grad:  tensor(0.0630, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 10 was 98.0%.\n",
      "current params: [tensor(251.0213, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.8562, dtype=torch.float64), tensor(0.9990, dtype=torch.float64), tensor(0.9990, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0332, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0333, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9809, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0014, dtype=torch.float64)Grad:  tensor(5.5717, dtype=torch.float64)Grad:  tensor(0.7676, dtype=torch.float64)Grad:  tensor(0.0949, dtype=torch.float64)Grad:  tensor(0.0626, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 11 was 98.0%.\n",
      "current params: [tensor(247.5635, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.8500, dtype=torch.float64), tensor(0.9990, dtype=torch.float64), tensor(0.9990, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0335, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0337, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9805, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0010, dtype=torch.float64)Grad:  tensor(3.8511, dtype=torch.float64)Grad:  tensor(4.1345, dtype=torch.float64)Grad:  tensor(0.0884, dtype=torch.float64)Grad:  tensor(0.0591, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 12 was 98.0%.\n",
      "current params: [tensor(244.6186, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.8361, dtype=torch.float64), tensor(0.9989, dtype=torch.float64), tensor(0.9989, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0338, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0342, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9806, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0013, dtype=torch.float64)Grad:  tensor(5.0399, dtype=torch.float64)Grad:  tensor(1.3141, dtype=torch.float64)Grad:  tensor(0.0937, dtype=torch.float64)Grad:  tensor(0.0619, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 13 was 98.0%.\n",
      "current params: [tensor(241.5941, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.8257, dtype=torch.float64), tensor(0.9989, dtype=torch.float64), tensor(0.9989, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0341, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0347, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9808, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0013, dtype=torch.float64)Grad:  tensor(5.2999, dtype=torch.float64)Grad:  tensor(1.9237, dtype=torch.float64)Grad:  tensor(0.0917, dtype=torch.float64)Grad:  tensor(0.0611, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 14 was 98.0%.\n",
      "current params: [tensor(238.4942, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.8156, dtype=torch.float64), tensor(0.9988, dtype=torch.float64), tensor(0.9988, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0343, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0352, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9804, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0014, dtype=torch.float64)Grad:  tensor(5.7196, dtype=torch.float64)Grad:  tensor(0.7755, dtype=torch.float64)Grad:  tensor(0.0939, dtype=torch.float64)Grad:  tensor(0.0621, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 15 was 98.0%.\n",
      "current params: [tensor(235.2826, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.8087, dtype=torch.float64), tensor(0.9987, dtype=torch.float64), tensor(0.9988, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0346, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0356, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9807, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0016, dtype=torch.float64)Grad:  tensor(6.1766, dtype=torch.float64)Grad:  tensor(0.2932, dtype=torch.float64)Grad:  tensor(0.0944, dtype=torch.float64)Grad:  tensor(0.0626, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 16 was 98.0%.\n",
      "current params: [tensor(231.9432, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.8044, dtype=torch.float64), tensor(0.9987, dtype=torch.float64), tensor(0.9987, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0349, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0360, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9805, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad:  tensor(0.0009, dtype=torch.float64)Grad:  tensor(3.4997, dtype=torch.float64)Grad:  tensor(0.8176, dtype=torch.float64)Grad:  tensor(0.0960, dtype=torch.float64)Grad:  tensor(0.0626, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 17 was 98.0%.\n",
      "current params: [tensor(229.2969, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.8002, dtype=torch.float64), tensor(0.9986, dtype=torch.float64), tensor(0.9986, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0352, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0363, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9808, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0012, dtype=torch.float64)Grad:  tensor(4.8330, dtype=torch.float64)Grad:  tensor(0.8286, dtype=torch.float64)Grad:  tensor(0.0942, dtype=torch.float64)Grad:  tensor(0.0622, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 18 was 98.0%.\n",
      "current params: [tensor(226.6463, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.7960, dtype=torch.float64), tensor(0.9986, dtype=torch.float64), tensor(0.9986, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0354, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0366, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9806, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0011, dtype=torch.float64)Grad:  tensor(4.4207, dtype=torch.float64)Grad:  tensor(2.1029, dtype=torch.float64)Grad:  tensor(0.0910, dtype=torch.float64)Grad:  tensor(0.0606, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 19 was 98.0%.\n",
      "current params: [tensor(224.1217, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.7888, dtype=torch.float64), tensor(0.9985, dtype=torch.float64), tensor(0.9985, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0357, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0370, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9803, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(1.0143e-06, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0014, dtype=torch.float64)Grad:  tensor(5.3631, dtype=torch.float64)Grad:  tensor(2.4338, dtype=torch.float64)Grad:  tensor(0.0886, dtype=torch.float64)Grad:  tensor(0.0598, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 20 was 98.0%.\n",
      "current params: [tensor(221.4335, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.7793, dtype=torch.float64), tensor(0.9985, dtype=torch.float64), tensor(0.9985, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0360, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0375, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9807, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0018, dtype=torch.float64)Grad:  tensor(6.9746, dtype=torch.float64)Grad:  tensor(0.2030, dtype=torch.float64)Grad:  tensor(0.0926, dtype=torch.float64)Grad:  tensor(0.0622, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 21 was 98.0%.\n",
      "current params: [tensor(218.3027, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.7742, dtype=torch.float64), tensor(0.9984, dtype=torch.float64), tensor(0.9985, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0363, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0379, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9805, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0009, dtype=torch.float64)Grad:  tensor(3.5199, dtype=torch.float64)Grad:  tensor(0.3347, dtype=torch.float64)Grad:  tensor(0.0965, dtype=torch.float64)Grad:  tensor(0.0630, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 22 was 98.0%.\n",
      "current params: [tensor(215.8394, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.7708, dtype=torch.float64), tensor(0.9984, dtype=torch.float64), tensor(0.9984, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0365, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0382, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9804, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0019, dtype=torch.float64)Grad:  tensor(7.2528, dtype=torch.float64)Grad:  tensor(0.5269, dtype=torch.float64)Grad:  tensor(0.0908, dtype=torch.float64)Grad:  tensor(0.0615, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 23 was 98.0%.\n",
      "current params: [tensor(212.8231, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.7678, dtype=torch.float64), tensor(0.9984, dtype=torch.float64), tensor(0.9984, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0368, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0386, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9803, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(1.0604e-06, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0009, dtype=torch.float64)Grad:  tensor(3.5110, dtype=torch.float64)Grad:  tensor(2.9231, dtype=torch.float64)Grad:  tensor(0.0884, dtype=torch.float64)Grad:  tensor(0.0592, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 24 was 98.0%.\n",
      "current params: [tensor(210.4540, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.7597, dtype=torch.float64), tensor(0.9983, dtype=torch.float64), tensor(0.9983, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0371, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0390, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9803, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0011, dtype=torch.float64)Grad:  tensor(4.1541, dtype=torch.float64)Grad:  tensor(2.2819, dtype=torch.float64)Grad:  tensor(0.0892, dtype=torch.float64)Grad:  tensor(0.0599, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 25 was 98.0%.\n",
      "current params: [tensor(208.2568, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.7506, dtype=torch.float64), tensor(0.9983, dtype=torch.float64), tensor(0.9983, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0373, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0394, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9803, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad:  tensor(0.0008, dtype=torch.float64)Grad:  tensor(3.0892, dtype=torch.float64)Grad:  tensor(0.4744, dtype=torch.float64)Grad:  tensor(0.0961, dtype=torch.float64)Grad:  tensor(0.0627, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 26 was 98.0%.\n",
      "current params: [tensor(206.4060, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.7450, dtype=torch.float64), tensor(0.9982, dtype=torch.float64), tensor(0.9982, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0375, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0398, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9802, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0020, dtype=torch.float64)Grad:  tensor(7.5264, dtype=torch.float64)Grad:  tensor(1.0508, dtype=torch.float64)Grad:  tensor(0.0879, dtype=torch.float64)Grad:  tensor(0.0603, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 27 was 98.0%.\n",
      "current params: [tensor(203.7115, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.7398, dtype=torch.float64), tensor(0.9982, dtype=torch.float64), tensor(0.9982, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0378, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0402, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9803, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0016, dtype=torch.float64)Grad:  tensor(6.0754, dtype=torch.float64)Grad:  tensor(1.1371, dtype=torch.float64)Grad:  tensor(0.0894, dtype=torch.float64)Grad:  tensor(0.0606, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 28 was 98.0%.\n",
      "current params: [tensor(200.9643, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.7348, dtype=torch.float64), tensor(0.9981, dtype=torch.float64), tensor(0.9981, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0381, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0406, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9802, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0011, dtype=torch.float64)Grad:  tensor(4.1148, dtype=torch.float64)Grad:  tensor(0.2691, dtype=torch.float64)Grad:  tensor(0.0947, dtype=torch.float64)Grad:  tensor(0.0625, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 29 was 98.0%.\n",
      "current params: [tensor(198.6468, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.7316, dtype=torch.float64), tensor(0.9981, dtype=torch.float64), tensor(0.9981, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0384, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0409, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9805, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0009, dtype=torch.float64)Grad:  tensor(3.3009, dtype=torch.float64)Grad:  tensor(0.4316, dtype=torch.float64)Grad:  tensor(0.0953, dtype=torch.float64)Grad:  tensor(0.0626, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 30 was 98.0%.\n",
      "current params: [tensor(196.7316, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.7291, dtype=torch.float64), tensor(0.9981, dtype=torch.float64), tensor(0.9981, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0386, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0412, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9802, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0007, dtype=torch.float64)Grad:  tensor(2.8092, dtype=torch.float64)Grad:  tensor(0.4354, dtype=torch.float64)Grad:  tensor(0.0958, dtype=torch.float64)Grad:  tensor(0.0626, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 31 was 98.0%.\n",
      "current params: [tensor(195.1293, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.7268, dtype=torch.float64), tensor(0.9980, dtype=torch.float64), tensor(0.9980, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0388, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0414, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9805, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0011, dtype=torch.float64)Grad:  tensor(3.9655, dtype=torch.float64)Grad:  tensor(0.3586, dtype=torch.float64)Grad:  tensor(0.0942, dtype=torch.float64)Grad:  tensor(0.0624, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 32 was 98.0%.\n",
      "current params: [tensor(193.4208, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.7249, dtype=torch.float64), tensor(0.9980, dtype=torch.float64), tensor(0.9980, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0390, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0417, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9803, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0008, dtype=torch.float64)Grad:  tensor(3.0445, dtype=torch.float64)Grad:  tensor(0.5135, dtype=torch.float64)Grad:  tensor(0.0950, dtype=torch.float64)Grad:  tensor(0.0624, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 33 was 98.0%.\n",
      "current params: [tensor(191.8703, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.7228, dtype=torch.float64), tensor(0.9979, dtype=torch.float64), tensor(0.9980, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0392, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0419, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9805, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0013, dtype=torch.float64)Grad:  tensor(5.0007, dtype=torch.float64)Grad:  tensor(0.7384, dtype=torch.float64)Grad:  tensor(0.0911, dtype=torch.float64)Grad:  tensor(0.0613, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 34 was 98.0%.\n",
      "current params: [tensor(189.9617, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.7201, dtype=torch.float64), tensor(0.9979, dtype=torch.float64), tensor(0.9979, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0394, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0422, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9805, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad:  tensor(0.0010, dtype=torch.float64)Grad:  tensor(3.6747, dtype=torch.float64)Grad:  tensor(0.8210, dtype=torch.float64)Grad:  tensor(0.0926, dtype=torch.float64)Grad:  tensor(0.0616, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 35 was 98.0%.\n",
      "current params: [tensor(188.1770, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.7169, dtype=torch.float64), tensor(0.9979, dtype=torch.float64), tensor(0.9979, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0397, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0425, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9801, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0008, dtype=torch.float64)Grad:  tensor(2.8649, dtype=torch.float64)Grad:  tensor(1.4429, dtype=torch.float64)Grad:  tensor(0.0915, dtype=torch.float64)Grad:  tensor(0.0607, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 36 was 98.0%.\n",
      "current params: [tensor(186.6368, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.7120, dtype=torch.float64), tensor(0.9978, dtype=torch.float64), tensor(0.9978, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0399, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0428, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9802, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0009, dtype=torch.float64)Grad:  tensor(3.2997, dtype=torch.float64)Grad:  tensor(0.7887, dtype=torch.float64)Grad:  tensor(0.0930, dtype=torch.float64)Grad:  tensor(0.0616, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 37 was 98.0%.\n",
      "current params: [tensor(185.1206, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.7078, dtype=torch.float64), tensor(0.9978, dtype=torch.float64), tensor(0.9978, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0401, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0430, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9803, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0017, dtype=torch.float64)Grad:  tensor(6.3760, dtype=torch.float64)Grad:  tensor(0.5316, dtype=torch.float64)Grad:  tensor(0.0889, dtype=torch.float64)Grad:  tensor(0.0609, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 38 was 98.0%.\n",
      "current params: [tensor(182.9485, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.7045, dtype=torch.float64), tensor(0.9978, dtype=torch.float64), tensor(0.9978, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0403, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0434, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9804, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0019, dtype=torch.float64)Grad:  tensor(6.9923, dtype=torch.float64)Grad:  tensor(3.3553, dtype=torch.float64)Grad:  tensor(0.0770, dtype=torch.float64)Grad:  tensor(0.0557, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 39 was 98.0%.\n",
      "current params: [tensor(180.3497, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6958, dtype=torch.float64), tensor(0.9977, dtype=torch.float64), tensor(0.9977, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0407, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0439, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9800, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0009, dtype=torch.float64)Grad:  tensor(3.2916, dtype=torch.float64)Grad:  tensor(2.5485, dtype=torch.float64)Grad:  tensor(0.0857, dtype=torch.float64)Grad:  tensor(0.0583, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 40 was 98.0%.\n",
      "current params: [tensor(178.3387, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6862, dtype=torch.float64), tensor(0.9977, dtype=torch.float64), tensor(0.9977, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0409, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0444, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9802, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0011, dtype=torch.float64)Grad:  tensor(4.1869, dtype=torch.float64)Grad:  tensor(0.1986, dtype=torch.float64)Grad:  tensor(0.0931, dtype=torch.float64)Grad:  tensor(0.0621, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 41 was 98.0%.\n",
      "current params: [tensor(176.4284, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6810, dtype=torch.float64), tensor(0.9977, dtype=torch.float64), tensor(0.9977, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0412, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0448, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9803, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0018, dtype=torch.float64)Grad:  tensor(6.4428, dtype=torch.float64)Grad:  tensor(0.0531, dtype=torch.float64)Grad:  tensor(0.0895, dtype=torch.float64)Grad:  tensor(0.0614, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 42 was 98.0%.\n",
      "current params: [tensor(174.1075, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6783, dtype=torch.float64), tensor(0.9976, dtype=torch.float64), tensor(0.9976, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0415, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0452, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9800, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0008, dtype=torch.float64)Grad:  tensor(2.9262, dtype=torch.float64)Grad:  tensor(0.5696, dtype=torch.float64)Grad:  tensor(0.0933, dtype=torch.float64)Grad:  tensor(0.0617, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 43 was 97.9%.\n",
      "current params: [tensor(172.3258, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6758, dtype=torch.float64), tensor(0.9976, dtype=torch.float64), tensor(0.9976, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0418, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0455, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9799, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad:  tensor(0.0010, dtype=torch.float64)Grad:  tensor(3.6836, dtype=torch.float64)Grad:  tensor(1.7939, dtype=torch.float64)Grad:  tensor(0.0868, dtype=torch.float64)Grad:  tensor(0.0590, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 44 was 98.0%.\n",
      "current params: [tensor(170.6534, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6708, dtype=torch.float64), tensor(0.9976, dtype=torch.float64), tensor(0.9976, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0420, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0458, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9801, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0014, dtype=torch.float64)Grad:  tensor(5.0753, dtype=torch.float64)Grad:  tensor(1.1827, dtype=torch.float64)Grad:  tensor(0.0866, dtype=torch.float64)Grad:  tensor(0.0596, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 45 was 97.9%.\n",
      "current params: [tensor(168.7490, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6659, dtype=torch.float64), tensor(0.9975, dtype=torch.float64), tensor(0.9975, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0423, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0462, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9799, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0007, dtype=torch.float64)Grad:  tensor(2.6980, dtype=torch.float64)Grad:  tensor(0.1925, dtype=torch.float64)Grad:  tensor(0.0947, dtype=torch.float64)Grad:  tensor(0.0623, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 46 was 97.9%.\n",
      "current params: [tensor(167.2281, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6631, dtype=torch.float64), tensor(0.9975, dtype=torch.float64), tensor(0.9975, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0425, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0465, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9799, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0007, dtype=torch.float64)Grad:  tensor(2.4086, dtype=torch.float64)Grad:  tensor(0.1695, dtype=torch.float64)Grad:  tensor(0.0952, dtype=torch.float64)Grad:  tensor(0.0625, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 47 was 98.0%.\n",
      "current params: [tensor(165.9581, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6613, dtype=torch.float64), tensor(0.9975, dtype=torch.float64), tensor(0.9975, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0427, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0467, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9801, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0017, dtype=torch.float64)Grad:  tensor(6.0469, dtype=torch.float64)Grad:  tensor(2.3729, dtype=torch.float64)Grad:  tensor(0.0789, dtype=torch.float64)Grad:  tensor(0.0566, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 48 was 97.9%.\n",
      "current params: [tensor(164.0608, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6556, dtype=torch.float64), tensor(0.9974, dtype=torch.float64), tensor(0.9974, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0429, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0471, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9797, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(8.6454e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0015, dtype=torch.float64)Grad:  tensor(5.2814, dtype=torch.float64)Grad:  tensor(1.1896, dtype=torch.float64)Grad:  tensor(0.0851, dtype=torch.float64)Grad:  tensor(0.0591, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 49 was 97.9%.\n",
      "current params: [tensor(162.0195, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6504, dtype=torch.float64), tensor(0.9974, dtype=torch.float64), tensor(0.9974, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0433, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0476, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9797, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0008, dtype=torch.float64)Grad:  tensor(2.9688, dtype=torch.float64)Grad:  tensor(2.8932, dtype=torch.float64)Grad:  tensor(0.0815, dtype=torch.float64)Grad:  tensor(0.0564, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 50 was 97.9%.\n",
      "current params: [tensor(160.3843, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6421, dtype=torch.float64), tensor(0.9974, dtype=torch.float64), tensor(0.9974, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0435, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0480, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9800, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0010, dtype=torch.float64)Grad:  tensor(3.4594, dtype=torch.float64)Grad:  tensor(1.5657, dtype=torch.float64)Grad:  tensor(0.0862, dtype=torch.float64)Grad:  tensor(0.0589, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 51 was 97.9%.\n",
      "current params: [tensor(158.8497, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6350, dtype=torch.float64), tensor(0.9974, dtype=torch.float64), tensor(0.9973, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0437, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0484, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9799, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0016, dtype=torch.float64)Grad:  tensor(5.6780, dtype=torch.float64)Grad:  tensor(0.0539, dtype=torch.float64)Grad:  tensor(0.0887, dtype=torch.float64)Grad:  tensor(0.0611, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 52 was 97.9%.\n",
      "current params: [tensor(156.9179, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6313, dtype=torch.float64), tensor(0.9973, dtype=torch.float64), tensor(0.9973, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0440, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0488, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9799, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad:  tensor(0.0006, dtype=torch.float64)Grad:  tensor(2.2496, dtype=torch.float64)Grad:  tensor(0.6317, dtype=torch.float64)Grad:  tensor(0.0924, dtype=torch.float64)Grad:  tensor(0.0612, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 53 was 97.9%.\n",
      "current params: [tensor(155.4894, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6283, dtype=torch.float64), tensor(0.9973, dtype=torch.float64), tensor(0.9973, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0443, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0491, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9800, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0010, dtype=torch.float64)Grad:  tensor(3.5587, dtype=torch.float64)Grad:  tensor(0.9039, dtype=torch.float64)Grad:  tensor(0.0884, dtype=torch.float64)Grad:  tensor(0.0601, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 54 was 97.9%.\n",
      "current params: [tensor(154.0416, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6250, dtype=torch.float64), tensor(0.9973, dtype=torch.float64), tensor(0.9973, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0445, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0494, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9799, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0007, dtype=torch.float64)Grad:  tensor(2.5552, dtype=torch.float64)Grad:  tensor(1.6490, dtype=torch.float64)Grad:  tensor(0.0866, dtype=torch.float64)Grad:  tensor(0.0587, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 55 was 97.9%.\n",
      "current params: [tensor(152.7912, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6201, dtype=torch.float64), tensor(0.9972, dtype=torch.float64), tensor(0.9972, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0447, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0497, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9796, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0013, dtype=torch.float64)Grad:  tensor(4.7589, dtype=torch.float64)Grad:  tensor(1.2403, dtype=torch.float64)Grad:  tensor(0.0839, dtype=torch.float64)Grad:  tensor(0.0585, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 56 was 97.9%.\n",
      "current params: [tensor(151.1885, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6153, dtype=torch.float64), tensor(0.9972, dtype=torch.float64), tensor(0.9972, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0450, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0501, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9798, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0011, dtype=torch.float64)Grad:  tensor(3.7426, dtype=torch.float64)Grad:  tensor(0.1845, dtype=torch.float64)Grad:  tensor(0.0909, dtype=torch.float64)Grad:  tensor(0.0613, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 57 was 97.9%.\n",
      "current params: [tensor(149.6188, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6126, dtype=torch.float64), tensor(0.9972, dtype=torch.float64), tensor(0.9972, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0452, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0504, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9799, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0010, dtype=torch.float64)Grad:  tensor(3.5610, dtype=torch.float64)Grad:  tensor(0.2688, dtype=torch.float64)Grad:  tensor(0.0906, dtype=torch.float64)Grad:  tensor(0.0612, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 58 was 97.9%.\n",
      "current params: [tensor(148.1026, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6107, dtype=torch.float64), tensor(0.9971, dtype=torch.float64), tensor(0.9971, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0455, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0507, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9797, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0015, dtype=torch.float64)Grad:  tensor(5.2580, dtype=torch.float64)Grad:  tensor(0.1173, dtype=torch.float64)Grad:  tensor(0.0876, dtype=torch.float64)Grad:  tensor(0.0607, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 59 was 97.9%.\n",
      "current params: [tensor(146.2733, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6095, dtype=torch.float64), tensor(0.9971, dtype=torch.float64), tensor(0.9971, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0458, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0511, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9799, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0013, dtype=torch.float64)Grad:  tensor(4.5737, dtype=torch.float64)Grad:  tensor(2.4565, dtype=torch.float64)Grad:  tensor(0.0766, dtype=torch.float64)Grad:  tensor(0.0554, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 60 was 97.9%.\n",
      "current params: [tensor(144.4310, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6042, dtype=torch.float64), tensor(0.9971, dtype=torch.float64), tensor(0.9971, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0461, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0515, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9798, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0016, dtype=torch.float64)Grad:  tensor(5.7122, dtype=torch.float64)Grad:  tensor(0.1623, dtype=torch.float64)Grad:  tensor(0.0858, dtype=torch.float64)Grad:  tensor(0.0603, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 61 was 97.9%.\n",
      "current params: [tensor(142.3625, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6013, dtype=torch.float64), tensor(0.9971, dtype=torch.float64), tensor(0.9970, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0465, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0519, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9795, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad:  tensor(0.0014, dtype=torch.float64)Grad:  tensor(4.8575, dtype=torch.float64)Grad:  tensor(0.1858, dtype=torch.float64)Grad:  tensor(0.0872, dtype=torch.float64)Grad:  tensor(0.0605, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 62 was 97.9%.\n",
      "current params: [tensor(140.3572, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5994, dtype=torch.float64), tensor(0.9970, dtype=torch.float64), tensor(0.9970, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0468, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0523, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9794, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0007, dtype=torch.float64)Grad:  tensor(2.5643, dtype=torch.float64)Grad:  tensor(0.2905, dtype=torch.float64)Grad:  tensor(0.0915, dtype=torch.float64)Grad:  tensor(0.0613, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 63 was 97.9%.\n",
      "current params: [tensor(138.8405, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5979, dtype=torch.float64), tensor(0.9970, dtype=torch.float64), tensor(0.9970, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0471, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0526, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9796, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0007, dtype=torch.float64)Grad:  tensor(2.3006, dtype=torch.float64)Grad:  tensor(0.2280, dtype=torch.float64)Grad:  tensor(0.0922, dtype=torch.float64)Grad:  tensor(0.0615, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 64 was 97.9%.\n",
      "current params: [tensor(137.6188, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5967, dtype=torch.float64), tensor(0.9970, dtype=torch.float64), tensor(0.9970, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0473, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0529, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9796, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0007, dtype=torch.float64)Grad:  tensor(2.5780, dtype=torch.float64)Grad:  tensor(2.4041, dtype=torch.float64)Grad:  tensor(0.0791, dtype=torch.float64)Grad:  tensor(0.0557, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 65 was 97.9%.\n",
      "current params: [tensor(136.4878, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5916, dtype=torch.float64), tensor(0.9969, dtype=torch.float64), tensor(0.9969, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0475, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0532, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9794, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0007, dtype=torch.float64)Grad:  tensor(2.4432, dtype=torch.float64)Grad:  tensor(0.3053, dtype=torch.float64)Grad:  tensor(0.0912, dtype=torch.float64)Grad:  tensor(0.0611, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 66 was 97.9%.\n",
      "current params: [tensor(135.4267, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5884, dtype=torch.float64), tensor(0.9969, dtype=torch.float64), tensor(0.9969, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0477, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0535, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9793, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0011, dtype=torch.float64)Grad:  tensor(3.6786, dtype=torch.float64)Grad:  tensor(0.3148, dtype=torch.float64)Grad:  tensor(0.0881, dtype=torch.float64)Grad:  tensor(0.0604, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 67 was 97.9%.\n",
      "current params: [tensor(134.1506, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5862, dtype=torch.float64), tensor(0.9969, dtype=torch.float64), tensor(0.9969, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0480, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0538, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9795, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0013, dtype=torch.float64)Grad:  tensor(4.2799, dtype=torch.float64)Grad:  tensor(2.2250, dtype=torch.float64)Grad:  tensor(0.0751, dtype=torch.float64)Grad:  tensor(0.0549, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 68 was 97.9%.\n",
      "current params: [tensor(132.6469, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5809, dtype=torch.float64), tensor(0.9969, dtype=torch.float64), tensor(0.9968, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0483, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0542, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9792, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0015, dtype=torch.float64)Grad:  tensor(5.1844, dtype=torch.float64)Grad:  tensor(0.1522, dtype=torch.float64)Grad:  tensor(0.0848, dtype=torch.float64)Grad:  tensor(0.0600, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 69 was 97.9%.\n",
      "current params: [tensor(130.8531, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5780, dtype=torch.float64), tensor(0.9968, dtype=torch.float64), tensor(0.9968, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0486, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0546, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9793, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0010, dtype=torch.float64)Grad:  tensor(3.3720, dtype=torch.float64)Grad:  tensor(0.1883, dtype=torch.float64)Grad:  tensor(0.0888, dtype=torch.float64)Grad:  tensor(0.0607, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 70 was 97.9%.\n",
      "current params: [tensor(129.2779, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5761, dtype=torch.float64), tensor(0.9968, dtype=torch.float64), tensor(0.9968, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0489, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0549, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9792, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad:  tensor(0.0013, dtype=torch.float64)Grad:  tensor(4.3693, dtype=torch.float64)Grad:  tensor(0.0338, dtype=torch.float64)Grad:  tensor(0.0869, dtype=torch.float64)Grad:  tensor(0.0606, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 71 was 97.9%.\n",
      "current params: [tensor(127.6134, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5752, dtype=torch.float64), tensor(0.9968, dtype=torch.float64), tensor(0.9968, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0493, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0553, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9794, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0013, dtype=torch.float64)Grad:  tensor(4.2852, dtype=torch.float64)Grad:  tensor(1.1242, dtype=torch.float64)Grad:  tensor(0.0800, dtype=torch.float64)Grad:  tensor(0.0575, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 72 was 97.9%.\n",
      "current params: [tensor(125.9232, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5725, dtype=torch.float64), tensor(0.9968, dtype=torch.float64), tensor(0.9967, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0496, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0557, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9791, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0010, dtype=torch.float64)Grad:  tensor(3.2121, dtype=torch.float64)Grad:  tensor(0.8757, dtype=torch.float64)Grad:  tensor(0.0839, dtype=torch.float64)Grad:  tensor(0.0586, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 73 was 97.9%.\n",
      "current params: [tensor(124.4339, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5695, dtype=torch.float64), tensor(0.9967, dtype=torch.float64), tensor(0.9967, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0499, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0561, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9793, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0008, dtype=torch.float64)Grad:  tensor(2.5408, dtype=torch.float64)Grad:  tensor(3.6454, dtype=torch.float64)Grad:  tensor(0.0672, dtype=torch.float64)Grad:  tensor(0.0506, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 74 was 97.9%.\n",
      "current params: [tensor(123.1788, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5614, dtype=torch.float64), tensor(0.9967, dtype=torch.float64), tensor(0.9967, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0502, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0565, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9793, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0013, dtype=torch.float64)Grad:  tensor(4.4597, dtype=torch.float64)Grad:  tensor(2.8322, dtype=torch.float64)Grad:  tensor(0.0669, dtype=torch.float64)Grad:  tensor(0.0517, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 75 was 97.9%.\n",
      "current params: [tensor(121.6562, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5523, dtype=torch.float64), tensor(0.9967, dtype=torch.float64), tensor(0.9967, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0505, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0571, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9794, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0009, dtype=torch.float64)Grad:  tensor(3.0020, dtype=torch.float64)Grad:  tensor(0.6525, dtype=torch.float64)Grad:  tensor(0.0850, dtype=torch.float64)Grad:  tensor(0.0591, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 76 was 97.9%.\n",
      "current params: [tensor(120.2913, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5466, dtype=torch.float64), tensor(0.9967, dtype=torch.float64), tensor(0.9966, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0508, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0576, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9790, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0010, dtype=torch.float64)Grad:  tensor(3.3866, dtype=torch.float64)Grad:  tensor(0.4585, dtype=torch.float64)Grad:  tensor(0.0850, dtype=torch.float64)Grad:  tensor(0.0593, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 77 was 97.9%.\n",
      "current params: [tensor(118.9268, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5429, dtype=torch.float64), tensor(0.9966, dtype=torch.float64), tensor(0.9966, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0511, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0579, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9791, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0011, dtype=torch.float64)Grad:  tensor(3.7693, dtype=torch.float64)Grad:  tensor(2.3764, dtype=torch.float64)Grad:  tensor(0.0701, dtype=torch.float64)Grad:  tensor(0.0529, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 78 was 97.9%.\n",
      "current params: [tensor(117.4854, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5369, dtype=torch.float64), tensor(0.9966, dtype=torch.float64), tensor(0.9966, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0514, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0584, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9792, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0006, dtype=torch.float64)Grad:  tensor(1.8622, dtype=torch.float64)Grad:  tensor(0.4518, dtype=torch.float64)Grad:  tensor(0.0888, dtype=torch.float64)Grad:  tensor(0.0601, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 79 was 97.9%.\n",
      "current params: [tensor(116.3880, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5331, dtype=torch.float64), tensor(0.9966, dtype=torch.float64), tensor(0.9966, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0517, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0588, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9792, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad:  tensor(0.0007, dtype=torch.float64)Grad:  tensor(2.4008, dtype=torch.float64)Grad:  tensor(2.1037, dtype=torch.float64)Grad:  tensor(0.0750, dtype=torch.float64)Grad:  tensor(0.0543, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 80 was 97.8%.\n",
      "current params: [tensor(115.3517, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5276, dtype=torch.float64), tensor(0.9966, dtype=torch.float64), tensor(0.9965, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0519, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0591, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9788, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0008, dtype=torch.float64)Grad:  tensor(2.5150, dtype=torch.float64)Grad:  tensor(1.7046, dtype=torch.float64)Grad:  tensor(0.0772, dtype=torch.float64)Grad:  tensor(0.0553, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 81 was 97.8%.\n",
      "current params: [tensor(114.3207, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5219, dtype=torch.float64), tensor(0.9965, dtype=torch.float64), tensor(0.9965, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0521, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0595, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9788, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0010, dtype=torch.float64)Grad:  tensor(3.2415, dtype=torch.float64)Grad:  tensor(1.7320, dtype=torch.float64)Grad:  tensor(0.0745, dtype=torch.float64)Grad:  tensor(0.0546, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 82 was 97.9%.\n",
      "current params: [tensor(113.1429, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5160, dtype=torch.float64), tensor(0.9965, dtype=torch.float64), tensor(0.9965, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0524, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0600, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9791, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0007, dtype=torch.float64)Grad:  tensor(2.1578, dtype=torch.float64)Grad:  tensor(0.5078, dtype=torch.float64)Grad:  tensor(0.0866, dtype=torch.float64)Grad:  tensor(0.0594, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 83 was 97.8%.\n",
      "current params: [tensor(112.1118, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5122, dtype=torch.float64), tensor(0.9965, dtype=torch.float64), tensor(0.9965, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0526, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0603, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9790, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0016, dtype=torch.float64)Grad:  tensor(5.0843, dtype=torch.float64)Grad:  tensor(0.2453, dtype=torch.float64)Grad:  tensor(0.0795, dtype=torch.float64)Grad:  tensor(0.0584, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 84 was 97.8%.\n",
      "current params: [tensor(110.5588, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5099, dtype=torch.float64), tensor(0.9965, dtype=torch.float64), tensor(0.9964, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0530, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0607, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9788, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0005, dtype=torch.float64)Grad:  tensor(1.5469, dtype=torch.float64)Grad:  tensor(0.2028, dtype=torch.float64)Grad:  tensor(0.0903, dtype=torch.float64)Grad:  tensor(0.0607, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 85 was 97.8%.\n",
      "current params: [tensor(109.4656, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5084, dtype=torch.float64), tensor(0.9964, dtype=torch.float64), tensor(0.9964, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0533, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0610, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9787, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0007, dtype=torch.float64)Grad:  tensor(2.2497, dtype=torch.float64)Grad:  tensor(0.3103, dtype=torch.float64)Grad:  tensor(0.0871, dtype=torch.float64)Grad:  tensor(0.0598, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 86 was 97.8%.\n",
      "current params: [tensor(108.4558, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5071, dtype=torch.float64), tensor(0.9964, dtype=torch.float64), tensor(0.9964, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0535, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0613, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9786, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(1.9834e-06, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0006, dtype=torch.float64)Grad:  tensor(1.9944, dtype=torch.float64)Grad:  tensor(0.3099, dtype=torch.float64)Grad:  tensor(0.0877, dtype=torch.float64)Grad:  tensor(0.0598, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 87 was 97.8%.\n",
      "current params: [tensor(107.5383, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5059, dtype=torch.float64), tensor(0.9964, dtype=torch.float64), tensor(0.9963, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0538, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0615, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9788, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0006, dtype=torch.float64)Grad:  tensor(1.9223, dtype=torch.float64)Grad:  tensor(3.4548, dtype=torch.float64)Grad:  tensor(0.0624, dtype=torch.float64)Grad:  tensor(0.0484, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 88 was 97.8%.\n",
      "current params: [tensor(106.6807, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.4994, dtype=torch.float64), tensor(0.9964, dtype=torch.float64), tensor(0.9963, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0540, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0619, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9787, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad:  tensor(0.0007, dtype=torch.float64)Grad:  tensor(2.4218, dtype=torch.float64)Grad:  tensor(1.2587, dtype=torch.float64)Grad:  tensor(0.0782, dtype=torch.float64)Grad:  tensor(0.0560, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 89 was 97.8%.\n",
      "current params: [tensor(105.7466, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.4941, dtype=torch.float64), tensor(0.9963, dtype=torch.float64), tensor(0.9963, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0542, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0623, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9787, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0015, dtype=torch.float64)Grad:  tensor(4.9667, dtype=torch.float64)Grad:  tensor(0.2132, dtype=torch.float64)Grad:  tensor(0.0782, dtype=torch.float64)Grad:  tensor(0.0581, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 90 was 97.8%.\n",
      "current params: [tensor(104.2489, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.4911, dtype=torch.float64), tensor(0.9963, dtype=torch.float64), tensor(0.9963, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0546, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0627, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9787, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0011, dtype=torch.float64)Grad:  tensor(3.6165, dtype=torch.float64)Grad:  tensor(0.0506, dtype=torch.float64)Grad:  tensor(0.0836, dtype=torch.float64)Grad:  tensor(0.0595, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 91 was 97.8%.\n",
      "current params: [tensor(102.7496, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.4895, dtype=torch.float64), tensor(0.9963, dtype=torch.float64), tensor(0.9962, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0550, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0631, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9787, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0006, dtype=torch.float64)Grad:  tensor(1.7619, dtype=torch.float64)Grad:  tensor(0.3461, dtype=torch.float64)Grad:  tensor(0.0871, dtype=torch.float64)Grad:  tensor(0.0596, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 92 was 97.8%.\n",
      "current params: [tensor(101.6318, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.4881, dtype=torch.float64), tensor(0.9963, dtype=torch.float64), tensor(0.9962, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0553, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0634, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9785, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0010, dtype=torch.float64)Grad:  tensor(3.1879, dtype=torch.float64)Grad:  tensor(0.2420, dtype=torch.float64)Grad:  tensor(0.0827, dtype=torch.float64)Grad:  tensor(0.0589, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 93 was 97.8%.\n",
      "current params: [tensor(100.4058, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.4869, dtype=torch.float64), tensor(0.9962, dtype=torch.float64), tensor(0.9962, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0556, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0638, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9786, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0005, dtype=torch.float64)Grad:  tensor(1.4509, dtype=torch.float64)Grad:  tensor(0.1770, dtype=torch.float64)Grad:  tensor(0.0891, dtype=torch.float64)Grad:  tensor(0.0603, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 94 was 97.8%.\n",
      "current params: [tensor(99.4883, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.4861, dtype=torch.float64), tensor(0.9962, dtype=torch.float64), tensor(0.9962, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0558, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0640, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9786, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0009, dtype=torch.float64)Grad:  tensor(2.8632, dtype=torch.float64)Grad:  tensor(3.0049, dtype=torch.float64)Grad:  tensor(0.0585, dtype=torch.float64)Grad:  tensor(0.0478, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 95 was 97.8%.\n",
      "current params: [tensor(98.4265, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.4806, dtype=torch.float64), tensor(0.9962, dtype=torch.float64), tensor(0.9961, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0561, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0645, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9784, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0005, dtype=torch.float64)Grad:  tensor(1.6751, dtype=torch.float64)Grad:  tensor(0.1172, dtype=torch.float64)Grad:  tensor(0.0885, dtype=torch.float64)Grad:  tensor(0.0603, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 96 was 97.8%.\n",
      "current params: [tensor(97.5402, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.4776, dtype=torch.float64), tensor(0.9962, dtype=torch.float64), tensor(0.9961, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0564, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0648, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9783, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0010, dtype=torch.float64)Grad:  tensor(3.0461, dtype=torch.float64)Grad:  tensor(4.1219, dtype=torch.float64)Grad:  tensor(0.0464, dtype=torch.float64)Grad:  tensor(0.0426, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 97 was 97.8%.\n",
      "current params: [tensor(96.4505, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.4696, dtype=torch.float64), tensor(0.9962, dtype=torch.float64), tensor(0.9961, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0567, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0653, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9782, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(2.2179e-06, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad:  tensor(0.0007, dtype=torch.float64)Grad:  tensor(2.3148, dtype=torch.float64)Grad:  tensor(2.4494, dtype=torch.float64)Grad:  tensor(0.0638, dtype=torch.float64)Grad:  tensor(0.0498, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 98 was 97.8%.\n",
      "current params: [tensor(95.4129, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.4616, dtype=torch.float64), tensor(0.9961, dtype=torch.float64), tensor(0.9961, dtype=torch.float64)]\n",
      "Dimer Max:  [tensor(0.0570, dtype=torch.float64, grad_fn=<MulBackward0>), tensor(0.0658, dtype=torch.float64, grad_fn=<MulBackward0>)]\n",
      "tensor(0.9784, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0015, dtype=torch.float64)Grad:  tensor(4.6281, dtype=torch.float64)Grad:  tensor(0.0825, dtype=torch.float64)Grad:  tensor(0.0769, dtype=torch.float64)Grad:  tensor(0.0580, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 99 was 97.8%.\n",
      "optimization complete\n",
      "Final params: [tensor(95.4129, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.4616, dtype=torch.float64), tensor(0.9961, dtype=torch.float64), tensor(0.9961, dtype=torch.float64)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<KineticAssembly_AD.vectorized_rxn_net.VectorizedRxnNet at 0x7f673aa5a748>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runtime=10\n",
    "vec_rn.reset(reset_params=True)\n",
    "optim = Optimizer(reaction_network=vec_rn,\n",
    "                  sim_runtime=runtime,\n",
    "                  optim_iterations=100,\n",
    "                  learning_rate=[2e-3,1e-8,2e-3,1e-5,1e-5],\n",
    "                  device='cpu',method=\"RMSprop\",mom=0.5)\n",
    "optim.rn.update_reaction_net(rn)\n",
    "optim.optimize(conc_scale=1e-1,mod_factor=1.0,conc_thresh=1e-1,mod_bool=True,yield_species=15,max_yield=0,chap_mode=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optim.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optim.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optim.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_var(v1,v2):\n",
    "    sq_sum=0\n",
    "    for i in range(len(v1)):\n",
    "        sq_sum=(v1[i]-v2[i])**2+sq_sum\n",
    "    \n",
    "    sq_sum = ((sq_sum)**0.5)/(len(v1)-1)\n",
    "    return(sq_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-4ab4344d8560>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mn_rates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn_copies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_yields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0myields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_yields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mab_yields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdimer_max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mabt_yields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchap_max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "yields= []\n",
    "ab_yields=[]\n",
    "abt_yields=[]\n",
    "final_copies=[]\n",
    "final_rates = []\n",
    "final_t50 = []\n",
    "final_t85 = []\n",
    "final_t95 = []\n",
    "final_t99 = []\n",
    "times = []\n",
    "n_copies=len(rn.chap_uid_map.keys())\n",
    "n_rates = 2*n_copies\n",
    "for i in range(len(optim.final_yields)):\n",
    "    yields.append(optim.final_yields[i][0].item())\n",
    "    ab_yields.append(optim.dimer_max[i].item())\n",
    "    abt_yields.append(optim.chap_max[i].item())\n",
    "    times.append(optim.endtimes[i])\n",
    "    rate_params = []\n",
    "    copy_params = []\n",
    "    for j in range(n_copies):\n",
    "#     print(optim.final_solns[i])\n",
    "        copy_params.append(optim.final_solns[i][j].numpy())\n",
    "    for r in range(n_rates):\n",
    "        rate_params.append(optim.final_solns[i][r+n_copies].numpy())\n",
    "    \n",
    "    final_copies.append(copy_params)\n",
    "    final_rates.append(rate_params)\n",
    "    \n",
    "    if type(optim.final_t50[i])==int:\n",
    "        final_t50.append(1) \n",
    "    else:\n",
    "        final_t50.append(optim.final_t50[i].item()) \n",
    "    if type(optim.final_t85[i])==int:\n",
    "        final_t85.append(1) \n",
    "    else:\n",
    "        final_t85.append(optim.final_t85[i].item()) \n",
    "    if type(optim.final_t95[i])==int:\n",
    "        final_t95.append(1)\n",
    "    else:\n",
    "        final_t95.append(optim.final_t95[i].item())\n",
    "    if type(optim.final_t99[i])==int:\n",
    "        final_t99.append(1)\n",
    "    else:\n",
    "        final_t99.append(optim.final_t99[i].item())\n",
    "\n",
    "sort_indx=np.argsort(np.array(yields))\n",
    "\n",
    "\n",
    "# unsorted_yields=np.array(yields)\n",
    "# unsorted_copies = np.array(final_copies)\n",
    "# unsorted_rates = np.array(final_rates)\n",
    "\n",
    "# mask = unsorted_yields>0.2\n",
    "\n",
    "sorted_yields=np.array(yields)#[sort_indx]\n",
    "sorted_ab_yields=np.array(ab_yields)\n",
    "sorted_abt_yields=np.array(abt_yields)\n",
    "sorted_times = np.array(times)\n",
    "sorted_copies = np.array(final_copies)#[sort_indx]\n",
    "sorted_rates = np.array(final_rates)#[sort_indx]\n",
    "sorted_t50 = np.array(final_t50)\n",
    "sorted_t85 = np.array(final_t85)\n",
    "sorted_t95 = np.array(final_t95)\n",
    "sorted_t99 = np.array(final_t99)\n",
    "\n",
    "print(\"Parameters with Max yield: \")\n",
    "print(\"Max Yield: \",sorted_yields[-1],\"\\nParams: \",sorted_copies[-1], sorted_rates[-1])\n",
    "print(\"Iteration: \", sort_indx[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(optim.final_yielssds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Writing all solutions to a file\n",
    "# filename=\"Tetramer_Solutions_Chaperone_20kT_Mode2\"\n",
    "# with open(filename,'a') as fl:\n",
    "#     fl.write(\"#! Yield\\tAB_Yield\\tABT_Yield\\tTimes\\tC_x\\tk1\\tk2\\tt50\\tt85\\tt90\\n\")\n",
    "#     for i in range(1,len(sorted_yields)):\n",
    "#         fl.write(\"%f\" %(sorted_yields[i]))\n",
    "#         fl.write(\"\\t%f\" %(sorted_ab_yields[i]))\n",
    "#         fl.write(\"\\t%f\" %(sorted_abt_yields[i]))\n",
    "#         fl.write(\"\\t%f\" %(sorted_times[i]))\n",
    "        \n",
    "#         for j in range(sorted_copies[0].shape[0]):         \n",
    "#             fl.write(\"\\t%f\" %(sorted_copies[i][j]))\n",
    "#         for r1 in range(sorted_rates[0].shape[0]):\n",
    "#             fl.write(\"\\t%f\" %(sorted_rates[i][r1]))\n",
    "#         fl.write(\"\\t%f\\t%f\\t%f\\n\" %(sorted_t50[i],sorted_t85[i],sorted_t95[i]))\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_dict = {}\n",
    "sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "from reaction_network import gtostr\n",
    "for n in rn.network.nodes():\n",
    "    #print(n)\n",
    "    #print(rn.network.nodes()[n])\n",
    "    for k,v in rn.network[n].items():\n",
    "        uid = v['uid']\n",
    "        r1 = set(gtostr(rn.network.nodes[n]['struct']))\n",
    "        p = set(gtostr(rn.network.nodes[k]['struct']))\n",
    "        r2 = p-r1\n",
    "        reactants = (r1,r2)\n",
    "        uid_val = {'reactants':reactants,'kon':v['k_on'],'score':v['rxn_score'],'koff':v['k_off']}\n",
    "        if uid not in uid_dict.keys():\n",
    "            uid_dict[uid] = uid_val\n",
    "    print(gtostr(rn.network.nodes[n]['struct']))\n",
    "    #for r_set in rn.get_reactant_sets(n):\n",
    "    #    print(tuple(r_set))\n",
    "    #print(rn.network[n]['struct'])\n",
    "ind_sort = np.argsort(vec_rn.kon.detach().numpy())\n",
    "for i in ind_sort:\n",
    "    print(vec_rn.kon[i])\n",
    "    print(uid_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uid_dict = {}\n",
    "sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "from reaction_network import gtostr\n",
    "from torch import DoubleTensor as Tensor\n",
    "\n",
    "def get_max_edge(n):\n",
    "    \"\"\"\n",
    "    Calculates the max rate (k_on) for a given node\n",
    "    To find out the maximum flow path to the final complex starting from the current node.\n",
    "    \n",
    "    Can also calculate the total rate of consumption of a node by summing up all rates. \n",
    "    Can tell which component is used quickly.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        edges = rn.network.out_edges(n)\n",
    "        #Loop over all edges\n",
    "        #Get attributes\n",
    "        if len(edges)==0:\n",
    "            return(False)\n",
    "        kon_max = -1\n",
    "        next_node = -1\n",
    "        \n",
    "        kon_sum = 0\n",
    "        for edge in edges:\n",
    "            data = rn.network.get_edge_data(edge[0],edge[1])\n",
    "            #print(data)\n",
    "            #Get uid\n",
    "            uid = data['uid']\n",
    "            #Get updated kon\n",
    "            temp_kon = vec_rn.kon[uid]\n",
    "            kon_sum+=temp_kon\n",
    "            \n",
    "#             #Calculate k_off also\n",
    "#             std_c = Tensor([1.])\n",
    "#             l_kon = torch.log(temp_kon)\n",
    "#             l_koff = (vec_rn.rxn_score_vec[uid] * 1. / (self._R * self._T)) + l_kon + torch.log(std_c)\n",
    "            if temp_kon > kon_max:\n",
    "                kon_max = temp_kon\n",
    "                next_node=edge[1]\n",
    "        return(kon_max,next_node,kon_sum)\n",
    "    except Exception as err:\n",
    "        raise(err)\n",
    "\n",
    "pathway = []\n",
    "kon_sumarray = []\n",
    "total_con_rate = {}\n",
    "for n in rn.network.nodes():\n",
    "    \n",
    "    n_str = gtostr(rn.network.nodes[n]['struct']) \n",
    "    \n",
    "    paths = [n_str]\n",
    "    kon_sum = 0\n",
    "    temp_node = n\n",
    "    max_edge = True\n",
    "    consumption_rate = 0\n",
    "    if n < len(rn.network.nodes()):#num_monomers:\n",
    "#         print(\"Current node: \")\n",
    "#         print(n_str)\n",
    "        while max_edge:\n",
    "            max_edge = get_max_edge(temp_node)\n",
    "            if max_edge:\n",
    "                total_con_rate[gtostr(rn.network.nodes[temp_node]['struct'])] = max_edge[2]\n",
    "                temp_node = max_edge[1]\n",
    "                kon_sum += max_edge[0].item()\n",
    "                \n",
    "#                 print(\"Next node: \")\n",
    "#                 print(temp_node)\n",
    "\n",
    "                paths.append(gtostr(rn.network.nodes[temp_node]['struct']))\n",
    "            else:\n",
    "                break\n",
    "        pathway.append(paths)\n",
    "        kon_sumarray.append(kon_sum)\n",
    "        paths=[]\n",
    "\n",
    "print(pathway)\n",
    "print(kon_sumarray)\n",
    "#print(total_con_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in sorted(total_con_rate.items(),key=lambda x : x[1]):\n",
    "    print(k,\" : \", v.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's first visualize some of the data.\n",
    "\n",
    "**Without any optimization**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nodes_list = ['A','B','S','M','AB','BMS','ABS','AMS','ABMS','AM','AS']\n",
    "#nodes_list = ['A','B','ABMS']\n",
    "optim.plot_observable(0,nodes_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**After 750 optimization iterations**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optim.plot_observable(-1,nodes_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.plot_yield()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It seems like we've found a stable solution that produces greater yield than equilibrium. This should be thermodynamically\n",
    "impossible. Let's try to find an explanation. We'll run simulations using the learned optimal parameters at a few different\n",
    "timescales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "optim_rn = optim.rn\n",
    "for i, runtime in enumerate([1, 8, 64]):\n",
    "    optim_rn.reset()\n",
    "    sim = VecSim(optim_rn, runtime, device='cpu')\n",
    "    y = sim.simulate()\n",
    "    sim.plot_observable(nodes_list,ax=ax[i],)\n",
    "    ax[i].set_title(\"runtime: \" + str(runtime) + \" seconds\")\n",
    "fig.set_size_inches(18, 6)\n",
    "node_map = {}\n",
    "for node in rn.network.nodes():\n",
    "    node_map[gtostr(rn.network.nodes[node]['struct'])] = node\n",
    "\n",
    "print(node_map)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_map = {}\n",
    "for node in rn.network.nodes():\n",
    "    node_map[gtostr(rn.network.nodes[node]['struct'])] = node\n",
    "\n",
    "print(node_map)\n",
    "def get_max_edge(n):\n",
    "    \"\"\"\n",
    "    Calculates the max rate (k_on) for a given node\n",
    "    To find out the maximum flow path to the final complex starting from the current node.\n",
    "    \n",
    "    Can also calculate the total rate of consumption of a node by summing up all rates. \n",
    "    Can tell which component is used quickly.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        edges = rn.network.out_edges(n)\n",
    "        #Loop over all edges\n",
    "        #Get attributes\n",
    "        kon_max = -1\n",
    "        next_node = -1\n",
    "\n",
    "        kon_sum = 0\n",
    "        total_flux_outedges = 0\n",
    "        total_flux_inedges = 0\n",
    "        if len(edges)==0:\n",
    "            return(False)\n",
    "            \n",
    "        for edge in edges:\n",
    "            data = rn.network.get_edge_data(edge[0],edge[1])\n",
    "            #print(data)\n",
    "            #Get uid\n",
    "            uid = data['uid']\n",
    "\n",
    "            #Get updated kon\n",
    "            temp_kon = vec_rn.kon[uid]\n",
    "            kon_sum+=temp_kon\n",
    "            \n",
    "            if temp_kon > kon_max:\n",
    "                kon_max = temp_kon\n",
    "                next_node=edge[1]\n",
    "             \n",
    "        return(kon_max,next_node,kon_sum)\n",
    "    except Exception as err:\n",
    "        raise(err)\n",
    "\n",
    "        \n",
    "def get_node_flux(n):\n",
    "    total_flux_outedges = 0\n",
    "    total_flux_inedges = 0\n",
    "    #Go over all the out edges\n",
    "    edges_out = rn.network.out_edges(n)\n",
    "    if len(edges_out)>0:\n",
    "\n",
    "        for edge in edges_out:\n",
    "            data = rn.network.get_edge_data(edge[0],edge[1])\n",
    "            #print(data)\n",
    "            #Get uid\n",
    "            uid = data['uid']\n",
    "\n",
    "            #Get updated kon\n",
    "            temp_kon = vec_rn.kon[uid]\n",
    "\n",
    "            #Calculate k_off also\n",
    "            std_c = Tensor([1.])\n",
    "            l_kon = torch.log(temp_kon)\n",
    "            l_koff = (vec_rn.rxn_score_vec[uid] * 1. / (vec_rn._R * vec_rn._T)) + l_kon + torch.log(std_c)\n",
    "            koff = torch.exp(l_koff)\n",
    "\n",
    "            #Getting conc. of reactants and products\n",
    "            #Get product\n",
    "            prod = gtostr(rn.network.nodes[edge[1]]['struct']) \n",
    "            #Get other reactant\n",
    "            react = \"\".join(sorted(list(set(prod) - set(gtostr(rn.network.nodes[edge[0]]['struct']) ))))\n",
    "\n",
    "            #Net flux from this edge = Generation - consumption\n",
    "            edge_flux = koff*vec_rn.copies_vec[edge[1]] - temp_kon*(vec_rn.copies_vec[edge[0]])*(vec_rn.copies_vec[node_map[react]])\n",
    "            #edge_flux = koff*vec_rn.copies_vec[edge[1]] \n",
    "\n",
    "            print(\"Reaction: \", gtostr(rn.network.nodes[edge[0]]['struct']), \"+\",react,\" -> \",prod)\n",
    "            print(\"Net flux: \",edge_flux)\n",
    "            print(\"kon : \",temp_kon)\n",
    "            print(\"koff: \",koff)\n",
    "            print(\"Reaction data OUTWARD: \")\n",
    "            print(data)\n",
    "\n",
    "            total_flux_outedges+=edge_flux\n",
    "    \n",
    "    #Now go over all the in edges\n",
    "    edges_in = rn.network.in_edges(n)\n",
    "    react_list = []\n",
    "    if len(edges_in) > 0:\n",
    "        for edge in edges_in:\n",
    "            if edge[0] in react_list:\n",
    "                continue\n",
    "            data = rn.network.get_edge_data(edge[0],edge[1])\n",
    "            uid = data['uid']\n",
    "\n",
    "\n",
    "            #Get generation rates; which would be kon\n",
    "            temp_kon = vec_rn.kon[uid]\n",
    "\n",
    "            #Get consumption rates; which is k_off\n",
    "            std_c = Tensor([1.])\n",
    "            l_kon = torch.log(temp_kon)\n",
    "            l_koff = (vec_rn.rxn_score_vec[uid] * 1. / (vec_rn._R * vec_rn._T)) + l_kon + torch.log(std_c)\n",
    "            koff = torch.exp(l_koff)\n",
    "\n",
    "            #Get conc. of reactants and products\n",
    "            prod = gtostr(rn.network.nodes[edge[1]]['struct'])\n",
    "            #Get other reactant\n",
    "            react = \"\".join(sorted(list(set(prod) - set(gtostr(rn.network.nodes[edge[0]]['struct']) ))))\n",
    "            react_list.append(node_map[react])\n",
    "            #Net flux from this edge = Generation - consumption\n",
    "            edge_flux_in = temp_kon*(vec_rn.copies_vec[edge[0]])*(vec_rn.copies_vec[node_map[react]])- koff*vec_rn.copies_vec[edge[1]]\n",
    "            #edge_flux_in = koff*vec_rn.copies_vec[edge[1]]\n",
    "            \n",
    "\n",
    "\n",
    "            print(\"Reaction: \", prod ,\" -> \",gtostr(rn.network.nodes[edge[0]]['struct']), \"+\",react)\n",
    "            print(\"Net flux: \",edge_flux_in)\n",
    "            print(\"kon : \",temp_kon)\n",
    "            print(\"koff: \",koff)\n",
    "            print(\"Raction data INWARD: \")\n",
    "            print(data)\n",
    "\n",
    "            total_flux_inedges+=edge_flux_in\n",
    "    net_node_flux = total_flux_outedges + total_flux_inedges\n",
    "    \n",
    "    return(net_node_flux)\n",
    "    \n",
    "pathway = []\n",
    "kon_sumarray = []\n",
    "total_con_rate = {}\n",
    "net_flux = {}\n",
    "for n in rn.network.nodes():\n",
    "    \n",
    "    n_str = gtostr(rn.network.nodes[n]['struct']) \n",
    "    \n",
    "    paths = [n_str]\n",
    "    kon_sum = 0\n",
    "    temp_node = n\n",
    "    max_edge = True\n",
    "    consumption_rate = 0\n",
    "    if n < len(rn.network.nodes()):#num_monomers:\n",
    "#         print(\"Current node: \")\n",
    "#         print(n_str)\n",
    "        while max_edge:\n",
    "            max_edge = get_max_edge(temp_node)\n",
    "            if max_edge:\n",
    "                total_con_rate[gtostr(rn.network.nodes[temp_node]['struct'])] = max_edge[2]\n",
    "                \n",
    "                temp_node = max_edge[1]\n",
    "                kon_sum += max_edge[0].item()\n",
    "                \n",
    "                \n",
    "#                 print(\"Next node: \")\n",
    "#                 print(temp_node)\n",
    "\n",
    "                paths.append(gtostr(rn.network.nodes[temp_node]['struct']))\n",
    "            else:\n",
    "                break\n",
    "        pathway.append(paths)\n",
    "        kon_sumarray.append(kon_sum)\n",
    "        paths=[]\n",
    "    print(\"-------------------------------------------------------------------------------\")\n",
    "    print(\"-------------------------------------------------------------------------------\")\n",
    "    print(\"|                                                                             |\")\n",
    "    node_flux = get_node_flux(n)\n",
    "    net_flux[gtostr(rn.network.nodes[n]['struct'])] = node_flux\n",
    "    print(\"|                                                                             |\")\n",
    "    print(\"-------------------------------------------------------------------------------\")\n",
    "    print(\"-------------------------------------------------------------------------------\")\n",
    "\n",
    "print(pathway)\n",
    "print(kon_sumarray)\n",
    "\n",
    "#print(total_con_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in sorted(net_flux.items(),key=lambda x : x[1]):\n",
    "    print(k,\" : \", v)\n",
    "\n",
    "print(vec_rn.copies_vec)\n",
    "print(vec_rn.kon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(solution)\n",
    "poly_system = EquilibriumSolver(rn)\n",
    "solution = poly_system.solve(init_val=vec_rn.copies_vec.detach().numpy().tolist())\n",
    "#solution = poly_system.solve(verifyBool = False)\n",
    "if solution == None:\n",
    "    print(\"No Equilibrium solution\")\n",
    "else:\n",
    "    print(solution)\n",
    "    print(\"Equilibrium expected yield: \", 100 * solution[-1] / min(vec_rn.initial_copies[:vec_rn.num_monomers]), '%')\n",
    "print(vec_rn.kon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the equilibrium reached by the system still matches the equilibrium solution. We have however found a set of parameters that can increase available complete AP2 at some point before equilibrium to levels significantly higher than at equilibrium. We don't observe any trapping, but have uncovered an interesting effect. \n",
    "\n",
    "Now we'll move on to looking at ARP23. This is 7 subunits, which drastically increases the number of possible reactions. Expect longer runtimes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
