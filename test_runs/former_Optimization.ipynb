{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Framework using auto-diff to predict binding rates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# make sure jupyter path is correct for loading local moudules\n",
    "import sys\n",
    "# path to steric_simulator module relative to notebook\n",
    "sys.path.append(\"../../\")\n",
    "import copy\n",
    "from KineticAssembly_AD import ReactionNetwork, VectorizedRxnNet, VecSim, Optimizer, EquilibriumSolver\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch import DoubleTensor as Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Reaction Network\n",
    "#### Read the corresponding input file and call the ReactionNetwork class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['default_assoc', 1.0]\n",
      "New node added - Node index: 4 ; Node label: AB \n",
      "New node added - Node index: 5 ; Node label: AD \n",
      "Trying internal bonds\n",
      "New node added - Node index: 6 ; Node label: BC \n",
      "New node added - Node index: 7 ; Node label: ABD \n",
      "Trying internal bonds\n",
      "New node added - Node index: 8 ; Node label: CD \n",
      "New node added - Node index: 9 ; Node label: ABC \n",
      "New node added - Node index: 10 ; Node label: ACD \n",
      "New node added - Node index: 11 ; Node label: ABCD \n",
      "Trying internal bonds\n",
      "New node added - Node index: 12 ; Node label: BCD \n",
      "Trying internal bonds\n",
      "Trying internal bonds\n",
      "Trying internal bonds\n",
      "Trying internal bonds\n",
      "Trying internal bonds\n",
      "Trying internal bonds\n",
      "Trying internal bonds\n",
      "Trying internal bonds\n",
      "Trying internal bonds\n",
      "Trying internal bonds\n",
      "Reaction Network Completed\n"
     ]
    }
   ],
   "source": [
    "base_input = './former.pwr'\n",
    "rn = ReactionNetwork(base_input, one_step=True)\n",
    "rn.resolve_tree()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking reaction network\n",
    "Looping over all network nodes to check if all species are created\n",
    "Creating a dictionary for later reference. This dictionary holds the reactants as keys and values as the reaction index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -- A\n",
      "1 -- B\n",
      "2 -- C\n",
      "3 -- D\n",
      "4 -- AB\n",
      "5 -- AD\n",
      "6 -- BC\n",
      "7 -- ABD\n",
      "8 -- CD\n",
      "9 -- ABC\n",
      "10 -- ACD\n",
      "11 -- ABCD\n",
      "12 -- BCD\n",
      "{(0, 4): 0, (0, 5): 1, (0, 9): 13, (0, 10): 14, (0, 11): 17, (1, 4): 0, (1, 6): 2, (1, 7): 3, (1, 12): 15, (1, 11): 16, (2, 6): 2, (2, 8): 4, (2, 9): 5, (2, 10): 6, (2, 11): 7, (3, 5): 1, (3, 8): 4, (3, 7): 8, (3, 12): 9, (3, 11): 10, (4, 9): 5, (4, 7): 8, (4, 11): 11, (5, 7): 3, (5, 10): 6, (5, 11): 12, (6, 12): 9, (6, 11): 12, (6, 9): 13, (7, 11): 7, (8, 11): 11, (8, 10): 14, (8, 12): 15, (9, 11): 10, (10, 11): 16, (12, 11): 17}\n"
     ]
    }
   ],
   "source": [
    "uid_dict = {}\n",
    "sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "from reaction_network import gtostr\n",
    "for n in rn.network.nodes():\n",
    "    print(n,\"--\",gtostr(rn.network.nodes[n]['struct']))\n",
    "    for k,v in rn.network[n].items():\n",
    "        uid = v['uid']\n",
    "        r1 = set(gtostr(rn.network.nodes[n]['struct']))\n",
    "        p = set(gtostr(rn.network.nodes[k]['struct']))\n",
    "        r2 = p-r1\n",
    "        reactants = (r1,r2)\n",
    "        uid_dict[(n,k)] = uid\n",
    "\n",
    "print(uid_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the initial parameter values \n",
    "For a hetero-trimer there are 6 reaction rates.\n",
    "Also defines the Vectorized Rxn NEt class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 0}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 1}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 13}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 14}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 17}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 0}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 2}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 3}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 15}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 16}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 2}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 4}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 5}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 6}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 7}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 1}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 4}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 8}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 9}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 10}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 5}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 8}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 11}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 3}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 6}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 12}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 9}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 12}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 13}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 7}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 11}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 14}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 15}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 10}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 16}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 17}\n",
      "Reaction rates:  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "dGs:  tensor([-20., -20., -20., -20., -20., -20., -20., -40., -20., -20., -40., -40.,\n",
      "        -40., -20., -20., -20., -40., -40.], dtype=torch.float64)\n",
      "Species Concentrations:  tensor([100., 100., 100., 100.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.], dtype=torch.float64)\n",
      "Shifting to device:  cpu\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "new_kon = torch.zeros([rn._rxn_count], requires_grad=True).double()\n",
    "new_kon = new_kon + Tensor([1])\n",
    "\n",
    "update_kon_dict = {}\n",
    "for edge in rn.network.edges:\n",
    "    update_kon_dict[edge] = new_kon[uid_dict[edge]]\n",
    "\n",
    "nx.set_edge_attributes(rn.network,update_kon_dict,'k_on')\n",
    "for edge in rn.network.edges:\n",
    "    print(rn.network.get_edge_data(edge[0],edge[1]))\n",
    "\n",
    "\n",
    "vec_rn = VectorizedRxnNet(rn, dev='cpu')\n",
    "print(vec_rn.kon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "B\n",
      "C\n",
      "D\n",
      "AB\n",
      "AD\n",
      "BC\n",
      "ABD\n",
      "CD\n",
      "ABC\n",
      "ACD\n",
      "ABCD\n",
      "BCD\n",
      "tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>)\n",
      "{'reactants': ({'A'}, {'B'}), 'kon': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'score': tensor([-20.], dtype=torch.float64), 'koff': None, 'uid': 0}\n",
      "tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>)\n",
      "{'reactants': ({'B'}, {'C', 'D'}), 'kon': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'score': tensor([-20.], dtype=torch.float64), 'koff': None, 'uid': 15}\n",
      "tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>)\n",
      "{'reactants': ({'A'}, {'C', 'D'}), 'kon': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'score': tensor([-20.], dtype=torch.float64), 'koff': None, 'uid': 14}\n",
      "tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>)\n",
      "{'reactants': ({'A'}, {'C', 'B'}), 'kon': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'score': tensor([-20.], dtype=torch.float64), 'koff': None, 'uid': 13}\n",
      "tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>)\n",
      "{'reactants': ({'A', 'D'}, {'C', 'B'}), 'kon': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'score': tensor([-40.], dtype=torch.float64), 'koff': None, 'uid': 12}\n",
      "tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>)\n",
      "{'reactants': ({'A', 'B'}, {'C', 'D'}), 'kon': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'score': tensor([-40.], dtype=torch.float64), 'koff': None, 'uid': 11}\n",
      "tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>)\n",
      "{'reactants': ({'D'}, {'A', 'C', 'B'}), 'kon': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'score': tensor([-40.], dtype=torch.float64), 'koff': None, 'uid': 10}\n",
      "tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>)\n",
      "{'reactants': ({'D'}, {'C', 'B'}), 'kon': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'score': tensor([-20.], dtype=torch.float64), 'koff': None, 'uid': 9}\n",
      "tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>)\n",
      "{'reactants': ({'D'}, {'A', 'B'}), 'kon': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'score': tensor([-20.], dtype=torch.float64), 'koff': None, 'uid': 8}\n",
      "tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>)\n",
      "{'reactants': ({'C'}, {'A', 'B', 'D'}), 'kon': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'score': tensor([-40.], dtype=torch.float64), 'koff': None, 'uid': 7}\n",
      "tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>)\n",
      "{'reactants': ({'C'}, {'A', 'D'}), 'kon': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'score': tensor([-20.], dtype=torch.float64), 'koff': None, 'uid': 6}\n",
      "tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>)\n",
      "{'reactants': ({'C'}, {'A', 'B'}), 'kon': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'score': tensor([-20.], dtype=torch.float64), 'koff': None, 'uid': 5}\n",
      "tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>)\n",
      "{'reactants': ({'C'}, {'D'}), 'kon': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'score': tensor([-20.], dtype=torch.float64), 'koff': None, 'uid': 4}\n",
      "tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>)\n",
      "{'reactants': ({'B'}, {'A', 'D'}), 'kon': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'score': tensor([-20.], dtype=torch.float64), 'koff': None, 'uid': 3}\n",
      "tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>)\n",
      "{'reactants': ({'B'}, {'C'}), 'kon': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'score': tensor([-20.], dtype=torch.float64), 'koff': None, 'uid': 2}\n",
      "tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>)\n",
      "{'reactants': ({'A'}, {'D'}), 'kon': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'score': tensor([-20.], dtype=torch.float64), 'koff': None, 'uid': 1}\n",
      "tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>)\n",
      "{'reactants': ({'B'}, {'A', 'C', 'D'}), 'kon': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'score': tensor([-40.], dtype=torch.float64), 'koff': None, 'uid': 16}\n",
      "tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>)\n",
      "{'reactants': ({'A'}, {'C', 'B', 'D'}), 'kon': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'score': tensor([-40.], dtype=torch.float64), 'koff': None, 'uid': 17}\n"
     ]
    }
   ],
   "source": [
    "uid_dict = {}\n",
    "sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "from reaction_network import gtostr\n",
    "for n in rn.network.nodes():\n",
    "    #print(n)\n",
    "    #print(rn.network.nodes()[n])\n",
    "    for k,v in rn.network[n].items():\n",
    "        uid = v['uid']\n",
    "        r1 = set(gtostr(rn.network.nodes[n]['struct']))\n",
    "        p = set(gtostr(rn.network.nodes[k]['struct']))\n",
    "        r2 = p-r1\n",
    "        reactants = (r1,r2)\n",
    "        uid_val = {'reactants':reactants,'kon':v['k_on'],'score':v['rxn_score'],'koff':v['k_off'],'uid':uid}\n",
    "        if uid not in uid_dict.keys():\n",
    "            uid_dict[uid] = uid_val\n",
    "    print(gtostr(rn.network.nodes[n]['struct']))\n",
    "    #for r_set in rn.get_reactant_sets(n):\n",
    "    #    print(tuple(r_set))\n",
    "    #print(rn.network[n]['struct'])\n",
    "ind_sort = np.argsort(vec_rn.kon.detach().numpy())\n",
    "for i in ind_sort:\n",
    "    print(vec_rn.kon[i])\n",
    "    print(uid_dict[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Using the optimizer ##\n",
    "\n",
    "### Define an instance of the optimizer class\n",
    "#### Input Arguments:\n",
    "\n",
    "reaction_network : Input the vectorized rxn network\n",
    "\n",
    "sim_runtime: The runtime of the kinetic simulation. Needs to be same as the time over the experimental reaction data.\n",
    "\n",
    "optim_iterations: No. of iterations to run the optimization. Can start at low values(100) and increase depending upon memory usage.\n",
    "\n",
    "learning_rate = The size of the gradient descent step for updating parameter values. Needs to be atleast (1e-3-1e-1)* min{parameter value}. If learning rate is too high, it can take a longer step and sometimes lead to negative value of parameters which is unphysical. Requires some trial runs to find the best value. \n",
    "\n",
    "device: cpu or gpu\n",
    "\n",
    "method: Choose which pytorch based optimized to use for gradient descent - Adam or RMSprop\n",
    "\n",
    "mom: Only for RMSprop method. Use momentum term during gradient descent. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.utils.benchmark as benchmark\n",
    "import time as time_mod\n",
    "\n",
    "t1 = time_mod.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vec_rn.reset(reset_params=True)\n",
    "optim = Optimizer(reaction_network=vec_rn,\n",
    "                  sim_runtime=1,\n",
    "                  optim_iterations=100,\n",
    "                  learning_rate=1e-2,\n",
    "                  device='cpu',method=\"RMSprop\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the optimization method\n",
    "\n",
    "#### Input arguments\n",
    "\n",
    "conc_scale, conc_thresh, mod_bool, mod_factor = As defined for the VecSim class. \n",
    "\n",
    "max_thresh: Max. allowed values of parameters being updated. Beyond this maximum a penalty is imposed on the cost function. (Regularization)\n",
    "\n",
    "max_yield: It is a control variable that is used to store the updated parameter values over all iterations for further analysis. The parameter values are stored only if the current yield exceed this max_yield. \n",
    "\n",
    "yield_species: Yield of the species being optimized(node index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reaction Parameters before optimization: \n",
      "[Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       dtype=torch.float64, requires_grad=True)]\n",
      "Optimizer State: <bound method Optimizer.state_dict of RMSprop (\n",
      "Parameter Group 0\n",
      "    alpha: 0.99\n",
      "    centered: False\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    weight_decay: 0\n",
      ")>\n",
      "Using CPU\n",
      "Yield on sim. iteration 0 was 13.5%.\n",
      "current params: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.1356, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 1 was 15.1%.\n",
      "current params: tensor([1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 1.1000, 0.9000, 1.1000,\n",
      "        1.1000, 0.9000, 0.9000, 0.9000, 1.1000, 1.1000, 1.1000, 0.9000, 0.9000],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.1517, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 2 was 16.3%.\n",
      "current params: tensor([1.1748, 1.1748, 1.1742, 1.1700, 1.1742, 1.1700, 1.1700, 0.8220, 1.1700,\n",
      "        1.1687, 0.8220, 0.8190, 0.8190, 1.1700, 1.1700, 1.1687, 0.8220, 0.8292],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.1632, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 3 was 17.2%.\n",
      "current params: tensor([1.2382, 1.2382, 1.2368, 1.2267, 1.2368, 1.2267, 1.2267, 0.7550, 1.2267,\n",
      "        1.2240, 0.7550, 0.7481, 0.7481, 1.2267, 1.2267, 1.2240, 0.7550, 0.7715],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.1726, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 4 was 18.0%.\n",
      "current params: tensor([1.2945, 1.2945, 1.2922, 1.2756, 1.2922, 1.2756, 1.2756, 0.6949, 1.2756,\n",
      "        1.2715, 0.6950, 0.6832, 0.6832, 1.2756, 1.2756, 1.2715, 0.6950, 0.7216],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.1807, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 5 was 18.8%.\n",
      "current params: tensor([1.3459, 1.3459, 1.3426, 1.3191, 1.3426, 1.3191, 1.3191, 0.6399, 1.3191,\n",
      "        1.3138, 0.6399, 0.6227, 0.6227, 1.3192, 1.3192, 1.3138, 0.6399, 0.6770],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.1880, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 6 was 19.4%.\n",
      "current params: tensor([1.3936, 1.3936, 1.3893, 1.3586, 1.3893, 1.3586, 1.3586, 0.5885, 1.3586,\n",
      "        1.3523, 0.5885, 0.5656, 0.5656, 1.3588, 1.3588, 1.3523, 0.5885, 0.6363],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.1946, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 7 was 20.0%.\n",
      "current params: tensor([1.4383, 1.4383, 1.4330, 1.3951, 1.4330, 1.3951, 1.3951, 0.5402, 1.3951,\n",
      "        1.3878, 0.5402, 0.5112, 0.5112, 1.3954, 1.3954, 1.3878, 0.5402, 0.5987],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2008, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 8 was 20.6%.\n",
      "current params: tensor([1.4805, 1.4805, 1.4743, 1.4290, 1.4743, 1.4291, 1.4291, 0.4942, 1.4290,\n",
      "        1.4209, 0.4942, 0.4587, 0.4587, 1.4295, 1.4295, 1.4209, 0.4942, 0.5635],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2065, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 9 was 21.2%.\n",
      "current params: tensor([1.5206, 1.5206, 1.5134, 1.4609, 1.5134, 1.4610, 1.4610, 0.4502, 1.4609,\n",
      "        1.4521, 0.4502, 0.4079, 0.4079, 1.4615, 1.4615, 1.4521, 0.4502, 0.5303],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2120, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 10 was 21.7%.\n",
      "current params: tensor([1.5588, 1.5588, 1.5507, 1.4910, 1.5507, 1.4911, 1.4911, 0.4078, 1.4910,\n",
      "        1.4817, 0.4078, 0.3583, 0.3583, 1.4918, 1.4918, 1.4817, 0.4078, 0.4988],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2172, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 11 was 22.2%.\n",
      "current params: tensor([1.5955, 1.5955, 1.5865, 1.5195, 1.5865, 1.5196, 1.5196, 0.3670, 1.5195,\n",
      "        1.5099, 0.3669, 0.3098, 0.3098, 1.5205, 1.5205, 1.5099, 0.3669, 0.4688],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2222, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 12 was 22.7%.\n",
      "current params: tensor([1.6308, 1.6308, 1.6209, 1.5466, 1.6209, 1.5468, 1.5468, 0.3273, 1.5466,\n",
      "        1.5370, 0.3273, 0.2620, 0.2620, 1.5480, 1.5480, 1.5370, 0.3273, 0.4401],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2270, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 13 was 23.1%.\n",
      "current params: tensor([1.6648, 1.6648, 1.6540, 1.5725, 1.6540, 1.5727, 1.5727, 0.2887, 1.5725,\n",
      "        1.5629, 0.2887, 0.2146, 0.2146, 1.5742, 1.5742, 1.5629, 0.2887, 0.4125],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2317, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 14 was 23.6%.\n",
      "current params: tensor([1.6977, 1.6977, 1.6859, 1.5973, 1.6859, 1.5975, 1.5975, 0.2509, 1.5973,\n",
      "        1.5879, 0.2509, 0.1674, 0.1674, 1.5994, 1.5994, 1.5879, 0.2509, 0.3859],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2363, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 15 was 24.0%.\n",
      "current params: tensor([1.7295, 1.7295, 1.7168, 1.6210, 1.7168, 1.6212, 1.6212, 0.2140, 1.6210,\n",
      "        1.6121, 0.2140, 0.1199, 0.1199, 1.6235, 1.6235, 1.6121, 0.2140, 0.3602],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2408, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 16 was 24.5%.\n",
      "current params: tensor([1.7603, 1.7603, 1.7467, 1.6438, 1.7467, 1.6440, 1.6440, 0.1776, 1.6438,\n",
      "        1.6356, 0.1777, 0.0717, 0.0717, 1.6467, 1.6467, 1.6356, 0.1777, 0.3352],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2452, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 17 was 24.6%.\n",
      "current params: tensor([1.7902, 1.7902, 1.7757, 1.6655, 1.7757, 1.6658, 1.6658, 0.1418, 1.6655,\n",
      "        1.6583, 0.1418, 0.1717, 0.1717, 1.6691, 1.6691, 1.6583, 0.1418, 0.3111],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2467, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 18 was 25.0%.\n",
      "current params: tensor([1.8197, 1.8197, 1.8043, 1.6872, 1.8043, 1.6875, 1.6875, 0.1076, 1.6872,\n",
      "        1.6804, 0.1078, 0.1715, 0.1715, 1.6914, 1.6914, 1.6804, 0.1078, 0.2872],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2501, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 19 was 25.3%.\n",
      "current params: tensor([1.8487, 1.8487, 1.8324, 1.7083, 1.8324, 1.7087, 1.7087, 0.0742, 1.7083,\n",
      "        1.7019, 0.0743, 0.1713, 0.1713, 1.7131, 1.7131, 1.7019, 0.0743, 0.2639],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2534, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 20 was 24.8%.\n",
      "current params: tensor([1.8770, 1.8770, 1.8598, 1.7289, 1.8598, 1.7293, 1.7293, 0.1742, 1.7289,\n",
      "        1.7229, 0.1743, 0.1711, 0.1711, 1.7343, 1.7343, 1.7229, 0.1743, 0.2411],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2488, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 21 was 24.9%.\n",
      "current params: tensor([1.9059, 1.9059, 1.8883, 1.7501, 1.8883, 1.7504, 1.7504, 0.1741, 1.7501,\n",
      "        1.7430, 0.1742, 0.1709, 0.1709, 1.7556, 1.7556, 1.7430, 0.1742, 0.2187],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2500, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 22 was 25.1%.\n",
      "current params: tensor([1.9344, 1.9344, 1.9166, 1.7710, 1.9166, 1.7712, 1.7712, 0.1740, 1.7710,\n",
      "        1.7627, 0.1741, 0.1707, 0.1707, 1.7765, 1.7765, 1.7627, 0.1741, 0.1967],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2511, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 23 was 25.2%.\n",
      "current params: tensor([1.9625, 1.9625, 1.9446, 1.7916, 1.9446, 1.7918, 1.7918, 0.1739, 1.7916,\n",
      "        1.7819, 0.1740, 0.1706, 0.1706, 1.7971, 1.7971, 1.7819, 0.1740, 0.1752],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2523, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 24 was 25.3%.\n",
      "current params: tensor([1.9903, 1.9903, 1.9723, 1.8120, 1.9723, 1.8122, 1.8122, 0.1738, 1.8120,\n",
      "        1.8006, 0.1739, 0.1704, 0.1704, 1.8174, 1.8174, 1.8006, 0.1739, 0.1540],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2534, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 25 was 25.4%.\n",
      "current params: tensor([2.0177, 2.0177, 1.9997, 1.8321, 1.9997, 1.8323, 1.8323, 0.1736, 1.8321,\n",
      "        1.8189, 0.1738, 0.1702, 0.1702, 1.8374, 1.8374, 1.8189, 0.1738, 0.1332],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2545, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 26 was 25.5%.\n",
      "current params: tensor([2.0447, 2.0447, 2.0268, 1.8520, 2.0268, 1.8522, 1.8522, 0.1735, 1.8520,\n",
      "        1.8369, 0.1737, 0.1700, 0.1700, 1.8571, 1.8571, 1.8369, 0.1737, 0.1127],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2555, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 27 was 25.6%.\n",
      "current params: tensor([2.0715, 2.0715, 2.0536, 1.8717, 2.0536, 1.8718, 1.8718, 0.1734, 1.8717,\n",
      "        1.8545, 0.1736, 0.1698, 0.1698, 1.8765, 1.8765, 1.8545, 0.1736, 0.0926],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2566, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 28 was 25.1%.\n",
      "current params: tensor([2.0979, 2.0979, 2.0801, 1.8912, 2.0801, 1.8913, 1.8913, 0.1733, 1.8912,\n",
      "        1.8718, 0.1735, 0.1696, 0.1696, 1.8956, 1.8956, 1.8718, 0.1735, 0.1925],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2514, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 29 was 25.1%.\n",
      "current params: tensor([2.1250, 2.1250, 2.1071, 1.9109, 2.1071, 1.9109, 1.9109, 0.1732, 1.9109,\n",
      "        1.8888, 0.1734, 0.1695, 0.1695, 1.9153, 1.9153, 1.8888, 0.1734, 0.1917],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2514, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 30 was 25.1%.\n",
      "current params: tensor([2.1518, 2.1518, 2.1338, 1.9304, 2.1338, 1.9305, 1.9305, 0.1731, 1.9304,\n",
      "        1.9056, 0.1733, 0.1693, 0.1693, 1.9348, 1.9348, 1.9056, 0.1733, 0.1909],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2514, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 31 was 25.1%.\n",
      "current params: tensor([2.1784, 2.1784, 2.1603, 1.9498, 2.1603, 1.9498, 1.9498, 0.1730, 1.9498,\n",
      "        1.9221, 0.1732, 0.1691, 0.1691, 1.9541, 1.9541, 1.9221, 0.1732, 0.1902],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2513, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 32 was 25.1%.\n",
      "current params: tensor([2.2047, 2.2047, 2.1865, 1.9690, 2.1865, 1.9691, 1.9691, 0.1729, 1.9690,\n",
      "        1.9384, 0.1731, 0.1689, 0.1689, 1.9733, 1.9733, 1.9384, 0.1731, 0.1894],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2513, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 33 was 25.1%.\n",
      "current params: tensor([2.2307, 2.2307, 2.2124, 1.9880, 2.2124, 1.9882, 1.9882, 0.1728, 1.9880,\n",
      "        1.9544, 0.1730, 0.1687, 0.1687, 1.9923, 1.9923, 1.9544, 0.1730, 0.1887],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2513, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 34 was 25.1%.\n",
      "current params: tensor([2.2565, 2.2565, 2.2381, 2.0070, 2.2381, 2.0071, 2.0071, 0.1727, 2.0070,\n",
      "        1.9702, 0.1729, 0.1685, 0.1685, 2.0112, 2.0112, 1.9702, 0.1729, 0.1879],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2512, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 35 was 25.1%.\n",
      "current params: tensor([2.2820, 2.2820, 2.2634, 2.0258, 2.2634, 2.0260, 2.0260, 0.1726, 2.0258,\n",
      "        1.9858, 0.1728, 0.1684, 0.1684, 2.0300, 2.0300, 1.9858, 0.1728, 0.1872],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2512, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 36 was 25.1%.\n",
      "current params: tensor([2.3071, 2.3071, 2.2885, 2.0444, 2.2885, 2.0447, 2.0447, 0.1725, 2.0444,\n",
      "        2.0011, 0.1727, 0.1682, 0.1682, 2.0486, 2.0486, 2.0011, 0.1727, 0.1864],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2512, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 37 was 25.1%.\n",
      "current params: tensor([2.3321, 2.3321, 2.3133, 2.0629, 2.3133, 2.0632, 2.0632, 0.1724, 2.0629,\n",
      "        2.0163, 0.1726, 0.1680, 0.1680, 2.0671, 2.0671, 2.0163, 0.1726, 0.1857],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2511, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 38 was 25.1%.\n",
      "current params: tensor([2.3567, 2.3567, 2.3379, 2.0813, 2.3379, 2.0817, 2.0817, 0.1723, 2.0813,\n",
      "        2.0312, 0.1725, 0.1678, 0.1678, 2.0855, 2.0855, 2.0312, 0.1725, 0.1850],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2511, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 39 was 25.1%.\n",
      "current params: tensor([2.3811, 2.3811, 2.3622, 2.0996, 2.3622, 2.1000, 2.1000, 0.1722, 2.0996,\n",
      "        2.0460, 0.1724, 0.1676, 0.1676, 2.1037, 2.1037, 2.0460, 0.1724, 0.1842],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2510, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 40 was 25.0%.\n",
      "current params: tensor([2.4052, 2.4052, 2.3862, 2.1177, 2.3862, 2.1182, 2.1182, 0.1721, 2.1177,\n",
      "        2.0607, 0.1723, 0.1674, 0.1674, 2.1218, 2.1218, 2.0607, 0.1723, 0.1835],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2510, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 41 was 25.0%.\n",
      "current params: tensor([2.4290, 2.4290, 2.4099, 2.1357, 2.4099, 2.1363, 2.1363, 0.1720, 2.1357,\n",
      "        2.0751, 0.1722, 0.1672, 0.1672, 2.1398, 2.1398, 2.0751, 0.1722, 0.1828],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2509, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 42 was 25.0%.\n",
      "current params: tensor([2.4526, 2.4526, 2.4334, 2.1536, 2.4334, 2.1542, 2.1542, 0.1719, 2.1536,\n",
      "        2.0894, 0.1721, 0.1670, 0.1670, 2.1577, 2.1577, 2.0894, 0.1721, 0.1821],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2509, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 43 was 25.0%.\n",
      "current params: tensor([2.4759, 2.4759, 2.4567, 2.1713, 2.4567, 2.1720, 2.1720, 0.1718, 2.1713,\n",
      "        2.1035, 0.1720, 0.1668, 0.1668, 2.1754, 2.1754, 2.1035, 0.1720, 0.1814],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2509, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 44 was 25.0%.\n",
      "current params: tensor([2.4990, 2.4990, 2.4797, 2.1890, 2.4797, 2.1898, 2.1898, 0.1718, 2.1890,\n",
      "        2.1175, 0.1719, 0.1666, 0.1666, 2.1930, 2.1930, 2.1175, 0.1719, 0.1807],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2508, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 45 was 25.0%.\n",
      "current params: tensor([2.5218, 2.5218, 2.5025, 2.2065, 2.5025, 2.2074, 2.2074, 0.1717, 2.2065,\n",
      "        2.1314, 0.1718, 0.1664, 0.1664, 2.2106, 2.2106, 2.1314, 0.1718, 0.1800],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2508, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 46 was 25.0%.\n",
      "current params: tensor([2.5444, 2.5444, 2.5250, 2.2239, 2.5250, 2.2249, 2.2249, 0.1716, 2.2239,\n",
      "        2.1451, 0.1717, 0.1662, 0.1662, 2.2280, 2.2280, 2.1451, 0.1717, 0.1793],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2507, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 47 was 25.0%.\n",
      "current params: tensor([2.5668, 2.5668, 2.5473, 2.2412, 2.5473, 2.2423, 2.2423, 0.1715, 2.2412,\n",
      "        2.1587, 0.1716, 0.1660, 0.1660, 2.2453, 2.2453, 2.1587, 0.1716, 0.1786],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2507, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 48 was 25.0%.\n",
      "current params: tensor([2.5889, 2.5889, 2.5694, 2.2584, 2.5694, 2.2596, 2.2596, 0.1714, 2.2584,\n",
      "        2.1722, 0.1715, 0.1658, 0.1658, 2.2625, 2.2625, 2.1722, 0.1715, 0.1779],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2506, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 49 was 25.0%.\n",
      "current params: tensor([2.6108, 2.6108, 2.5913, 2.2755, 2.5913, 2.2768, 2.2768, 0.1713, 2.2755,\n",
      "        2.1855, 0.1714, 0.1656, 0.1656, 2.2796, 2.2796, 2.1855, 0.1714, 0.1772],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2506, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 50 was 25.0%.\n",
      "current params: tensor([2.6326, 2.6326, 2.6129, 2.2925, 2.6129, 2.2939, 2.2939, 0.1712, 2.2925,\n",
      "        2.1987, 0.1713, 0.1654, 0.1654, 2.2966, 2.2966, 2.1987, 0.1713, 0.1765],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2506, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 51 was 25.0%.\n",
      "current params: tensor([2.6541, 2.6541, 2.6344, 2.3094, 2.6344, 2.3109, 2.3109, 0.1711, 2.3094,\n",
      "        2.2118, 0.1712, 0.1652, 0.1652, 2.3135, 2.3135, 2.2118, 0.1712, 0.1759],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2505, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 52 was 25.0%.\n",
      "current params: tensor([2.6754, 2.6754, 2.6556, 2.3262, 2.6556, 2.3277, 2.3277, 0.1710, 2.3262,\n",
      "        2.2249, 0.1711, 0.1650, 0.1650, 2.3303, 2.3303, 2.2249, 0.1711, 0.1752],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2505, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 53 was 25.0%.\n",
      "current params: tensor([2.6965, 2.6965, 2.6766, 2.3429, 2.6766, 2.3445, 2.3445, 0.1709, 2.3429,\n",
      "        2.2378, 0.1710, 0.1648, 0.1648, 2.3470, 2.3470, 2.2378, 0.1710, 0.1745],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2504, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 54 was 25.0%.\n",
      "current params: tensor([2.7174, 2.7174, 2.6975, 2.3595, 2.6975, 2.3612, 2.3612, 0.1708, 2.3595,\n",
      "        2.2506, 0.1709, 0.1646, 0.1646, 2.3636, 2.3636, 2.2506, 0.1709, 0.1738],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2504, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 55 was 25.0%.\n",
      "current params: tensor([2.7381, 2.7381, 2.7182, 2.3760, 2.7182, 2.3779, 2.3779, 0.1707, 2.3760,\n",
      "        2.2633, 0.1708, 0.1644, 0.1644, 2.3801, 2.3801, 2.2633, 0.1708, 0.1732],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2504, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 56 was 25.0%.\n",
      "current params: tensor([2.7586, 2.7586, 2.7387, 2.3925, 2.7387, 2.3944, 2.3944, 0.1706, 2.3925,\n",
      "        2.2759, 0.1707, 0.1642, 0.1642, 2.3965, 2.3965, 2.2759, 0.1707, 0.1725],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2503, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 57 was 25.0%.\n",
      "current params: tensor([2.7790, 2.7790, 2.7590, 2.4088, 2.7590, 2.4108, 2.4108, 0.1705, 2.4088,\n",
      "        2.2884, 0.1706, 0.1640, 0.1640, 2.4129, 2.4129, 2.2884, 0.1706, 0.1718],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2503, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 58 was 25.0%.\n",
      "current params: tensor([2.7991, 2.7991, 2.7791, 2.4251, 2.7791, 2.4272, 2.4272, 0.1704, 2.4251,\n",
      "        2.3008, 0.1705, 0.1638, 0.1638, 2.4291, 2.4291, 2.3008, 0.1705, 0.1712],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2503, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 59 was 25.0%.\n",
      "current params: tensor([2.8191, 2.8191, 2.7991, 2.4412, 2.7991, 2.4434, 2.4434, 0.1703, 2.4412,\n",
      "        2.3132, 0.1705, 0.1636, 0.1636, 2.4453, 2.4453, 2.3132, 0.1705, 0.1705],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2502, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 60 was 25.0%.\n",
      "current params: tensor([2.8390, 2.8390, 2.8189, 2.4573, 2.8189, 2.4596, 2.4596, 0.1702, 2.4573,\n",
      "        2.3254, 0.1704, 0.1634, 0.1634, 2.4614, 2.4614, 2.3254, 0.1704, 0.1698],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2502, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 61 was 25.0%.\n",
      "current params: tensor([2.8587, 2.8587, 2.8385, 2.4733, 2.8385, 2.4757, 2.4757, 0.1702, 2.4733,\n",
      "        2.3376, 0.1703, 0.1631, 0.1631, 2.4774, 2.4774, 2.3376, 0.1703, 0.1692],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2502, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 62 was 25.0%.\n",
      "current params: tensor([2.8782, 2.8782, 2.8580, 2.4892, 2.8580, 2.4917, 2.4917, 0.1701, 2.4892,\n",
      "        2.3497, 0.1702, 0.1629, 0.1629, 2.4933, 2.4933, 2.3497, 0.1702, 0.1685],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2501, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 63 was 25.0%.\n",
      "current params: tensor([2.8975, 2.8975, 2.8773, 2.5050, 2.8773, 2.5077, 2.5077, 0.1700, 2.5050,\n",
      "        2.3618, 0.1701, 0.1627, 0.1627, 2.5092, 2.5092, 2.3618, 0.1701, 0.1679],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2501, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 64 was 25.0%.\n",
      "current params: tensor([2.9167, 2.9167, 2.8965, 2.5208, 2.8965, 2.5235, 2.5235, 0.1699, 2.5208,\n",
      "        2.3737, 0.1700, 0.1625, 0.1625, 2.5249, 2.5249, 2.3737, 0.1700, 0.1672],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2501, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 65 was 25.0%.\n",
      "current params: tensor([2.9358, 2.9358, 2.9155, 2.5365, 2.9155, 2.5393, 2.5393, 0.1698, 2.5365,\n",
      "        2.3856, 0.1699, 0.1623, 0.1623, 2.5406, 2.5406, 2.3856, 0.1699, 0.1665],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2501, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 66 was 25.0%.\n",
      "current params: tensor([2.9547, 2.9547, 2.9344, 2.5521, 2.9344, 2.5550, 2.5550, 0.1697, 2.5521,\n",
      "        2.3974, 0.1698, 0.1621, 0.1621, 2.5562, 2.5562, 2.3974, 0.1698, 0.1659],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2500, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 67 was 25.0%.\n",
      "current params: tensor([2.9735, 2.9735, 2.9531, 2.5676, 2.9531, 2.5707, 2.5707, 0.1696, 2.5676,\n",
      "        2.4092, 0.1697, 0.1618, 0.1618, 2.5718, 2.5718, 2.4092, 0.1697, 0.1652],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2500, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 68 was 25.0%.\n",
      "current params: tensor([2.9922, 2.9922, 2.9717, 2.5831, 2.9717, 2.5862, 2.5862, 0.1695, 2.5831,\n",
      "        2.4209, 0.1696, 0.1616, 0.1616, 2.5873, 2.5873, 2.4209, 0.1696, 0.1646],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2500, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 69 was 24.9%.\n",
      "current params: tensor([3.0107, 3.0107, 2.9902, 2.5985, 2.9902, 2.6017, 2.6017, 0.1694, 2.5985,\n",
      "        2.4325, 0.1695, 0.1614, 0.1614, 2.6027, 2.6027, 2.4325, 0.1695, 0.1639],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2500, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 70 was 24.9%.\n",
      "current params: tensor([3.0291, 3.0291, 3.0086, 2.6138, 3.0086, 2.6172, 2.6172, 0.1693, 2.6138,\n",
      "        2.4440, 0.1694, 0.1612, 0.1612, 2.6180, 2.6180, 2.4440, 0.1694, 0.1633],\n",
      "       dtype=torch.float64)\n",
      "tensor(0.2500, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "optim.rn.update_reaction_net(rn)\n",
    "optim.optimize(conc_scale=1e-1,conc_thresh=1e-1,mod_bool=True,mod_factor=10,max_thresh=1e8,max_yield=0,yield_species=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = time_mod.perf_counter()\n",
    "print(\"Time taken for complete analysis: %.4f\" %(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track the yield over optim iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optim.plot_yield()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all the parameter values\n",
    "\n",
    "### This can be stored in a file for later analysis or used to find the best parameter value depending upon a condition. For e.g. the values that give a minimum error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yields= []\n",
    "final_params=[]\n",
    "\n",
    "final_t50 = []\n",
    "final_t85 = []\n",
    "final_t95 = []\n",
    "final_t99 = []\n",
    "\n",
    "for i in range(len(optim.final_yields)):\n",
    "    yields.append(optim.final_yields[i])\n",
    "    final_params.append(optim.final_solns[i])\n",
    "    \n",
    "    #Storing the different time points it reaches a particular yield threshold\n",
    "    if optim.final_t85[i] == -1:\n",
    "        final_t85.append(1) \n",
    "    else:\n",
    "        final_t85.append(optim.final_t85[i]) \n",
    "    if optim.final_t95[i] == -1:\n",
    "        final_t95.append(1)\n",
    "    else:\n",
    "        final_t95.append(optim.final_t95[i])\n",
    "\n",
    "final_yield_arr = np.array(yields)\n",
    "final_param_arr = np.array(final_params)\n",
    "final_t85 = np.array(final_t85)\n",
    "final_t95 = np.array(final_t95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the ratio of ktri vs kdim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mask_r = final_yield_arr > 0.5\n",
    "\n",
    "#Calculate the ratio\n",
    "ratio = final_param_arr[:,1]/final_param_arr[:,0]\n",
    "\n",
    "#Normalize the time scale (t = t*conc*max_rate)\n",
    "conc=vec_rn.initial_copies[0].item()\n",
    "scale_time = final_t95[mask_r]*conc*np.max(final_param_arr[mask_r],axis=1)\n",
    "#Calculate the y_per_time\n",
    "y_per_time = 0.95/scale_time\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "ax.plot(ratio,y_per_time,linestyle='',marker='o')\n",
    "ax.set_ylabel(\"Efficiency\",fontsize=20)\n",
    "ax.set_xlabel(\"Ratio\",fontsize=20)\n",
    "ax.tick_params(labelsize=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_indx = np.argmax(y_per_time)\n",
    "max_ratio = ratio[max_indx]\n",
    "max_rates = final_param_arr[max_indx]\n",
    "print(\"Ratio with maximum efficiency: \",max_ratio)\n",
    "\n",
    "reaction_rates = np.zeros(rn._rxn_count)\n",
    "counter=0\n",
    "for cls,uids in vec_rn.rxn_class.items():\n",
    "    for rid in uids:\n",
    "        reaction_rates[rid]=max_rates[counter]\n",
    "    counter+=1\n",
    "\n",
    "print(\"Optimal Rates: \",list(reaction_rates))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
