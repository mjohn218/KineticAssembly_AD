{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Equilibrium Maximazation of Yield #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# make sure jupyter path is correct for loading local moudules\n",
    "import sys\n",
    "# path to steric_simulator module relative to notebook\n",
    "sys.path.append(\"../../../\")\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from KineticAssembly_AD import ReactionNetwork, VectorizedRxnNet, VecSim, Optimizer, EquilibriumSolver\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch import DoubleTensor as Tensor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with the AP2 complex that we've worked with before. Pairwise $\\Delta Gs$ were derived from the PDB structures via Rossetta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['default_assoc', 1.0]\n",
      "['monomer_add_only', False]\n",
      "['chaperone', True]\n",
      "[(0, {'struct': <networkx.classes.graph.Graph object at 0x0000025BC6CDA1D0>, 'copies': tensor([100.], dtype=torch.float64), 'subunits': 1}), (1, {'struct': <networkx.classes.graph.Graph object at 0x0000025BBEEF8240>, 'copies': tensor([100.], dtype=torch.float64), 'subunits': 1}), (2, {'struct': <networkx.classes.graph.Graph object at 0x0000025BBEEF8780>, 'copies': tensor([100.], dtype=torch.float64), 'subunits': 1}), (3, {'struct': <networkx.classes.graph.Graph object at 0x0000025BBEEF8828>, 'copies': tensor([100.], dtype=torch.float64), 'subunits': 1}), (4, {'struct': <networkx.classes.graph.Graph object at 0x0000025BBEEF8208>, 'copies': tensor([300.], dtype=torch.float64), 'subunits': 1})]\n",
      "New node added - Node index: 5 ; Node label: AM \n",
      "New node added - Node index: 6 ; Node label: AB \n",
      "New node added - Node index: 7 ; Node label: AS \n",
      "New node added - Node index: 8 ; Node label: BM \n",
      "New node added - Node index: 9 ; Node label: MS \n",
      "New node added - Node index: 10 ; Node label: ABM \n",
      "New node added - Node index: 11 ; Node label: AMS \n",
      "New node added - Node index: 12 ; Node label: BS \n",
      "New node added - Node index: 13 ; Node label: ABS \n",
      "New node added - Node index: 14 ; Node label: BMS \n",
      "New node added - Node index: 15 ; Node label: ABMS \n",
      "The number of bonds formed are not compensated by the number of edges\n",
      "This could be possible due to presence of a repeating subunit\n",
      "SOurce1:  2 11\n",
      "The common reactant is:  B\n",
      "Edge added between:  2 15\n",
      "The number of bonds formed are not compensated by the number of edges\n",
      "This could be possible due to presence of a repeating subunit\n",
      "SOurce1:  3 10\n",
      "The common reactant is:  S\n",
      "Edge added between:  3 15\n",
      "*******Chaperone Reaction**********\n",
      "[4, 6] ['A', 'B', 'X']\n",
      "New node added - Node index: 16 ; Node label: ABX \n",
      "*******Chaperone Reaction**********\n",
      "[4, 10] ['A', 'B', 'M', 'X']\n",
      "New node added - Node index: 17 ; Node label: ABMX \n",
      "The number of bonds formed are not compensated by the number of edges\n",
      "This could be possible due to presence of a repeating subunit\n",
      "SOurce1:  5 12\n",
      "*******Chaperone Reaction**********\n",
      "[4, 6] ['A', 'B', 'X']\n",
      "The number of bonds formed are not compensated by the number of edges\n",
      "This could be possible due to presence of a repeating subunit\n",
      "SOurce1:  6 9\n",
      "The number of bonds formed are not compensated by the number of edges\n",
      "This could be possible due to presence of a repeating subunit\n",
      "SOurce1:  7 8\n",
      "*******Chaperone Reaction**********\n",
      "[4, 10] ['A', 'B', 'M', 'X']\n",
      "The number of bonds formed are not compensated by the number of edges\n",
      "This could be possible due to presence of a repeating subunit\n",
      "SOurce1:  13 1\n",
      "The common reactant is:  M\n",
      "Edge added between:  1 15\n",
      "The number of bonds formed are not compensated by the number of edges\n",
      "This could be possible due to presence of a repeating subunit\n",
      "SOurce1:  14 0\n",
      "The common reactant is:  A\n",
      "Edge added between:  0 15\n",
      "Resolving Chaperone Rxns::\n",
      "[([4, 6], ['A', 'B', 'X']), ([4, 10], ['A', 'B', 'M', 'X'])]\n",
      "Reaction Network Completed\n"
     ]
    }
   ],
   "source": [
    "base_input = 'tetramer_chaperone.pwr'\n",
    "rn = ReactionNetwork(base_input, one_step=True)\n",
    "rn.resolve_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(1, 1): [0, 1, 2, 3, 4, 7], (2, 1): [5, 6, 8, 9, 10, 12, 13, 14, 16, 21, 22, 23, 24], (3, 1): [11, 15, 17, 25, 26], (2, 2): [18, 19, 20]}\n",
      "{4: [27, 16, 28, 17]}\n"
     ]
    }
   ],
   "source": [
    "print(rn.rxn_class)\n",
    "print(rn.chap_uid_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -- A\n",
      "1 -- M\n",
      "2 -- B\n",
      "3 -- S\n",
      "4 -- X\n",
      "5 -- AM\n",
      "6 -- AB\n",
      "7 -- AS\n",
      "8 -- BM\n",
      "9 -- MS\n",
      "10 -- ABM\n",
      "11 -- AMS\n",
      "12 -- BS\n",
      "13 -- ABS\n",
      "14 -- BMS\n",
      "15 -- ABMS\n",
      "16 -- ABX\n",
      "17 -- ABMX\n",
      "{(0, 5): 0, (0, 6): 1, (0, 7): 2, (0, 10): 21, (0, 11): 22, (0, 13): 23, (0, 15): 26, (1, 5): 0, (1, 8): 3, (1, 9): 4, (1, 10): 5, (1, 11): 6, (1, 14): 24, (1, 15): 25, (2, 6): 1, (2, 8): 3, (2, 12): 7, (2, 10): 8, (2, 13): 9, (2, 14): 10, (2, 15): 11, (3, 7): 2, (3, 9): 4, (3, 12): 7, (3, 11): 12, (3, 13): 13, (3, 14): 14, (3, 15): 15, (4, 16): 16, (4, 17): 17, (5, 10): 8, (5, 11): 12, (5, 15): 18, (6, 10): 5, (6, 13): 13, (6, 16): 16, (6, 15): 19, (7, 11): 6, (7, 13): 9, (7, 15): 20, (8, 14): 14, (8, 15): 20, (8, 10): 21, (9, 14): 10, (9, 15): 19, (9, 11): 22, (10, 15): 15, (10, 17): 17, (11, 15): 11, (12, 15): 18, (12, 13): 23, (12, 14): 24, (13, 15): 25, (14, 15): 26, (16, 0): 27, (16, 2): 27, (16, 4): 27, (17, 0): 28, (17, 1): 28, (17, 2): 28, (17, 4): 28}\n"
     ]
    }
   ],
   "source": [
    "uid_dict = {}\n",
    "sys.path.append(\"../../\")\n",
    "import numpy as np\n",
    "from reaction_network import gtostr\n",
    "for n in rn.network.nodes():\n",
    "    #print(n)\n",
    "    #print(rn.network.nodes()[n])\n",
    "    print(n,\"--\",gtostr(rn.network.nodes[n]['struct']))\n",
    "    for k,v in rn.network[n].items():\n",
    "        uid = v['uid']\n",
    "        r1 = set(gtostr(rn.network.nodes[n]['struct']))\n",
    "        p = set(gtostr(rn.network.nodes[k]['struct']))\n",
    "        r2 = p-r1\n",
    "        reactants = (r1,r2)\n",
    "        uid_dict[(n,k)] = uid\n",
    "\n",
    "print(uid_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 0}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 1}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 2}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 21}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 22}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 23}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-60.], dtype=torch.float64), 'uid': 26}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 0}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 3}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 4}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 5}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 6}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 24}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-60.], dtype=torch.float64), 'uid': 25}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 1}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 3}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 7}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 8}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 9}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 10}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-60.], dtype=torch.float64), 'uid': 11}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 2}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 4}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 7}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 12}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 13}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 14}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-60.], dtype=torch.float64), 'uid': 15}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 16}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 17}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 8}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 12}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-80.], dtype=torch.float64), 'uid': 18}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 5}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 13}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 16}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-80.], dtype=torch.float64), 'uid': 19}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 6}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 9}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-80.], dtype=torch.float64), 'uid': 20}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 14}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-80.], dtype=torch.float64), 'uid': 20}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 21}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 10}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-80.], dtype=torch.float64), 'uid': 19}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 22}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-60.], dtype=torch.float64), 'uid': 15}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 17}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-60.], dtype=torch.float64), 'uid': 11}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-80.], dtype=torch.float64), 'uid': 18}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 23}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 24}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-60.], dtype=torch.float64), 'uid': 25}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-60.], dtype=torch.float64), 'uid': 26}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-100.]), 'uid': 27}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-100.]), 'uid': 27}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-100.]), 'uid': 27}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-100.]), 'uid': 28}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-100.]), 'uid': 28}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-100.]), 'uid': 28}\n",
      "{'k_on': 1.0, 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-100.]), 'uid': 28}\n",
      "Reaction rates:  tensor([ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         6.4000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000, 10.0000,  1.0000,  1.0000], dtype=torch.float64,\n",
      "       grad_fn=<CopySlices>)\n",
      "dGs:  tensor([ -20.,  -20.,  -20.,  -20.,  -20.,  -40.,  -40.,  -20.,  -40.,  -40.,\n",
      "         -40.,  -60.,  -40.,  -40.,  -40.,  -60.,  -20.,  -20.,  -80.,  -80.,\n",
      "         -80.,  -40.,  -40.,  -40.,  -40.,  -60.,  -60., -100., -100.],\n",
      "       dtype=torch.float64)\n",
      "Species Concentrations:  tensor([100., 100., 100., 100., 300.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.], dtype=torch.float64)\n",
      "Shifting to device:  cpu\n",
      "tensor([ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         6.4000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000, 10.0000,  1.0000,  1.0000], dtype=torch.float64,\n",
      "       grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "#Do modifications here\n",
    "#Changing Initial Conditions\n",
    "import networkx as nx\n",
    "#Changin k_on\n",
    "new_kon = torch.zeros([rn._rxn_count], requires_grad=True).double()\n",
    "new_kon = new_kon + Tensor([1.]*np.array(1e0))\n",
    "new_kon[16] = 6.4\n",
    "new_kon[26] = 10\n",
    "update_kon_dict = {}\n",
    "for edge in rn.network.edges:\n",
    "    print(rn.network.get_edge_data(edge[0],edge[1]))\n",
    "    update_kon_dict[edge] = new_kon[uid_dict[edge]]\n",
    "\n",
    "nx.set_edge_attributes(rn.network,update_kon_dict,'k_on')\n",
    "\n",
    "# for edge in rn.network.edges:\n",
    "#     print(rn.network.get_edge_data(edge[0],edge[1]))\n",
    "vec_rn = VectorizedRxnNet(rn, dev='cpu',assoc_is_param=False,chap_is_param=True)\n",
    "print(vec_rn.kon)\n",
    "\n",
    "#Changing initial concentrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Using the optimizer with a 1 second simulation runtime ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "Be careful about choosing yield_species; It defaults to the largest complex\n",
      "Reaction Parameters before optimization: \n",
      "[Parameter containing:\n",
      "tensor(300., dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor(6.4000, dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor(1., dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor(1., dtype=torch.float64, requires_grad=True), Parameter containing:\n",
      "tensor(1., dtype=torch.float64, requires_grad=True)]\n",
      "Optimizer State: <bound method Optimizer.state_dict of RMSprop (\n",
      "Parameter Group 0\n",
      "    alpha: 0.99\n",
      "    centered: False\n",
      "    eps: 1e-08\n",
      "    lr: 0.6\n",
      "    momentum: 0.5\n",
      "    weight_decay: 0\n",
      "\n",
      "Parameter Group 1\n",
      "    alpha: 0.99\n",
      "    centered: False\n",
      "    eps: 1e-08\n",
      "    lr: 6.4e-08\n",
      "    momentum: 0.5\n",
      "    weight_decay: 0\n",
      "\n",
      "Parameter Group 2\n",
      "    alpha: 0.99\n",
      "    centered: False\n",
      "    eps: 1e-08\n",
      "    lr: 0.002\n",
      "    momentum: 0.5\n",
      "    weight_decay: 0\n",
      "\n",
      "Parameter Group 3\n",
      "    alpha: 0.99\n",
      "    centered: False\n",
      "    eps: 1e-08\n",
      "    lr: 0.002\n",
      "    momentum: 0.5\n",
      "    weight_decay: 0\n",
      "\n",
      "Parameter Group 4\n",
      "    alpha: 0.99\n",
      "    centered: False\n",
      "    eps: 1e-08\n",
      "    lr: 0.002\n",
      "    momentum: 0.5\n",
      "    weight_decay: 0\n",
      ")>\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 0 was 98.0%.\n",
      "current params: [tensor(300., dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64), tensor(1., dtype=torch.float64)]\n",
      "tensor(0.9808, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(9.5186e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0015, dtype=torch.float64)Grad:  tensor(6.2327, dtype=torch.float64)Grad:  tensor(1.4102, dtype=torch.float64)Grad:  tensor(0.0955, dtype=torch.float64)Grad:  tensor(0.0626, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 1 was 98.0%.\n",
      "current params: [tensor(294.0004, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.9800, dtype=torch.float64), tensor(0.9800, dtype=torch.float64), tensor(0.9800, dtype=torch.float64)]\n",
      "tensor(0.9810, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(9.5838e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0016, dtype=torch.float64)Grad:  tensor(6.7233, dtype=torch.float64)Grad:  tensor(1.9355, dtype=torch.float64)Grad:  tensor(0.0960, dtype=torch.float64)Grad:  tensor(0.0631, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 2 was 98.0%.\n",
      "current params: [tensor(286.5750, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.9538, dtype=torch.float64), tensor(0.9558, dtype=torch.float64), tensor(0.9558, dtype=torch.float64)]\n",
      "tensor(0.9808, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(9.7835e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0020, dtype=torch.float64)Grad:  tensor(8.3498, dtype=torch.float64)Grad:  tensor(2.2729, dtype=torch.float64)Grad:  tensor(0.0960, dtype=torch.float64)Grad:  tensor(0.0637, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 3 was 98.0%.\n",
      "current params: [tensor(278.7794, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.9269, dtype=torch.float64), tensor(0.9321, dtype=torch.float64), tensor(0.9319, dtype=torch.float64)]\n",
      "tensor(0.9806, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(9.9762e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0014, dtype=torch.float64)Grad:  tensor(5.8270, dtype=torch.float64)Grad:  tensor(4.8792, dtype=torch.float64)Grad:  tensor(0.0953, dtype=torch.float64)Grad:  tensor(0.0631, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 4 was 98.0%.\n",
      "current params: [tensor(272.2787, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.8968, dtype=torch.float64), tensor(0.9102, dtype=torch.float64), tensor(0.9100, dtype=torch.float64)]\n",
      "tensor(0.9806, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0024, dtype=torch.float64)Grad:  tensor(9.7728, dtype=torch.float64)Grad:  tensor(1.6827, dtype=torch.float64)Grad:  tensor(0.1004, dtype=torch.float64)Grad:  tensor(0.0665, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 5 was 98.0%.\n",
      "current params: [tensor(265.4670, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.8763, dtype=torch.float64), tensor(0.8898, dtype=torch.float64), tensor(0.8896, dtype=torch.float64)]\n",
      "tensor(0.9807, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0012, dtype=torch.float64)Grad:  tensor(4.7858, dtype=torch.float64)Grad:  tensor(0.9293, dtype=torch.float64)Grad:  tensor(0.1095, dtype=torch.float64)Grad:  tensor(0.0699, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 6 was 98.0%.\n",
      "current params: [tensor(260.3691, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.8629, dtype=torch.float64), tensor(0.8705, dtype=torch.float64), tensor(0.8705, dtype=torch.float64)]\n",
      "tensor(0.9804, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0019, dtype=torch.float64)Grad:  tensor(7.8628, dtype=torch.float64)Grad:  tensor(1.2613, dtype=torch.float64)Grad:  tensor(0.1078, dtype=torch.float64)Grad:  tensor(0.0700, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 7 was 98.0%.\n",
      "current params: [tensor(255.2711, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.8522, dtype=torch.float64), tensor(0.8526, dtype=torch.float64), tensor(0.8527, dtype=torch.float64)]\n",
      "tensor(0.9801, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0010, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0025, dtype=torch.float64)Grad:  tensor(10.2862, dtype=torch.float64)Grad:  tensor(1.8461, dtype=torch.float64)Grad:  tensor(0.1059, dtype=torch.float64)Grad:  tensor(0.0698, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 8 was 98.0%.\n",
      "current params: [tensor(249.7806, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.8412, dtype=torch.float64), tensor(0.8361, dtype=torch.float64), tensor(0.8363, dtype=torch.float64)]\n",
      "tensor(0.9802, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0010, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0016, dtype=torch.float64)Grad:  tensor(6.4268, dtype=torch.float64)Grad:  tensor(2.1548, dtype=torch.float64)Grad:  tensor(0.1114, dtype=torch.float64)Grad:  tensor(0.0717, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 9 was 98.0%.\n",
      "current params: [tensor(245.2606, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.8293, dtype=torch.float64), tensor(0.8205, dtype=torch.float64), tensor(0.8208, dtype=torch.float64)]\n",
      "tensor(0.9801, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0010, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0015, dtype=torch.float64)Grad:  tensor(6.1648, dtype=torch.float64)Grad:  tensor(5.1936, dtype=torch.float64)Grad:  tensor(0.1058, dtype=torch.float64)Grad:  tensor(0.0692, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 10 was 97.9%.\n",
      "current params: [tensor(241.3466, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.8111, dtype=torch.float64), tensor(0.8060, dtype=torch.float64), tensor(0.8063, dtype=torch.float64)]\n",
      "tensor(0.9800, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0010, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0016, dtype=torch.float64)Grad:  tensor(6.1825, dtype=torch.float64)Grad:  tensor(0.9387, dtype=torch.float64)Grad:  tensor(0.1189, dtype=torch.float64)Grad:  tensor(0.0754, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 11 was 97.9%.\n",
      "current params: [tensor(237.7758, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.7998, dtype=torch.float64), tensor(0.7917, dtype=torch.float64), tensor(0.7923, dtype=torch.float64)]\n",
      "tensor(0.9800, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0014, dtype=torch.float64)Grad:  tensor(5.7226, dtype=torch.float64)Grad:  tensor(1.1328, dtype=torch.float64)Grad:  tensor(0.1211, dtype=torch.float64)Grad:  tensor(0.0765, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 12 was 97.9%.\n",
      "current params: [tensor(234.5274, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.7916, dtype=torch.float64), tensor(0.7778, dtype=torch.float64), tensor(0.7786, dtype=torch.float64)]\n",
      "tensor(0.9795, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0027, dtype=torch.float64)Grad:  tensor(10.6161, dtype=torch.float64)Grad:  tensor(2.3049, dtype=torch.float64)Grad:  tensor(0.1139, dtype=torch.float64)Grad:  tensor(0.0745, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 13 was 97.9%.\n",
      "current params: [tensor(230.4098, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.7822, dtype=torch.float64), tensor(0.7647, dtype=torch.float64), tensor(0.7657, dtype=torch.float64)]\n",
      "tensor(0.9795, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0022, dtype=torch.float64)Grad:  tensor(8.8039, dtype=torch.float64)Grad:  tensor(1.2024, dtype=torch.float64)Grad:  tensor(0.1211, dtype=torch.float64)Grad:  tensor(0.0776, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 14 was 97.9%.\n",
      "current params: [tensor(226.3778, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.7748, dtype=torch.float64), tensor(0.7520, dtype=torch.float64), tensor(0.7530, dtype=torch.float64)]\n",
      "tensor(0.9793, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0033, dtype=torch.float64)Grad:  tensor(12.8761, dtype=torch.float64)Grad:  tensor(3.1878, dtype=torch.float64)Grad:  tensor(0.1118, dtype=torch.float64)Grad:  tensor(0.0747, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 15 was 97.9%.\n",
      "current params: [tensor(221.7385, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.7643, dtype=torch.float64), tensor(0.7401, dtype=torch.float64), tensor(0.7411, dtype=torch.float64)]\n",
      "tensor(0.9791, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0028, dtype=torch.float64)Grad:  tensor(10.9563, dtype=torch.float64)Grad:  tensor(0.5825, dtype=torch.float64)Grad:  tensor(0.1238, dtype=torch.float64)Grad:  tensor(0.0799, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 16 was 97.8%.\n",
      "current params: [tensor(217.3067, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.7578, dtype=torch.float64), tensor(0.7282, dtype=torch.float64), tensor(0.7292, dtype=torch.float64)]\n",
      "tensor(0.9789, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(6.7945e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0024, dtype=torch.float64)Grad:  tensor(9.1466, dtype=torch.float64)Grad:  tensor(2.4288, dtype=torch.float64)Grad:  tensor(0.1221, dtype=torch.float64)Grad:  tensor(0.0788, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 17 was 97.9%.\n",
      "current params: [tensor(213.3826, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.7495, dtype=torch.float64), tensor(0.7167, dtype=torch.float64), tensor(0.7177, dtype=torch.float64)]\n",
      "tensor(0.9791, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0033, dtype=torch.float64)Grad:  tensor(12.5140, dtype=torch.float64)Grad:  tensor(2.9885, dtype=torch.float64)Grad:  tensor(0.1173, dtype=torch.float64)Grad:  tensor(0.0779, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 18 was 97.8%.\n",
      "current params: [tensor(209.2219, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.7394, dtype=torch.float64), tensor(0.7057, dtype=torch.float64), tensor(0.7067, dtype=torch.float64)]\n",
      "tensor(0.9789, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0026, dtype=torch.float64)Grad:  tensor(9.9657, dtype=torch.float64)Grad:  tensor(0.6519, dtype=torch.float64)Grad:  tensor(0.1303, dtype=torch.float64)Grad:  tensor(0.0833, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 19 was 97.8%.\n",
      "current params: [tensor(205.4443, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.7330, dtype=torch.float64), tensor(0.6947, dtype=torch.float64), tensor(0.6956, dtype=torch.float64)]\n",
      "tensor(0.9787, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0017, dtype=torch.float64)Grad:  tensor(6.5604, dtype=torch.float64)Grad:  tensor(4.7054, dtype=torch.float64)Grad:  tensor(0.1228, dtype=torch.float64)Grad:  tensor(0.0789, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 20 was 97.8%.\n",
      "current params: [tensor(202.4477, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.7213, dtype=torch.float64), tensor(0.6840, dtype=torch.float64), tensor(0.6851, dtype=torch.float64)]\n",
      "tensor(0.9783, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0016, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0020, dtype=torch.float64)Grad:  tensor(7.3839, dtype=torch.float64)Grad:  tensor(5.5370, dtype=torch.float64)Grad:  tensor(0.1202, dtype=torch.float64)Grad:  tensor(0.0781, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 21 was 97.8%.\n",
      "current params: [tensor(199.7175, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.7063, dtype=torch.float64), tensor(0.6738, dtype=torch.float64), tensor(0.6749, dtype=torch.float64)]\n",
      "tensor(0.9781, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0017, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0033, dtype=torch.float64)Grad:  tensor(12.3545, dtype=torch.float64)Grad:  tensor(2.0565, dtype=torch.float64)Grad:  tensor(0.1271, dtype=torch.float64)Grad:  tensor(0.0831, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 22 was 97.7%.\n",
      "current params: [tensor(196.3853, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6956, dtype=torch.float64), tensor(0.6636, dtype=torch.float64), tensor(0.6647, dtype=torch.float64)]\n",
      "tensor(0.9779, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0018, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0030, dtype=torch.float64)Grad:  tensor(11.2541, dtype=torch.float64)Grad:  tensor(0.8731, dtype=torch.float64)Grad:  tensor(0.1349, dtype=torch.float64)Grad:  tensor(0.0865, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 23 was 97.7%.\n",
      "current params: [tensor(192.9870, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6888, dtype=torch.float64), tensor(0.6534, dtype=torch.float64), tensor(0.6545, dtype=torch.float64)]\n",
      "tensor(0.9778, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0018, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0021, dtype=torch.float64)Grad:  tensor(7.9539, dtype=torch.float64)Grad:  tensor(2.3148, dtype=torch.float64)Grad:  tensor(0.1363, dtype=torch.float64)Grad:  tensor(0.0861, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 24 was 97.7%.\n",
      "current params: [tensor(190.0779, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6817, dtype=torch.float64), tensor(0.6432, dtype=torch.float64), tensor(0.6445, dtype=torch.float64)]\n",
      "tensor(0.9777, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0019, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0021, dtype=torch.float64)Grad:  tensor(7.9021, dtype=torch.float64)Grad:  tensor(0.9995, dtype=torch.float64)Grad:  tensor(0.1436, dtype=torch.float64)Grad:  tensor(0.0897, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 25 was 97.7%.\n",
      "current params: [tensor(187.4338, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6765, dtype=torch.float64), tensor(0.6329, dtype=torch.float64), tensor(0.6344, dtype=torch.float64)]\n",
      "tensor(0.9773, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0020, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0031, dtype=torch.float64)Grad:  tensor(11.5254, dtype=torch.float64)Grad:  tensor(2.7401, dtype=torch.float64)Grad:  tensor(0.1326, dtype=torch.float64)Grad:  tensor(0.0861, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 26 was 97.7%.\n",
      "current params: [tensor(184.4303, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6696, dtype=torch.float64), tensor(0.6230, dtype=torch.float64), tensor(0.6247, dtype=torch.float64)]\n",
      "tensor(0.9775, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0020, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0028, dtype=torch.float64)Grad:  tensor(10.1200, dtype=torch.float64)Grad:  tensor(1.5769, dtype=torch.float64)Grad:  tensor(0.1417, dtype=torch.float64)Grad:  tensor(0.0900, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 27 was 97.6%.\n",
      "current params: [tensor(181.4820, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6637, dtype=torch.float64), tensor(0.6132, dtype=torch.float64), tensor(0.6150, dtype=torch.float64)]\n",
      "tensor(0.9768, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(1.2083e-06, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0035, dtype=torch.float64)Grad:  tensor(12.8377, dtype=torch.float64)Grad:  tensor(0.3906, dtype=torch.float64)Grad:  tensor(0.1442, dtype=torch.float64)Grad:  tensor(0.0924, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 28 was 97.6%.\n",
      "current params: [tensor(178.2373, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6601, dtype=torch.float64), tensor(0.6035, dtype=torch.float64), tensor(0.6054, dtype=torch.float64)]\n",
      "tensor(0.9768, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0040, dtype=torch.float64)Grad:  tensor(14.6447, dtype=torch.float64)Grad:  tensor(0.9297, dtype=torch.float64)Grad:  tensor(0.1407, dtype=torch.float64)Grad:  tensor(0.0918, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 29 was 97.6%.\n",
      "current params: [tensor(174.6832, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6569, dtype=torch.float64), tensor(0.5940, dtype=torch.float64), tensor(0.5959, dtype=torch.float64)]\n",
      "tensor(0.9763, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0024, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0022, dtype=torch.float64)Grad:  tensor(7.9170, dtype=torch.float64)Grad:  tensor(0.7883, dtype=torch.float64)Grad:  tensor(0.1551, dtype=torch.float64)Grad:  tensor(0.0959, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 30 was 97.6%.\n",
      "current params: [tensor(171.8677, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6540, dtype=torch.float64), tensor(0.5843, dtype=torch.float64), tensor(0.5864, dtype=torch.float64)]\n",
      "tensor(0.9764, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0024, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0034, dtype=torch.float64)Grad:  tensor(12.1228, dtype=torch.float64)Grad:  tensor(0.9268, dtype=torch.float64)Grad:  tensor(0.1491, dtype=torch.float64)Grad:  tensor(0.0952, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 31 was 97.6%.\n",
      "current params: [tensor(168.9084, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6511, dtype=torch.float64), tensor(0.5748, dtype=torch.float64), tensor(0.5770, dtype=torch.float64)]\n",
      "tensor(0.9760, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0026, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0035, dtype=torch.float64)Grad:  tensor(12.3930, dtype=torch.float64)Grad:  tensor(1.1965, dtype=torch.float64)Grad:  tensor(0.1493, dtype=torch.float64)Grad:  tensor(0.0957, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 32 was 97.5%.\n",
      "current params: [tensor(165.8816, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6477, dtype=torch.float64), tensor(0.5654, dtype=torch.float64), tensor(0.5677, dtype=torch.float64)]\n",
      "tensor(0.9760, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0001, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0026, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0030, dtype=torch.float64)Grad:  tensor(10.6058, dtype=torch.float64)Grad:  tensor(0.8676, dtype=torch.float64)Grad:  tensor(0.1564, dtype=torch.float64)Grad:  tensor(0.0985, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 33 was 97.5%.\n",
      "current params: [tensor(163.0632, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6447, dtype=torch.float64), tensor(0.5561, dtype=torch.float64), tensor(0.5585, dtype=torch.float64)]\n",
      "tensor(0.9753, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(1.6195e-06, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0030, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0025, dtype=torch.float64)Grad:  tensor(9.0270, dtype=torch.float64)Grad:  tensor(0.2859, dtype=torch.float64)Grad:  tensor(0.1648, dtype=torch.float64)Grad:  tensor(0.1018, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 34 was 97.4%.\n",
      "current params: [tensor(160.5519, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6427, dtype=torch.float64), tensor(0.5466, dtype=torch.float64), tensor(0.5493, dtype=torch.float64)]\n",
      "tensor(0.9749, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(9.8281e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0030, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0021, dtype=torch.float64)Grad:  tensor(7.5044, dtype=torch.float64)Grad:  tensor(0.9327, dtype=torch.float64)Grad:  tensor(0.1669, dtype=torch.float64)Grad:  tensor(0.1023, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 35 was 97.4%.\n",
      "current params: [tensor(158.3825, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6402, dtype=torch.float64), tensor(0.5371, dtype=torch.float64), tensor(0.5401, dtype=torch.float64)]\n",
      "tensor(0.9750, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(9.7608e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0031, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0044, dtype=torch.float64)Grad:  tensor(15.4622, dtype=torch.float64)Grad:  tensor(4.1398, dtype=torch.float64)Grad:  tensor(0.1358, dtype=torch.float64)Grad:  tensor(0.0921, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 36 was 97.4%.\n",
      "current params: [tensor(155.4861, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6326, dtype=torch.float64), tensor(0.5286, dtype=torch.float64), tensor(0.5315, dtype=torch.float64)]\n",
      "tensor(0.9746, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(9.6724e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0032, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0038, dtype=torch.float64)Grad:  tensor(13.1979, dtype=torch.float64)Grad:  tensor(1.9089, dtype=torch.float64)Grad:  tensor(0.1545, dtype=torch.float64)Grad:  tensor(0.0999, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 37 was 97.3%.\n",
      "current params: [tensor(152.5267, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6259, dtype=torch.float64), tensor(0.5200, dtype=torch.float64), tensor(0.5229, dtype=torch.float64)]\n",
      "tensor(0.9740, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(9.3326e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0035, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0021, dtype=torch.float64)Grad:  tensor(7.4685, dtype=torch.float64)Grad:  tensor(2.0925, dtype=torch.float64)Grad:  tensor(0.1673, dtype=torch.float64)Grad:  tensor(0.1031, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 38 was 97.3%.\n",
      "current params: [tensor(150.1928, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6194, dtype=torch.float64), tensor(0.5112, dtype=torch.float64), tensor(0.5143, dtype=torch.float64)]\n",
      "tensor(0.9739, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(9.4884e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0035, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0036, dtype=torch.float64)Grad:  tensor(12.6504, dtype=torch.float64)Grad:  tensor(4.1075, dtype=torch.float64)Grad:  tensor(0.1462, dtype=torch.float64)Grad:  tensor(0.0964, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 39 was 97.3%.\n",
      "current params: [tensor(147.6061, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6102, dtype=torch.float64), tensor(0.5029, dtype=torch.float64), tensor(0.5059, dtype=torch.float64)]\n",
      "tensor(0.9731, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(5.1872e-06, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0039, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0046, dtype=torch.float64)Grad:  tensor(15.8319, dtype=torch.float64)Grad:  tensor(0.7067, dtype=torch.float64)Grad:  tensor(0.1620, dtype=torch.float64)Grad:  tensor(0.1055, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 40 was 97.2%.\n",
      "current params: [tensor(144.5933, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6046, dtype=torch.float64), tensor(0.4945, dtype=torch.float64), tensor(0.4975, dtype=torch.float64)]\n",
      "tensor(0.9729, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(8.9723e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0039, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0051, dtype=torch.float64)Grad:  tensor(17.7085, dtype=torch.float64)Grad:  tensor(5.4180, dtype=torch.float64)Grad:  tensor(0.1286, dtype=torch.float64)Grad:  tensor(0.0918, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 41 was 97.2%.\n",
      "current params: [tensor(141.2387, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5944, dtype=torch.float64), tensor(0.4870, dtype=torch.float64), tensor(0.4895, dtype=torch.float64)]\n",
      "tensor(0.9722, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(7.9161e-07, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0043, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0044, dtype=torch.float64)Grad:  tensor(15.0758, dtype=torch.float64)Grad:  tensor(3.6101, dtype=torch.float64)Grad:  tensor(0.1470, dtype=torch.float64)Grad:  tensor(0.0992, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 42 was 97.2%.\n",
      "current params: [tensor(138.0235, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5845, dtype=torch.float64), tensor(0.4795, dtype=torch.float64), tensor(0.4816, dtype=torch.float64)]\n",
      "tensor(0.9720, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(1.8479e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0044, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0052, dtype=torch.float64)Grad:  tensor(17.6564, dtype=torch.float64)Grad:  tensor(0.3926, dtype=torch.float64)Grad:  tensor(0.1643, dtype=torch.float64)Grad:  tensor(0.1092, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 43 was 97.1%.\n",
      "current params: [tensor(134.6728, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5790, dtype=torch.float64), tensor(0.4716, dtype=torch.float64), tensor(0.4734, dtype=torch.float64)]\n",
      "tensor(0.9715, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(5.9785e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0045, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0036, dtype=torch.float64)Grad:  tensor(12.1974, dtype=torch.float64)Grad:  tensor(1.5168, dtype=torch.float64)Grad:  tensor(0.1712, dtype=torch.float64)Grad:  tensor(0.1097, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 44 was 97.0%.\n",
      "current params: [tensor(131.8047, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5743, dtype=torch.float64), tensor(0.4634, dtype=torch.float64), tensor(0.4651, dtype=torch.float64)]\n",
      "tensor(0.9709, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(7.4736e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0048, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0048, dtype=torch.float64)Grad:  tensor(16.2619, dtype=torch.float64)Grad:  tensor(4.4712, dtype=torch.float64)Grad:  tensor(0.1394, dtype=torch.float64)Grad:  tensor(0.0982, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 45 was 97.0%.\n",
      "current params: [tensor(128.8181, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5662, dtype=torch.float64), tensor(0.4559, dtype=torch.float64), tensor(0.4573, dtype=torch.float64)]\n",
      "tensor(0.9702, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(7.5459e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0051, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0035, dtype=torch.float64)Grad:  tensor(11.7395, dtype=torch.float64)Grad:  tensor(1.6195, dtype=torch.float64)Grad:  tensor(0.1748, dtype=torch.float64)Grad:  tensor(0.1120, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 46 was 96.9%.\n",
      "current params: [tensor(126.2117, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5601, dtype=torch.float64), tensor(0.4480, dtype=torch.float64), tensor(0.4492, dtype=torch.float64)]\n",
      "tensor(0.9699, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(7.4849e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0052, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0039, dtype=torch.float64)Grad:  tensor(13.0228, dtype=torch.float64)Grad:  tensor(0.6147, dtype=torch.float64)Grad:  tensor(0.1818, dtype=torch.float64)Grad:  tensor(0.1165, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 47 was 96.8%.\n",
      "current params: [tensor(123.6869, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5562, dtype=torch.float64), tensor(0.4397, dtype=torch.float64), tensor(0.4409, dtype=torch.float64)]\n",
      "tensor(0.9690, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(7.2501e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0057, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0037, dtype=torch.float64)Grad:  tensor(12.3335, dtype=torch.float64)Grad:  tensor(4.4854, dtype=torch.float64)Grad:  tensor(0.1521, dtype=torch.float64)Grad:  tensor(0.1030, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 48 was 96.8%.\n",
      "current params: [tensor(121.2779, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5487, dtype=torch.float64), tensor(0.4320, dtype=torch.float64), tensor(0.4330, dtype=torch.float64)]\n",
      "tensor(0.9682, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(7.0479e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0060, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0031, dtype=torch.float64)Grad:  tensor(10.1813, dtype=torch.float64)Grad:  tensor(4.9438, dtype=torch.float64)Grad:  tensor(0.1551, dtype=torch.float64)Grad:  tensor(0.1034, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 49 was 96.7%.\n",
      "current params: [tensor(119.1298, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5390, dtype=torch.float64), tensor(0.4246, dtype=torch.float64), tensor(0.4254, dtype=torch.float64)]\n",
      "tensor(0.9678, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(6.9255e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0062, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0025, dtype=torch.float64)Grad:  tensor(8.1431, dtype=torch.float64)Grad:  tensor(3.9833, dtype=torch.float64)Grad:  tensor(0.1714, dtype=torch.float64)Grad:  tensor(0.1097, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 50 was 96.6%.\n",
      "current params: [tensor(117.3002, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5296, dtype=torch.float64), tensor(0.4170, dtype=torch.float64), tensor(0.4177, dtype=torch.float64)]\n",
      "tensor(0.9668, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(6.6741e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0066, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0048, dtype=torch.float64)Grad:  tensor(15.5539, dtype=torch.float64)Grad:  tensor(1.4626, dtype=torch.float64)Grad:  tensor(0.1748, dtype=torch.float64)Grad:  tensor(0.1170, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 51 was 96.5%.\n",
      "current params: [tensor(114.9681, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5231, dtype=torch.float64), tensor(0.4093, dtype=torch.float64), tensor(0.4098, dtype=torch.float64)]\n",
      "tensor(0.9659, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(6.5071e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0070, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0063, dtype=torch.float64)Grad:  tensor(20.3924, dtype=torch.float64)Grad:  tensor(0.4433, dtype=torch.float64)Grad:  tensor(0.1713, dtype=torch.float64)Grad:  tensor(0.1197, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 52 was 96.5%.\n",
      "current params: [tensor(112.0102, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5194, dtype=torch.float64), tensor(0.4016, dtype=torch.float64), tensor(0.4018, dtype=torch.float64)]\n",
      "tensor(0.9651, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(1.6847e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0074, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0052, dtype=torch.float64)Grad:  tensor(16.8637, dtype=torch.float64)Grad:  tensor(-1.1982, dtype=torch.float64)Grad:  tensor(0.2016, dtype=torch.float64)Grad:  tensor(0.1316, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 53 was 96.4%.\n",
      "current params: [tensor(109.0769, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5189, dtype=torch.float64), tensor(0.3934, dtype=torch.float64), tensor(0.3934, dtype=torch.float64)]\n",
      "tensor(0.9643, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(1.7497e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0079, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0036, dtype=torch.float64)Grad:  tensor(11.6389, dtype=torch.float64)Grad:  tensor(-0.8185, dtype=torch.float64)Grad:  tensor(0.2180, dtype=torch.float64)Grad:  tensor(0.1358, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 54 was 96.3%.\n",
      "current params: [tensor(106.6093, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5197, dtype=torch.float64), tensor(0.3847, dtype=torch.float64), tensor(0.3848, dtype=torch.float64)]\n",
      "tensor(0.9631, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(1.7360e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0084, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0042, dtype=torch.float64)Grad:  tensor(13.3499, dtype=torch.float64)Grad:  tensor(-0.9618, dtype=torch.float64)Grad:  tensor(0.2173, dtype=torch.float64)Grad:  tensor(0.1376, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 55 was 96.2%.\n",
      "current params: [tensor(104.2359, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5212, dtype=torch.float64), tensor(0.3758, dtype=torch.float64), tensor(0.3761, dtype=torch.float64)]\n",
      "tensor(0.9623, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(1.7231e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0089, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0035, dtype=torch.float64)Grad:  tensor(11.1025, dtype=torch.float64)Grad:  tensor(-0.7984, dtype=torch.float64)Grad:  tensor(0.2275, dtype=torch.float64)Grad:  tensor(0.1412, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 56 was 96.0%.\n",
      "current params: [tensor(102.1034, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5229, dtype=torch.float64), tensor(0.3667, dtype=torch.float64), tensor(0.3673, dtype=torch.float64)]\n",
      "tensor(0.9607, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(1.7086e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0096, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0043, dtype=torch.float64)Grad:  tensor(13.6432, dtype=torch.float64)Grad:  tensor(-1.0087, dtype=torch.float64)Grad:  tensor(0.2248, dtype=torch.float64)Grad:  tensor(0.1429, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 57 was 95.9%.\n",
      "current params: [tensor(99.8846, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5249, dtype=torch.float64), tensor(0.3577, dtype=torch.float64), tensor(0.3585, dtype=torch.float64)]\n",
      "tensor(0.9594, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(1.6894e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0103, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0030, dtype=torch.float64)Grad:  tensor(9.2648, dtype=torch.float64)Grad:  tensor(-0.6711, dtype=torch.float64)Grad:  tensor(0.2420, dtype=torch.float64)Grad:  tensor(0.1475, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 58 was 95.8%.\n",
      "current params: [tensor(97.9914, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5268, dtype=torch.float64), tensor(0.3485, dtype=torch.float64), tensor(0.3496, dtype=torch.float64)]\n",
      "tensor(0.9581, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(1.6629e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0110, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0040, dtype=torch.float64)Grad:  tensor(12.3357, dtype=torch.float64)Grad:  tensor(-0.9263, dtype=torch.float64)Grad:  tensor(0.2382, dtype=torch.float64)Grad:  tensor(0.1494, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 59 was 95.6%.\n",
      "current params: [tensor(96.0063, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5288, dtype=torch.float64), tensor(0.3394, dtype=torch.float64), tensor(0.3408, dtype=torch.float64)]\n",
      "tensor(0.9563, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(1.6366e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0119, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0054, dtype=torch.float64)Grad:  tensor(16.8617, dtype=torch.float64)Grad:  tensor(-1.3060, dtype=torch.float64)Grad:  tensor(0.2295, dtype=torch.float64)Grad:  tensor(0.1505, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 60 was 95.4%.\n",
      "current params: [tensor(93.6184, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5314, dtype=torch.float64), tensor(0.3305, dtype=torch.float64), tensor(0.3320, dtype=torch.float64)]\n",
      "tensor(0.9548, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(1.6142e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0126, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0058, dtype=torch.float64)Grad:  tensor(17.7977, dtype=torch.float64)Grad:  tensor(-1.4010, dtype=torch.float64)Grad:  tensor(0.2310, dtype=torch.float64)Grad:  tensor(0.1534, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 61 was 95.2%.\n",
      "current params: [tensor(90.9787, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5344, dtype=torch.float64), tensor(0.3219, dtype=torch.float64), tensor(0.3233, dtype=torch.float64)]\n",
      "tensor(0.9529, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(1.5957e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0136, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0054, dtype=torch.float64)Grad:  tensor(16.4653, dtype=torch.float64)Grad:  tensor(-1.3109, dtype=torch.float64)Grad:  tensor(0.2392, dtype=torch.float64)Grad:  tensor(0.1572, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 62 was 95.0%.\n",
      "current params: [tensor(88.3380, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5374, dtype=torch.float64), tensor(0.3132, dtype=torch.float64), tensor(0.3145, dtype=torch.float64)]\n",
      "tensor(0.9507, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(1.5770e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0147, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0062, dtype=torch.float64)Grad:  tensor(18.9464, dtype=torch.float64)Grad:  tensor(-1.5426, dtype=torch.float64)Grad:  tensor(0.2348, dtype=torch.float64)Grad:  tensor(0.1594, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 63 was 94.8%.\n",
      "current params: [tensor(85.5269, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5408, dtype=torch.float64), tensor(0.3047, dtype=torch.float64), tensor(0.3058, dtype=torch.float64)]\n",
      "tensor(0.9489, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(1.5609e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0156, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0058, dtype=torch.float64)Grad:  tensor(17.3507, dtype=torch.float64)Grad:  tensor(-1.4312, dtype=torch.float64)Grad:  tensor(0.2439, dtype=torch.float64)Grad:  tensor(0.1637, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 64 was 94.6%.\n",
      "current params: [tensor(82.7733, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5443, dtype=torch.float64), tensor(0.2963, dtype=torch.float64), tensor(0.2970, dtype=torch.float64)]\n",
      "tensor(0.9463, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(1.5448e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0169, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0085, dtype=torch.float64)Grad:  tensor(25.2254, dtype=torch.float64)Grad:  tensor(-2.1480, dtype=torch.float64)Grad:  tensor(0.2189, dtype=torch.float64)Grad:  tensor(0.1633, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 65 was 94.3%.\n",
      "current params: [tensor(79.5104, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5486, dtype=torch.float64), tensor(0.2883, dtype=torch.float64), tensor(0.2883, dtype=torch.float64)]\n",
      "tensor(0.9439, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(1.5404e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0181, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0051, dtype=torch.float64)Grad:  tensor(15.0675, dtype=torch.float64)Grad:  tensor(-1.2827, dtype=torch.float64)Grad:  tensor(0.2584, dtype=torch.float64)Grad:  tensor(0.1722, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 66 was 94.1%.\n",
      "current params: [tensor(76.7557, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5523, dtype=torch.float64), tensor(0.2799, dtype=torch.float64), tensor(0.2795, dtype=torch.float64)]\n",
      "tensor(0.9415, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(1.5266e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0193, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0070, dtype=torch.float64)Grad:  tensor(20.5772, dtype=torch.float64)Grad:  tensor(-1.8097, dtype=torch.float64)Grad:  tensor(0.2403, dtype=torch.float64)Grad:  tensor(0.1732, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 67 was 93.8%.\n",
      "current params: [tensor(73.8718, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5563, dtype=torch.float64), tensor(0.2717, dtype=torch.float64), tensor(0.2708, dtype=torch.float64)]\n",
      "tensor(0.9385, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(1.5182e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0208, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0076, dtype=torch.float64)Grad:  tensor(21.8987, dtype=torch.float64)Grad:  tensor(-1.9693, dtype=torch.float64)Grad:  tensor(0.2368, dtype=torch.float64)Grad:  tensor(0.1764, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 68 was 93.5%.\n",
      "current params: [tensor(70.8591, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5607, dtype=torch.float64), tensor(0.2638, dtype=torch.float64), tensor(0.2620, dtype=torch.float64)]\n",
      "tensor(0.9354, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(1.5159e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0223, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0049, dtype=torch.float64)Grad:  tensor(14.1780, dtype=torch.float64)Grad:  tensor(-1.2792, dtype=torch.float64)Grad:  tensor(0.2716, dtype=torch.float64)Grad:  tensor(0.1851, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 69 was 93.1%.\n",
      "current params: [tensor(68.3348, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5645, dtype=torch.float64), tensor(0.2555, dtype=torch.float64), tensor(0.2531, dtype=torch.float64)]\n",
      "tensor(0.9318, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(1.5038e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0240, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0072, dtype=torch.float64)Grad:  tensor(20.5424, dtype=torch.float64)Grad:  tensor(-1.9201, dtype=torch.float64)Grad:  tensor(0.2460, dtype=torch.float64)Grad:  tensor(0.1856, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 70 was 92.8%.\n",
      "current params: [tensor(65.6200, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5687, dtype=torch.float64), tensor(0.2474, dtype=torch.float64), tensor(0.2442, dtype=torch.float64)]\n",
      "tensor(0.9280, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(1.5004e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0259, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0096, dtype=torch.float64)Grad:  tensor(26.9112, dtype=torch.float64)Grad:  tensor(-2.5905, dtype=torch.float64)Grad:  tensor(0.2164, dtype=torch.float64)Grad:  tensor(0.1856, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 71 was 92.4%.\n",
      "current params: [tensor(62.4220, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5739, dtype=torch.float64), tensor(0.2400, dtype=torch.float64), tensor(0.2355, dtype=torch.float64)]\n",
      "tensor(0.9243, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(1.5170e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0277, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0039, dtype=torch.float64)Grad:  tensor(10.9337, dtype=torch.float64)Grad:  tensor(-1.0402, dtype=torch.float64)Grad:  tensor(0.2954, dtype=torch.float64)Grad:  tensor(0.2014, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 72 was 92.0%.\n",
      "current params: [tensor(60.0678, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5777, dtype=torch.float64), tensor(0.2317, dtype=torch.float64), tensor(0.2265, dtype=torch.float64)]\n",
      "tensor(0.9200, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(1.5111e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0298, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0040, dtype=torch.float64)Grad:  tensor(10.8984, dtype=torch.float64)Grad:  tensor(-1.0585, dtype=torch.float64)Grad:  tensor(0.2997, dtype=torch.float64)Grad:  tensor(0.2067, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 73 was 91.5%.\n",
      "current params: [tensor(58.1322, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5809, dtype=torch.float64), tensor(0.2231, dtype=torch.float64), tensor(0.2174, dtype=torch.float64)]\n",
      "tensor(0.9154, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(1.4935e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0321, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0050, dtype=torch.float64)Grad:  tensor(13.5815, dtype=torch.float64)Grad:  tensor(-1.3597, dtype=torch.float64)Grad:  tensor(0.2899, dtype=torch.float64)Grad:  tensor(0.2104, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 74 was 90.9%.\n",
      "current params: [tensor(56.2175, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5842, dtype=torch.float64), tensor(0.2145, dtype=torch.float64), tensor(0.2082, dtype=torch.float64)]\n",
      "tensor(0.9097, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(1.4793e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0350, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0055, dtype=torch.float64)Grad:  tensor(14.9401, dtype=torch.float64)Grad:  tensor(-1.5300, dtype=torch.float64)Grad:  tensor(0.2861, dtype=torch.float64)Grad:  tensor(0.2148, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 75 was 90.4%.\n",
      "current params: [tensor(54.2193, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5876, dtype=torch.float64), tensor(0.2061, dtype=torch.float64), tensor(0.1991, dtype=torch.float64)]\n",
      "tensor(0.9044, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(1.4704e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0376, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0089, dtype=torch.float64)Grad:  tensor(23.7038, dtype=torch.float64)Grad:  tensor(-2.5150, dtype=torch.float64)Grad:  tensor(0.2351, dtype=torch.float64)Grad:  tensor(0.2132, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 76 was 89.8%.\n",
      "current params: [tensor(51.6045, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5924, dtype=torch.float64), tensor(0.1985, dtype=torch.float64), tensor(0.1900, dtype=torch.float64)]\n",
      "tensor(0.8983, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(1.4946e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0405, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0120, dtype=torch.float64)Grad:  tensor(31.5825, dtype=torch.float64)Grad:  tensor(-3.4626, dtype=torch.float64)Grad:  tensor(0.1777, dtype=torch.float64)Grad:  tensor(0.2107, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 77 was 89.2%.\n",
      "current params: [tensor(48.2358, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.5988, dtype=torch.float64), tensor(0.1922, dtype=torch.float64), tensor(0.1811, dtype=torch.float64)]\n",
      "tensor(0.8928, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(1.5664e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0430, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0105, dtype=torch.float64)Grad:  tensor(27.0318, dtype=torch.float64)Grad:  tensor(-3.0614, dtype=torch.float64)Grad:  tensor(0.1929, dtype=torch.float64)Grad:  tensor(0.2195, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 78 was 88.6%.\n",
      "current params: [tensor(44.8197, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6057, dtype=torch.float64), tensor(0.1862, dtype=torch.float64), tensor(0.1723, dtype=torch.float64)]\n",
      "tensor(0.8864, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(1.6659e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0457, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0046, dtype=torch.float64)Grad:  tensor(11.6410, dtype=torch.float64)Grad:  tensor(-1.3319, dtype=torch.float64)Grad:  tensor(0.2978, dtype=torch.float64)Grad:  tensor(0.2418, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 79 was 87.9%.\n",
      "current params: [tensor(42.3518, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6106, dtype=torch.float64), tensor(0.1791, dtype=torch.float64), tensor(0.1631, dtype=torch.float64)]\n",
      "tensor(0.8794, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(1.7265e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0489, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0050, dtype=torch.float64)Grad:  tensor(12.4638, dtype=torch.float64)Grad:  tensor(-1.4762, dtype=torch.float64)Grad:  tensor(0.2864, dtype=torch.float64)Grad:  tensor(0.2474, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 80 was 87.1%.\n",
      "current params: [tensor(40.2935, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6148, dtype=torch.float64), tensor(0.1716, dtype=torch.float64), tensor(0.1538, dtype=torch.float64)]\n",
      "tensor(0.8719, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(1.7725e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0524, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0098, dtype=torch.float64)Grad:  tensor(23.8504, dtype=torch.float64)Grad:  tensor(-2.9671, dtype=torch.float64)Grad:  tensor(0.1735, dtype=torch.float64)Grad:  tensor(0.2381, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 81 was 86.4%.\n",
      "current params: [tensor(37.7077, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6204, dtype=torch.float64), tensor(0.1655, dtype=torch.float64), tensor(0.1446, dtype=torch.float64)]\n",
      "tensor(0.8640, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(1.8803e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0558, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0102, dtype=torch.float64)Grad:  tensor(24.3307, dtype=torch.float64)Grad:  tensor(-3.1397, dtype=torch.float64)Grad:  tensor(0.1443, dtype=torch.float64)Grad:  tensor(0.2417, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 82 was 85.5%.\n",
      "current params: [tensor(34.8408, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6267, dtype=torch.float64), tensor(0.1604, dtype=torch.float64), tensor(0.1356, dtype=torch.float64)]\n",
      "tensor(0.8559, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(2.0496e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0591, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0056, dtype=torch.float64)Grad:  tensor(13.0981, dtype=torch.float64)Grad:  tensor(-1.7324, dtype=torch.float64)Grad:  tensor(0.2429, dtype=torch.float64)Grad:  tensor(0.2656, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 83 was 84.7%.\n",
      "current params: [tensor(32.5418, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6319, dtype=torch.float64), tensor(0.1545, dtype=torch.float64), tensor(0.1263, dtype=torch.float64)]\n",
      "tensor(0.8474, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(2.1931e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0627, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0047, dtype=torch.float64)Grad:  tensor(10.7064, dtype=torch.float64)Grad:  tensor(-1.4583, dtype=torch.float64)Grad:  tensor(0.2561, dtype=torch.float64)Grad:  tensor(0.2774, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 84 was 83.6%.\n",
      "current params: [tensor(30.6693, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6362, dtype=torch.float64), tensor(0.1482, dtype=torch.float64), tensor(0.1168, dtype=torch.float64)]\n",
      "tensor(0.8370, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(2.2435e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0673, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0134, dtype=torch.float64)Grad:  tensor(29.6642, dtype=torch.float64)Grad:  tensor(-4.3058, dtype=torch.float64)Grad:  tensor(-0.0484, dtype=torch.float64)Grad:  tensor(0.2370, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 85 was 82.8%.\n",
      "current params: [tensor(27.7857, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6431, dtype=torch.float64), tensor(0.1456, dtype=torch.float64), tensor(0.1080, dtype=torch.float64)]\n",
      "tensor(0.8285, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(2.6493e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0702, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0053, dtype=torch.float64)Grad:  tensor(11.3301, dtype=torch.float64)Grad:  tensor(-1.6920, dtype=torch.float64)Grad:  tensor(0.1851, dtype=torch.float64)Grad:  tensor(0.2860, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 86 was 81.8%.\n",
      "current params: [tensor(25.5743, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6485, dtype=torch.float64), tensor(0.1419, dtype=torch.float64), tensor(0.0987, dtype=torch.float64)]\n",
      "tensor(0.8183, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(2.8798e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0742, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0141, dtype=torch.float64)Grad:  tensor(29.1286, dtype=torch.float64)Grad:  tensor(-4.6745, dtype=torch.float64)Grad:  tensor(-0.2432, dtype=torch.float64)Grad:  tensor(0.2219, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 87 was 80.9%.\n",
      "current params: [tensor(22.5224, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6563, dtype=torch.float64), tensor(0.1432, dtype=torch.float64), tensor(0.0904, dtype=torch.float64)]\n",
      "tensor(0.8100, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(3.5069e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0763, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0049, dtype=torch.float64)Grad:  tensor(9.6280, dtype=torch.float64)Grad:  tensor(-1.6064, dtype=torch.float64)Grad:  tensor(0.0885, dtype=torch.float64)Grad:  tensor(0.2905, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 88 was 79.9%.\n",
      "current params: [tensor(20.3181, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6619, dtype=torch.float64), tensor(0.1427, dtype=torch.float64), tensor(0.0815, dtype=torch.float64)]\n",
      "tensor(0.7991, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(4.1330e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0799, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(9.9628e-09, dtype=torch.float64)Grad:  tensor(0.0042, dtype=torch.float64)Grad:  tensor(0.0745, dtype=torch.float64)Grad:  tensor(0.3393, dtype=torch.float64)Grad:  tensor(0.3480, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 89 was 78.4%.\n",
      "current params: [tensor(19.2160, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6647, dtype=torch.float64), tensor(0.1380, dtype=torch.float64), tensor(0.0717, dtype=torch.float64)]\n",
      "tensor(0.7845, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(4.4984e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0859, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(9.9768e-09, dtype=torch.float64)Grad:  tensor(0.0042, dtype=torch.float64)Grad:  tensor(0.0746, dtype=torch.float64)Grad:  tensor(0.3329, dtype=torch.float64)Grad:  tensor(0.3564, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 90 was 76.7%.\n",
      "current params: [tensor(18.6649, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6659, dtype=torch.float64), tensor(0.1313, dtype=torch.float64), tensor(0.0613, dtype=torch.float64)]\n",
      "tensor(0.7675, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(4.7208e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0935, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(9.9838e-09, dtype=torch.float64)Grad:  tensor(0.0042, dtype=torch.float64)Grad:  tensor(0.0745, dtype=torch.float64)Grad:  tensor(0.3329, dtype=torch.float64)Grad:  tensor(0.3645, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 91 was 74.9%.\n",
      "current params: [tensor(18.3894, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6665, dtype=torch.float64), tensor(0.1238, dtype=torch.float64), tensor(0.0508, dtype=torch.float64)]\n",
      "tensor(0.7491, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(4.8507e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.1019, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(9.9873e-09, dtype=torch.float64)Grad:  tensor(0.0042, dtype=torch.float64)Grad:  tensor(0.0742, dtype=torch.float64)Grad:  tensor(0.3367, dtype=torch.float64)Grad:  tensor(0.3734, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 92 was 72.9%.\n",
      "current params: [tensor(18.2516, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6667, dtype=torch.float64), tensor(0.1158, dtype=torch.float64), tensor(0.0402, dtype=torch.float64)]\n",
      "tensor(0.7293, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(4.9550e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.1112, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(9.9891e-09, dtype=torch.float64)Grad:  tensor(0.0042, dtype=torch.float64)Grad:  tensor(0.0738, dtype=torch.float64)Grad:  tensor(0.3421, dtype=torch.float64)Grad:  tensor(0.3820, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 93 was 70.7%.\n",
      "current params: [tensor(18.1827, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6667, dtype=torch.float64), tensor(0.1076, dtype=torch.float64), tensor(0.0296, dtype=torch.float64)]\n",
      "tensor(0.7078, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(5.3125e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.1214, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.0150, dtype=torch.float64)Grad:  tensor(26.4232, dtype=torch.float64)Grad:  tensor(-5.0909, dtype=torch.float64)Grad:  tensor(-0.9103, dtype=torch.float64)Grad:  tensor(0.1206, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 94 was 69.5%.\n",
      "current params: [tensor(16.1390, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6722, dtype=torch.float64), tensor(0.1133, dtype=torch.float64), tensor(0.0226, dtype=torch.float64)]\n",
      "tensor(0.6950, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(8.2156e-05, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.1243, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(0.8426, dtype=torch.float64)Grad:  tensor(1385.2465, dtype=torch.float64)Grad:  tensor(-285.8589, dtype=torch.float64)Grad:  tensor(-138.2269, dtype=torch.float64)Grad:  tensor(-72.2922, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 95 was 69.4%.\n",
      "current params: [tensor(9.1256, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.6949, dtype=torch.float64), tensor(0.1361, dtype=torch.float64), tensor(0.0392, dtype=torch.float64)]\n",
      "tensor(0.6946, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0262, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0885, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(-1.1107e-05, dtype=torch.float64)Grad:  tensor(-0.0056, dtype=torch.float64)Grad:  tensor(0.0539, dtype=torch.float64)Grad:  tensor(-0.7248, dtype=torch.float64)Grad:  tensor(-0.7129, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 96 was 67.6%.\n",
      "current params: [tensor(5.6189, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.7063, dtype=torch.float64), tensor(0.1477, dtype=torch.float64), tensor(0.0476, dtype=torch.float64)]\n",
      "tensor(0.6761, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0611, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0553, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(-7.2963e-06, dtype=torch.float64)Grad:  tensor(-0.0015, dtype=torch.float64)Grad:  tensor(0.0402, dtype=torch.float64)Grad:  tensor(-0.2577, dtype=torch.float64)Grad:  tensor(-0.2755, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 97 was 66.1%.\n",
      "current params: [tensor(3.8656, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.7120, dtype=torch.float64), tensor(0.1535, dtype=torch.float64), tensor(0.0519, dtype=torch.float64)]\n",
      "tensor(0.6620, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0805, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0382, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(-9.1590e-06, dtype=torch.float64)Grad:  tensor(-0.0017, dtype=torch.float64)Grad:  tensor(0.0288, dtype=torch.float64)Grad:  tensor(-0.1823, dtype=torch.float64)Grad:  tensor(-0.1643, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 98 was 65.3%.\n",
      "current params: [tensor(2.9891, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.7148, dtype=torch.float64), tensor(0.1564, dtype=torch.float64), tensor(0.0541, dtype=torch.float64)]\n",
      "tensor(0.6534, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>) Dimer yield:  tensor(0.0908, dtype=torch.float64, grad_fn=<AddBackward0>) ABT yield:  tensor(0.0296, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Grad:  tensor(-6.0952e-06, dtype=torch.float64)Grad:  tensor(-0.0004, dtype=torch.float64)Grad:  tensor(0.0237, dtype=torch.float64)Grad:  tensor(-0.0962, dtype=torch.float64)Grad:  tensor(-0.1040, dtype=torch.float64)\n",
      "Using CPU\n",
      "Max Possible Yield:  tensor(100., dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "Yield on sim. iteration 99 was 64.8%.\n",
      "optimization complete\n",
      "Final params: [tensor(2.9891, dtype=torch.float64), tensor(6.4000, dtype=torch.float64), tensor(0.7148, dtype=torch.float64), tensor(0.1564, dtype=torch.float64), tensor(0.0541, dtype=torch.float64)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<KineticAssembly_AD.vectorized_rxn_net.VectorizedRxnNet at 0x25bc71addd8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runtime=10\n",
    "vec_rn.reset(reset_params=True)\n",
    "optim = Optimizer(reaction_network=vec_rn,\n",
    "                  sim_runtime=runtime,\n",
    "                  optim_iterations=100,\n",
    "                  learning_rate=[2e-3,1e-8,2e-3, 2e-3, 2e-3],\n",
    "                  device='cpu',method=\"RMSprop\",mom=0.5)\n",
    "optim.rn.update_reaction_net(rn)\n",
    "optim.optimize(conc_scale=1e-1,mod_factor=1.0,conc_thresh=1e-1,mod_bool=True,yield_species=15,max_yield=0,chap_mode=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_var(v1,v2):\n",
    "    sq_sum=0\n",
    "    for i in range(len(v1)):\n",
    "        sq_sum=(v1[i]-v2[i])**2+sq_sum\n",
    "    \n",
    "    sq_sum = ((sq_sum)**0.5)/(len(v1)-1)\n",
    "    return(sq_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yields= []\n",
    "ab_yields=[]\n",
    "abt_yields=[]\n",
    "final_copies=[]\n",
    "final_rates = []\n",
    "final_t50 = []\n",
    "final_t85 = []\n",
    "final_t95 = []\n",
    "final_t99 = []\n",
    "times = []\n",
    "n_copies=len(rn.chap_uid_map.keys())\n",
    "n_rates = 2*n_copies\n",
    "for i in range(len(optim.final_yields)):\n",
    "    yields.append(optim.final_yields[i])\n",
    "    ab_yields.append(optim.dimer_max[i])\n",
    "    abt_yields.append(optim.chap_max[i])\n",
    "    times.append(optim.endtimes[i])\n",
    "    rate_params = []\n",
    "    copy_params = []\n",
    "    for j in range(n_copies):\n",
    "#     print(optim.final_solns[i])\n",
    "        copy_params.append(optim.final_solns[i][j])\n",
    "    for r in range(n_rates):\n",
    "        rate_params.append(optim.final_solns[i][r+n_copies])\n",
    "    \n",
    "    final_copies.append(copy_params)\n",
    "    final_rates.append(rate_params)\n",
    "    \n",
    "    if type(optim.final_t50[i])==int or float:\n",
    "        final_t50.append(1) \n",
    "    \n",
    "    if type(optim.final_t85[i])==int or float:\n",
    "        final_t85.append(1) \n",
    "    \n",
    "    if type(optim.final_t95[i])==int or float:\n",
    "        final_t95.append(1)\n",
    "    \n",
    "    if type(optim.final_t99[i])==int or float:\n",
    "        final_t99.append(1)\n",
    "    \n",
    "\n",
    "sort_indx=np.argsort(np.array(yields))\n",
    "\n",
    "\n",
    "# unsorted_yields=np.array(yields)\n",
    "# unsorted_copies = np.array(final_copies)\n",
    "# unsorted_rates = np.array(final_rates)\n",
    "\n",
    "# mask = unsorted_yields>0.2\n",
    "\n",
    "sorted_yields=np.array(yields)#[sort_indx]\n",
    "sorted_ab_yields=np.array(ab_yields)\n",
    "sorted_abt_yields=np.array(abt_yields)\n",
    "sorted_times = np.array(times)\n",
    "sorted_copies = np.array(final_copies)#[sort_indx]\n",
    "sorted_rates = np.array(final_rates)#[sort_indx]\n",
    "sorted_t50 = np.array(final_t50)\n",
    "sorted_t85 = np.array(final_t85)\n",
    "sorted_t95 = np.array(final_t95)\n",
    "sorted_t99 = np.array(final_t99)\n",
    "\n",
    "print(\"Parameters with Max yield: \")\n",
    "print(\"Max Yield: \",sorted_yields[-1],\"\\nParams: \",sorted_copies[-1], sorted_rates[-1])\n",
    "print(\"Iteration: \", sort_indx[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(optim.final_yielssds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Writing all solutions to a file\n",
    "# filename=\"Tetramer_Solutions_Chaperone_20kT_Mode2\"\n",
    "# with open(filename,'a') as fl:\n",
    "#     fl.write(\"#! Yield\\tAB_Yield\\tABT_Yield\\tTimes\\tC_x\\tk1\\tk2\\tt50\\tt85\\tt90\\n\")\n",
    "#     for i in range(1,len(sorted_yields)):\n",
    "#         fl.write(\"%f\" %(sorted_yields[i]))\n",
    "#         fl.write(\"\\t%f\" %(sorted_ab_yields[i]))\n",
    "#         fl.write(\"\\t%f\" %(sorted_abt_yields[i]))\n",
    "#         fl.write(\"\\t%f\" %(sorted_times[i]))\n",
    "        \n",
    "#         for j in range(sorted_copies[0].shape[0]):         \n",
    "#             fl.write(\"\\t%f\" %(sorted_copies[i][j]))\n",
    "#         for r1 in range(sorted_rates[0].shape[0]):\n",
    "#             fl.write(\"\\t%f\" %(sorted_rates[i][r1]))\n",
    "#         fl.write(\"\\t%f\\t%f\\t%f\\n\" %(sorted_t50[i],sorted_t85[i],sorted_t95[i]))\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_dict = {}\n",
    "sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "from reaction_network import gtostr\n",
    "for n in rn.network.nodes():\n",
    "    #print(n)\n",
    "    #print(rn.network.nodes()[n])\n",
    "    for k,v in rn.network[n].items():\n",
    "        uid = v['uid']\n",
    "        r1 = set(gtostr(rn.network.nodes[n]['struct']))\n",
    "        p = set(gtostr(rn.network.nodes[k]['struct']))\n",
    "        r2 = p-r1\n",
    "        reactants = (r1,r2)\n",
    "        uid_val = {'reactants':reactants,'kon':v['k_on'],'score':v['rxn_score'],'koff':v['k_off']}\n",
    "        if uid not in uid_dict.keys():\n",
    "            uid_dict[uid] = uid_val\n",
    "    print(gtostr(rn.network.nodes[n]['struct']))\n",
    "    #for r_set in rn.get_reactant_sets(n):\n",
    "    #    print(tuple(r_set))\n",
    "    #print(rn.network[n]['struct'])\n",
    "ind_sort = np.argsort(vec_rn.kon.detach().numpy())\n",
    "for i in ind_sort:\n",
    "    print(vec_rn.kon[i])\n",
    "    print(uid_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uid_dict = {}\n",
    "sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "from reaction_network import gtostr\n",
    "from torch import DoubleTensor as Tensor\n",
    "\n",
    "def get_max_edge(n):\n",
    "    \"\"\"\n",
    "    Calculates the max rate (k_on) for a given node\n",
    "    To find out the maximum flow path to the final complex starting from the current node.\n",
    "    \n",
    "    Can also calculate the total rate of consumption of a node by summing up all rates. \n",
    "    Can tell which component is used quickly.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        edges = rn.network.out_edges(n)\n",
    "        #Loop over all edges\n",
    "        #Get attributes\n",
    "        if len(edges)==0:\n",
    "            return(False)\n",
    "        kon_max = -1\n",
    "        next_node = -1\n",
    "        \n",
    "        kon_sum = 0\n",
    "        for edge in edges:\n",
    "            data = rn.network.get_edge_data(edge[0],edge[1])\n",
    "            #print(data)\n",
    "            #Get uid\n",
    "            uid = data['uid']\n",
    "            #Get updated kon\n",
    "            temp_kon = vec_rn.kon[uid]\n",
    "            kon_sum+=temp_kon\n",
    "            \n",
    "#             #Calculate k_off also\n",
    "#             std_c = Tensor([1.])\n",
    "#             l_kon = torch.log(temp_kon)\n",
    "#             l_koff = (vec_rn.rxn_score_vec[uid] * 1. / (self._R * self._T)) + l_kon + torch.log(std_c)\n",
    "            if temp_kon > kon_max:\n",
    "                kon_max = temp_kon\n",
    "                next_node=edge[1]\n",
    "        return(kon_max,next_node,kon_sum)\n",
    "    except Exception as err:\n",
    "        raise(err)\n",
    "\n",
    "pathway = []\n",
    "kon_sumarray = []\n",
    "total_con_rate = {}\n",
    "for n in rn.network.nodes():\n",
    "    \n",
    "    n_str = gtostr(rn.network.nodes[n]['struct']) \n",
    "    \n",
    "    paths = [n_str]\n",
    "    kon_sum = 0\n",
    "    temp_node = n\n",
    "    max_edge = True\n",
    "    consumption_rate = 0\n",
    "    if n < len(rn.network.nodes()):#num_monomers:\n",
    "#         print(\"Current node: \")\n",
    "#         print(n_str)\n",
    "        while max_edge:\n",
    "            max_edge = get_max_edge(temp_node)\n",
    "            if max_edge:\n",
    "                total_con_rate[gtostr(rn.network.nodes[temp_node]['struct'])] = max_edge[2]\n",
    "                temp_node = max_edge[1]\n",
    "                kon_sum += max_edge[0].item()\n",
    "                \n",
    "#                 print(\"Next node: \")\n",
    "#                 print(temp_node)\n",
    "\n",
    "                paths.append(gtostr(rn.network.nodes[temp_node]['struct']))\n",
    "            else:\n",
    "                break\n",
    "        pathway.append(paths)\n",
    "        kon_sumarray.append(kon_sum)\n",
    "        paths=[]\n",
    "\n",
    "print(pathway)\n",
    "print(kon_sumarray)\n",
    "#print(total_con_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in sorted(total_con_rate.items(),key=lambda x : x[1]):\n",
    "    print(k,\" : \", v.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's first visualize some of the data.\n",
    "\n",
    "**Without any optimization**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nodes_list = ['A','B','S','M','AB','BMS','ABS','AMS','ABMS','AM','AS']\n",
    "#nodes_list = ['A','B','ABMS']\n",
    "optim.plot_observable(0,nodes_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**After 750 optimization iterations**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optim.plot_observable(-1,nodes_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.plot_yield()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It seems like we've found a stable solution that produces greater yield than equilibrium. This should be thermodynamically\n",
    "impossible. Let's try to find an explanation. We'll run simulations using the learned optimal parameters at a few different\n",
    "timescales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "optim_rn = optim.rn\n",
    "for i, runtime in enumerate([1, 8, 64]):\n",
    "    optim_rn.reset()\n",
    "    sim = VecSim(optim_rn, runtime, device='cpu')\n",
    "    y = sim.simulate()\n",
    "    sim.plot_observable(nodes_list,ax=ax[i],)\n",
    "    ax[i].set_title(\"runtime: \" + str(runtime) + \" seconds\")\n",
    "fig.set_size_inches(18, 6)\n",
    "node_map = {}\n",
    "for node in rn.network.nodes():\n",
    "    node_map[gtostr(rn.network.nodes[node]['struct'])] = node\n",
    "\n",
    "print(node_map)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_map = {}\n",
    "for node in rn.network.nodes():\n",
    "    node_map[gtostr(rn.network.nodes[node]['struct'])] = node\n",
    "\n",
    "print(node_map)\n",
    "def get_max_edge(n):\n",
    "    \"\"\"\n",
    "    Calculates the max rate (k_on) for a given node\n",
    "    To find out the maximum flow path to the final complex starting from the current node.\n",
    "    \n",
    "    Can also calculate the total rate of consumption of a node by summing up all rates. \n",
    "    Can tell which component is used quickly.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        edges = rn.network.out_edges(n)\n",
    "        #Loop over all edges\n",
    "        #Get attributes\n",
    "        kon_max = -1\n",
    "        next_node = -1\n",
    "\n",
    "        kon_sum = 0\n",
    "        total_flux_outedges = 0\n",
    "        total_flux_inedges = 0\n",
    "        if len(edges)==0:\n",
    "            return(False)\n",
    "            \n",
    "        for edge in edges:\n",
    "            data = rn.network.get_edge_data(edge[0],edge[1])\n",
    "            #print(data)\n",
    "            #Get uid\n",
    "            uid = data['uid']\n",
    "\n",
    "            #Get updated kon\n",
    "            temp_kon = vec_rn.kon[uid]\n",
    "            kon_sum+=temp_kon\n",
    "            \n",
    "            if temp_kon > kon_max:\n",
    "                kon_max = temp_kon\n",
    "                next_node=edge[1]\n",
    "             \n",
    "        return(kon_max,next_node,kon_sum)\n",
    "    except Exception as err:\n",
    "        raise(err)\n",
    "\n",
    "        \n",
    "def get_node_flux(n):\n",
    "    total_flux_outedges = 0\n",
    "    total_flux_inedges = 0\n",
    "    #Go over all the out edges\n",
    "    edges_out = rn.network.out_edges(n)\n",
    "    if len(edges_out)>0:\n",
    "\n",
    "        for edge in edges_out:\n",
    "            data = rn.network.get_edge_data(edge[0],edge[1])\n",
    "            #print(data)\n",
    "            #Get uid\n",
    "            uid = data['uid']\n",
    "\n",
    "            #Get updated kon\n",
    "            temp_kon = vec_rn.kon[uid]\n",
    "\n",
    "            #Calculate k_off also\n",
    "            std_c = Tensor([1.])\n",
    "            l_kon = torch.log(temp_kon)\n",
    "            l_koff = (vec_rn.rxn_score_vec[uid] * 1. / (vec_rn._R * vec_rn._T)) + l_kon + torch.log(std_c)\n",
    "            koff = torch.exp(l_koff)\n",
    "\n",
    "            #Getting conc. of reactants and products\n",
    "            #Get product\n",
    "            prod = gtostr(rn.network.nodes[edge[1]]['struct']) \n",
    "            #Get other reactant\n",
    "            react = \"\".join(sorted(list(set(prod) - set(gtostr(rn.network.nodes[edge[0]]['struct']) ))))\n",
    "\n",
    "            #Net flux from this edge = Generation - consumption\n",
    "            edge_flux = koff*vec_rn.copies_vec[edge[1]] - temp_kon*(vec_rn.copies_vec[edge[0]])*(vec_rn.copies_vec[node_map[react]])\n",
    "            #edge_flux = koff*vec_rn.copies_vec[edge[1]] \n",
    "\n",
    "            print(\"Reaction: \", gtostr(rn.network.nodes[edge[0]]['struct']), \"+\",react,\" -> \",prod)\n",
    "            print(\"Net flux: \",edge_flux)\n",
    "            print(\"kon : \",temp_kon)\n",
    "            print(\"koff: \",koff)\n",
    "            print(\"Reaction data OUTWARD: \")\n",
    "            print(data)\n",
    "\n",
    "            total_flux_outedges+=edge_flux\n",
    "    \n",
    "    #Now go over all the in edges\n",
    "    edges_in = rn.network.in_edges(n)\n",
    "    react_list = []\n",
    "    if len(edges_in) > 0:\n",
    "        for edge in edges_in:\n",
    "            if edge[0] in react_list:\n",
    "                continue\n",
    "            data = rn.network.get_edge_data(edge[0],edge[1])\n",
    "            uid = data['uid']\n",
    "\n",
    "\n",
    "            #Get generation rates; which would be kon\n",
    "            temp_kon = vec_rn.kon[uid]\n",
    "\n",
    "            #Get consumption rates; which is k_off\n",
    "            std_c = Tensor([1.])\n",
    "            l_kon = torch.log(temp_kon)\n",
    "            l_koff = (vec_rn.rxn_score_vec[uid] * 1. / (vec_rn._R * vec_rn._T)) + l_kon + torch.log(std_c)\n",
    "            koff = torch.exp(l_koff)\n",
    "\n",
    "            #Get conc. of reactants and products\n",
    "            prod = gtostr(rn.network.nodes[edge[1]]['struct'])\n",
    "            #Get other reactant\n",
    "            react = \"\".join(sorted(list(set(prod) - set(gtostr(rn.network.nodes[edge[0]]['struct']) ))))\n",
    "            react_list.append(node_map[react])\n",
    "            #Net flux from this edge = Generation - consumption\n",
    "            edge_flux_in = temp_kon*(vec_rn.copies_vec[edge[0]])*(vec_rn.copies_vec[node_map[react]])- koff*vec_rn.copies_vec[edge[1]]\n",
    "            #edge_flux_in = koff*vec_rn.copies_vec[edge[1]]\n",
    "            \n",
    "\n",
    "\n",
    "            print(\"Reaction: \", prod ,\" -> \",gtostr(rn.network.nodes[edge[0]]['struct']), \"+\",react)\n",
    "            print(\"Net flux: \",edge_flux_in)\n",
    "            print(\"kon : \",temp_kon)\n",
    "            print(\"koff: \",koff)\n",
    "            print(\"Raction data INWARD: \")\n",
    "            print(data)\n",
    "\n",
    "            total_flux_inedges+=edge_flux_in\n",
    "    net_node_flux = total_flux_outedges + total_flux_inedges\n",
    "    \n",
    "    return(net_node_flux)\n",
    "    \n",
    "pathway = []\n",
    "kon_sumarray = []\n",
    "total_con_rate = {}\n",
    "net_flux = {}\n",
    "for n in rn.network.nodes():\n",
    "    \n",
    "    n_str = gtostr(rn.network.nodes[n]['struct']) \n",
    "    \n",
    "    paths = [n_str]\n",
    "    kon_sum = 0\n",
    "    temp_node = n\n",
    "    max_edge = True\n",
    "    consumption_rate = 0\n",
    "    if n < len(rn.network.nodes()):#num_monomers:\n",
    "#         print(\"Current node: \")\n",
    "#         print(n_str)\n",
    "        while max_edge:\n",
    "            max_edge = get_max_edge(temp_node)\n",
    "            if max_edge:\n",
    "                total_con_rate[gtostr(rn.network.nodes[temp_node]['struct'])] = max_edge[2]\n",
    "                \n",
    "                temp_node = max_edge[1]\n",
    "                kon_sum += max_edge[0].item()\n",
    "                \n",
    "                \n",
    "#                 print(\"Next node: \")\n",
    "#                 print(temp_node)\n",
    "\n",
    "                paths.append(gtostr(rn.network.nodes[temp_node]['struct']))\n",
    "            else:\n",
    "                break\n",
    "        pathway.append(paths)\n",
    "        kon_sumarray.append(kon_sum)\n",
    "        paths=[]\n",
    "    print(\"-------------------------------------------------------------------------------\")\n",
    "    print(\"-------------------------------------------------------------------------------\")\n",
    "    print(\"|                                                                             |\")\n",
    "    node_flux = get_node_flux(n)\n",
    "    net_flux[gtostr(rn.network.nodes[n]['struct'])] = node_flux\n",
    "    print(\"|                                                                             |\")\n",
    "    print(\"-------------------------------------------------------------------------------\")\n",
    "    print(\"-------------------------------------------------------------------------------\")\n",
    "\n",
    "print(pathway)\n",
    "print(kon_sumarray)\n",
    "\n",
    "#print(total_con_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in sorted(net_flux.items(),key=lambda x : x[1]):\n",
    "    print(k,\" : \", v)\n",
    "\n",
    "print(vec_rn.copies_vec)\n",
    "print(vec_rn.kon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(solution)\n",
    "poly_system = EquilibriumSolver(rn)\n",
    "solution = poly_system.solve(init_val=vec_rn.copies_vec.detach().numpy().tolist())\n",
    "#solution = poly_system.solve(verifyBool = False)\n",
    "if solution == None:\n",
    "    print(\"No Equilibrium solution\")\n",
    "else:\n",
    "    print(solution)\n",
    "    print(\"Equilibrium expected yield: \", 100 * solution[-1] / min(vec_rn.initial_copies[:vec_rn.num_monomers]), '%')\n",
    "print(vec_rn.kon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the equilibrium reached by the system still matches the equilibrium solution. We have however found a set of parameters that can increase available complete AP2 at some point before equilibrium to levels significantly higher than at equilibrium. We don't observe any trapping, but have uncovered an interesting effect. \n",
    "\n",
    "Now we'll move on to looking at ARP23. This is 7 subunits, which drastically increases the number of possible reactions. Expect longer runtimes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
