{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Framework using auto-diff to optimize binding rates\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary modules\n",
    "\n",
    "Every Jupyter Notebook requires the path to the KineticAssembly_AD modules (.py files in the root directory) to be mentioned. This can be done by adding the path to the 'PATH' variable of the system environment. \n",
    "\n",
    "Additonal modules are also imported which are required to run any analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# make sure jupyter path is correct for loading local moudules\n",
    "import sys\n",
    "path_to_repo=\"C:\\\\Users\\\\denys\\\\AMGEN\\\\\"   \n",
    "#Insert your path here\n",
    "# path_to_repo=\"\"\n",
    "sys.path.append(path_to_repo)\n",
    "\n",
    "import copy\n",
    "from KineticAssembly_AD import ReactionNetwork, VectorizedRxnNet, VecSim, Optimizer, EquilibriumSolver\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch import DoubleTensor as Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Reaction Network\n",
    "Before we begin to run the optimization routine, we need to create a Reaction Network that stores all the parameters required to run a simulation and other routines. The Reaction Network can be created by reading an input file. More information on how to create an input file can be found in the User Guide. \n",
    "\n",
    "Here a simple trimer model is used to run a simulation.\n",
    "#### Read the corresponding input file and call the ReactionNetwork class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['default_assoc', 1.0]\n",
      "['monomer_add_only', True]\n",
      "['homo_rates', True]\n",
      "[(0, {'struct': <networkx.classes.graph.Graph object at 0x000001F355224CC0>, 'copies': tensor([100.], dtype=torch.float64), 'subunits': 1}), (1, {'struct': <networkx.classes.graph.Graph object at 0x000001F34D035898>, 'copies': tensor([100.], dtype=torch.float64), 'subunits': 1}), (2, {'struct': <networkx.classes.graph.Graph object at 0x000001F34D035978>, 'copies': tensor([100.], dtype=torch.float64), 'subunits': 1}), (3, {'struct': <networkx.classes.graph.Graph object at 0x000001F34D0359B0>, 'copies': tensor([100.], dtype=torch.float64), 'subunits': 1})]\n",
      "New node added - Node index: 4 ; Node label: AB \n",
      "New node added - Node index: 5 ; Node label: AD \n",
      "New node added - Node index: 6 ; Node label: BC \n",
      "New node added - Node index: 7 ; Node label: CD \n",
      "New node added - Node index: 8 ; Node label: ABC \n",
      "New node added - Node index: 9 ; Node label: ACD \n",
      "New node added - Node index: 10 ; Node label: ABD \n",
      "New node added - Node index: 11 ; Node label: BCD \n",
      "New node added - Node index: 12 ; Node label: ABCD \n",
      "Reaction Network Completed\n"
     ]
    }
   ],
   "source": [
    "base_input = './tetramer_rategrowth_ring.pwr'\n",
    "rn = ReactionNetwork(base_input, one_step=True)\n",
    "rn.resolve_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking reaction network\n",
    "\n",
    "The ReactionNetwork is a networkx object which creates a graph network with each node as species that can be present in the system according to the binding rules given in the input file. Each node has a unique index number that can be used to access attributes stored for that species. Each edge represents a reaction and is associated with a unique reaction_id, on and off rates and the dG value for that reaction.\n",
    "\n",
    "\n",
    "After creating a Reaction Network we can looping over all network nodes to check if all species are created\n",
    "Creating a dictionary for later reference. This dictionary holds the reactants as keys and values as the reaction index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Species present in the Reaction Network: \n",
      "Index  --  Species\n",
      "  0    --  A     \n",
      "  1    --  C     \n",
      "  2    --  B     \n",
      "  3    --  D     \n",
      "  4    --  AB    \n",
      "  5    --  AD    \n",
      "  6    --  BC    \n",
      "  7    --  CD    \n",
      "  8    --  ABC   \n",
      "  9    --  ACD   \n",
      " 10    --  ABD   \n",
      " 11    --  BCD   \n",
      " 12    --  ABCD  \n",
      "\n",
      "Total Number of Reactions:  16\n",
      "Total Number of Species:  13\n",
      "\n",
      "{(0, 4): 0, (0, 5): 1, (0, 8): 12, (0, 9): 13, (0, 12): 15, (1, 6): 2, (1, 7): 3, (1, 8): 4, (1, 9): 5, (1, 12): 14, (2, 4): 0, (2, 6): 2, (2, 10): 6, (2, 11): 7, (2, 12): 8, (3, 5): 1, (3, 7): 3, (3, 10): 9, (3, 11): 10, (3, 12): 11, (4, 8): 4, (4, 10): 9, (5, 9): 5, (5, 10): 6, (6, 11): 10, (6, 8): 12, (7, 11): 7, (7, 9): 13, (8, 12): 11, (9, 12): 8, (10, 12): 14, (11, 12): 15}\n"
     ]
    }
   ],
   "source": [
    "uid_dict = {}\n",
    "sys.path.append(\"../../\")\n",
    "import numpy as np\n",
    "from reaction_network import gtostr\n",
    "\n",
    "print(\"Species present in the Reaction Network: \")\n",
    "print(\"%3s  %2s  %2s\" %(\"Index\",\"--\",'Species'))\n",
    "for n in rn.network.nodes():\n",
    "    print(\"%3d  %4s  %-6s\" %(n,\"--\",gtostr(rn.network.nodes[n]['struct'])))\n",
    "    for k,v in rn.network[n].items():\n",
    "        uid = v['uid']\n",
    "        r1 = set(gtostr(rn.network.nodes[n]['struct']))\n",
    "        p = set(gtostr(rn.network.nodes[k]['struct']))\n",
    "        r2 = p-r1\n",
    "        reactants = (r1,r2)\n",
    "        uid_dict[(n,k)] = uid\n",
    "\n",
    "print()\n",
    "print(\"Total Number of Reactions: \",rn._rxn_count)\n",
    "print(\"Total Number of Species: \",len(rn.network.nodes()))\n",
    "        \n",
    "# Dictionary that stores source,destination of an edge and maps it to its unique id\n",
    "#Key : (First Reactant, Product)\n",
    "#Value : (Reaction_id)\n",
    "print()\n",
    "print(uid_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the initial parameter values \n",
    "The next step is to define the initial conditions for the simulation. The initial concentrations are specified from the input file. However, the initial value of the association rates can be specified either through the input file \n",
    "\n",
    "From the user_input file, currently the code only allows 1 value to be read (from default_assoc parameter).\n",
    "\n",
    "To set starting rates to different values the next code block takes in a list/array of all rxn rates and updates them in the reaction network object.\n",
    "\n",
    "For a hetero-trimer there are 6 reaction rates.\n",
    "Also defines the Vectorized Rxn Net class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 0}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 1}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 12}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 13}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 15}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 2}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 3}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 4}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 5}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 14}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 0}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 2}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 6}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 7}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 8}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 1}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 3}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 9}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 10}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 11}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 4}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 9}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 5}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 6}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 10}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 12}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 7}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 13}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 11}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 8}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 14}\n",
      "{'k_on': tensor(1., dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 15}\n",
      "Reaction rates:  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       dtype=torch.float64, grad_fn=<CopySlices>)\n",
      "dGs:  tensor([-20., -20., -20., -20., -20., -20., -20., -20., -40., -20., -20., -40.,\n",
      "        -20., -20., -40., -40.], dtype=torch.float64)\n",
      "Species Concentrations:  tensor([100., 100., 100., 100.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.], dtype=torch.float64)\n",
      "Shifting to device:  cpu\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       dtype=torch.float64, grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "#Define an empty torch tensor with length equal to number of reactions\n",
    "new_kon = torch.zeros([rn._rxn_count], requires_grad=True).double()\n",
    "\n",
    "#Set the initial value of the association rates\n",
    "#Note that this code sets all association rates at the same value\n",
    "\n",
    "#To set individual rates to different values, we need to create an list/array with different values.\n",
    "'''\n",
    "length = rn._rxn_count\n",
    "min_val = 0.1\n",
    "max_val = 3.0\n",
    "init_val = []\n",
    "\n",
    "np.random.seed(42)\n",
    "for i in range(length):\n",
    "    # Linearly interpolate the current maximum from min_val up to max_val\n",
    "    current_max = min_val + (i / (length - 1)) * (max_val - min_val)\n",
    "    # Draw a random float uniformly between min_val and current_max\n",
    "    val = np.random.uniform(min_val, current_max)\n",
    "    init_val.append(val)\n",
    "\n",
    "new_kon = new_kon + Tensor(init_val)\n",
    "    \n",
    "'''\n",
    "#Else we could assign all initial values to be equal to 1; performs bad for lower indeces\n",
    "init_val = 1\n",
    "new_kon = new_kon + Tensor([init_val])\n",
    "\n",
    "\n",
    "\n",
    "update_kon_dict = {}\n",
    "for edge in rn.network.edges:\n",
    "    update_kon_dict[edge] = new_kon[uid_dict[edge]]\n",
    "\n",
    "nx.set_edge_attributes(rn.network,update_kon_dict,'k_on')\n",
    "for edge in rn.network.edges:\n",
    "    print(rn.network.get_edge_data(edge[0],edge[1]))\n",
    "\n",
    "vec_rn = VectorizedRxnNet(rn, dev='cpu')\n",
    "print(vec_rn.kon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Species present in the Reaction Network: \n",
      "Index  --  Species\n",
      "  0    --  A     \n",
      "  1    --  C     \n",
      "  2    --  B     \n",
      "  3    --  D     \n",
      "  4    --  AB    \n",
      "  5    --  AD    \n",
      "  6    --  BC    \n",
      "  7    --  CD    \n",
      "  8    --  ABC   \n",
      "  9    --  ACD   \n",
      " 10    --  ABD   \n",
      " 11    --  BCD   \n",
      " 12    --  ABCD  \n",
      "\n",
      "Initial Binding Rates: \n",
      "Reaction        Id           kon\n",
      "\n",
      "('A', 'B')+   B       0 \t    1.00\n",
      "('A', 'D')+   D       1 \t    1.00\n",
      "('C', 'B')+   B       2 \t    1.00\n",
      "('C', 'D')+   D       3 \t    1.00\n",
      "('C', 'AB')+  AB       4 \t    1.00\n",
      "('C', 'DA')+  DA       5 \t    1.00\n",
      "('B', 'DA')+  DA       6 \t    1.00\n",
      "('B', 'DC')+  DC       7 \t    1.00\n",
      "('B', 'DCA')+ DCA       8 \t    1.00\n",
      "('D', 'AB')+  AB       9 \t    1.00\n",
      "('D', 'CB')+  CB      10 \t    1.00\n",
      "('D', 'ACB')+ ACB      11 \t    1.00\n",
      "('A', 'CB')+  CB      12 \t    1.00\n",
      "('A', 'DC')+  DC      13 \t    1.00\n",
      "('C', 'DAB')+ DAB      14 \t    1.00\n",
      "('A', 'DCB')+ DCB      15 \t    1.00\n"
     ]
    }
   ],
   "source": [
    "uid_dict = {}\n",
    "sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "from reaction_network import gtostr\n",
    "\n",
    "print(\"Species present in the Reaction Network: \")\n",
    "print(\"%3s  %2s  %2s\" %(\"Index\",\"--\",'Species'))\n",
    "\n",
    "for n in rn.network.nodes():\n",
    "    #print(n)\n",
    "    #print(rn.network.nodes()[n])\n",
    "    print(\"%3d  %4s  %-6s\" %(n,\"--\",gtostr(rn.network.nodes[n]['struct'])))\n",
    "    for k,v in rn.network[n].items():\n",
    "        uid = v['uid']\n",
    "        r1 = set(gtostr(rn.network.nodes[n]['struct']))\n",
    "        p = set(gtostr(rn.network.nodes[k]['struct']))\n",
    "        r2 = p-r1\n",
    "        reactants = (\"\".join(list(r1)),\"\".join(list(r2)))\n",
    "        uid_val = {'reactants':reactants,'kon':v['k_on'],'score':v['rxn_score'],'koff':v['k_off'],'uid':uid}\n",
    "        if uid not in uid_dict.keys():\n",
    "            uid_dict[uid] = uid_val\n",
    "\n",
    "print()\n",
    "print(\"Initial Binding Rates: \")\n",
    "\n",
    "ind_sort = np.argsort(vec_rn.kon.detach().numpy())\n",
    "print(\"%-16s%-3s %12s\" %(\"Reaction\",\"Id\",\"kon\"))\n",
    "print()\n",
    "   \n",
    "for i in ind_sort:\n",
    "    print(\"%-4s%1s%4s %7d \\t%8.2f\" %(uid_dict[i]['reactants'],\"+\",uid_dict[i]['reactants'][1],uid_dict[i]['uid'],vec_rn.kon[i].item()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Using the optimizer ##\n",
    "\n",
    "### Define an instance of the optimizer class\n",
    "#### Input Arguments:\n",
    "\n",
    "reaction_network : Input the vectorized rxn network\n",
    "\n",
    "sim_runtime: The runtime of the kinetic simulation. Needs to be same as the time over the experimental reaction data.\n",
    "\n",
    "optim_iterations: No. of iterations to run the optimization. Can start at low values(100) and increase depending upon memory usage.\n",
    "\n",
    "learning_rate = The size of the gradient descent step for updating parameter values. Needs to be atleast (1e-3-1e-1)* min{parameter value}. If learning rate is too high, it can take a longer step and sometimes lead to negative value of parameters which is unphysical. Requires some trial runs to find the best value. \n",
    "\n",
    "device: cpu or gpu\n",
    "\n",
    "method: Choose which pytorch based optimized to use for gradient descent - Adam or RMSprop\n",
    "\n",
    "mom: Only for RMSprop method. Use momentum term during gradient descent. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.utils.benchmark as benchmark\n",
    "import time as time_mod\n",
    "\n",
    "t1 = time_mod.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vec_rn.reset(reset_params=True)\n",
    "optim = Optimizer(reaction_network=vec_rn,\n",
    "                  sim_runtime=1,\n",
    "                  optim_iterations=500,\n",
    "                  learning_rate=1e-2,\n",
    "                  device='cpu',method=\"RMSprop\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the optimization method\n",
    "\n",
    "#### Input arguments\n",
    "\n",
    "conc_scale: Controls the conc step at each iteration. Since the numerical integration is not performed over fixed time steps but over fixed conc. steps. For e.g. for a value of 1uM, at each iteration step a total of app. 1uM is reacted (includes all species). Can be run using the default value. A general rule is use conc_scale = 0.01 * Max_yield\n",
    "\n",
    "conc_thresh: This can be used to periodically decrease the conc_scale parameter. After each iteration if the conc_scale is greater than the conc_thresh, then the conc_scale is decreased by mod_factor. Can be run using the default value. \n",
    "\n",
    "mod_bool: This argument is necessary to fix the mass balance criteria. Sometimes if the conc_scale is large, then the simulation can lead to a higher consumption of a particular species which is very low in conc, and create more of this species out of nothing. Default value:True\n",
    "\n",
    "max_thresh: Max. allowed values of parameters being updated. Beyond this maximum a penalty is imposed on the cost function. (Regularization)\n",
    "\n",
    "max_yield: It is a control variable that is used to store the updated parameter values over all iterations for further analysis. The parameter values are stored only if the current yield exceed this max_yield. \n",
    "\n",
    "yield_species: Yield of the species being optimized(node index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reaction Parameters before optimization: \n",
      "[Parameter containing:\n",
      "tensor([0., 0., 0.], dtype=torch.float64, requires_grad=True)]\n",
      "Optimizer State: <bound method Optimizer.state_dict of RMSprop (\n",
      "Parameter Group 0\n",
      "    alpha: 0.99\n",
      "    centered: False\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    weight_decay: 0\n",
      ")>\n",
      "Using CPU\n",
      "Yield on sim. iteration 0 was 32.1%.\n",
      "current params: tensor([0., 0., 0.], dtype=torch.float64)\n",
      "current ratio: nan\n",
      "tensor(0.3212, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.3211737579657142\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.3211737579657142\n",
      "t50: -1\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 1 was 37.8%.\n",
      "current params: tensor([-0.1000,  0.1000,  0.1000], dtype=torch.float64)\n",
      "current ratio: -0.9999998964751085\n",
      "tensor(0.3781, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.3781363519712918\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.3781363519712918\n",
      "t50: -1\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 2 was 38.4%.\n",
      "current params: tensor([-0.1062,  0.0156,  0.1647], dtype=torch.float64)\n",
      "current ratio: -1.5504877531313028\n",
      "tensor(0.3846, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.38458174606918305\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.38458174606918305\n",
      "t50: -1\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 3 was 42.7%.\n",
      "current params: tensor([-0.1822,  0.0665,  0.2324], dtype=torch.float64)\n",
      "current ratio: -1.2752346721963423\n",
      "tensor(0.4272, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.42718031829408676\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.42718031829408676\n",
      "t50: -1\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 4 was 45.6%.\n",
      "current params: tensor([-0.2327,  0.0782,  0.2886], dtype=torch.float64)\n",
      "current ratio: -1.2404597217819775\n",
      "tensor(0.4565, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.4565045115212515\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.4565045115212515\n",
      "t50: -1\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 5 was 48.7%.\n",
      "current params: tensor([-0.2847,  0.1031,  0.3415], dtype=torch.float64)\n",
      "current ratio: -1.199394740211254\n",
      "tensor(0.4877, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.4877297794037155\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.4877297794037155\n",
      "t50: -1\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 6 was 51.9%.\n",
      "current params: tensor([-0.3344,  0.1318,  0.3911], dtype=torch.float64)\n",
      "current ratio: -1.169442179586022\n",
      "tensor(0.5195, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.5194560917996617\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.5194560917996617\n",
      "t50: 3.5289120417759294\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 7 was 57.1%.\n",
      "current params: tensor([-0.4104,  0.2193,  0.4500], dtype=torch.float64)\n",
      "current ratio: -1.0964124491540723\n",
      "tensor(0.5715, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.5715100469186813\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.5715100469186813\n",
      "t50: 2.454983086477434\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 8 was 59.1%.\n",
      "current params: tensor([-0.4405,  0.2247,  0.4906], dtype=torch.float64)\n",
      "current ratio: -1.1137061967491941\n",
      "tensor(0.5920, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.5919817173795823\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.5919817173795823\n",
      "t50: 2.3711797793734393\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 9 was 62.2%.\n",
      "current params: tensor([-0.4847,  0.2596,  0.5350], dtype=torch.float64)\n",
      "current ratio: -1.103891527185961\n",
      "tensor(0.6230, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.6229792904129312\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.6229792904129312\n",
      "t50: 2.239163093355802\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 10 was 64.7%.\n",
      "current params: tensor([-0.5192,  0.2780,  0.5745], dtype=torch.float64)\n",
      "current ratio: -1.1064285974676897\n",
      "tensor(0.6473, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.6472715422209103\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.6472715422209103\n",
      "t50: 2.18897791513672\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 11 was 66.5%.\n",
      "current params: tensor([-0.5457,  0.2809,  0.6096], dtype=torch.float64)\n",
      "current ratio: -1.117164943827045\n",
      "tensor(0.6656, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.665610664154071\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.665610664154071\n",
      "t50: 2.185176987822937\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 12 was 68.1%.\n",
      "current params: tensor([-0.5705,  0.2804,  0.6432], dtype=torch.float64)\n",
      "current ratio: -1.127418468905951\n",
      "tensor(0.6813, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.6812581845885926\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.6812581845885926\n",
      "t50: 2.188554711641852\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 13 was 70.2%.\n",
      "current params: tensor([-0.6006,  0.2918,  0.6779], dtype=torch.float64)\n",
      "current ratio: -1.1287398989204607\n",
      "tensor(0.7021, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.702059855870454\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.702059855870454\n",
      "t50: 2.176999375877476\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 14 was 71.6%.\n",
      "current params: tensor([-0.6242,  0.2893,  0.7094], dtype=torch.float64)\n",
      "current ratio: -1.1364955267460095\n",
      "tensor(0.7169, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7169473914614112\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7169473914614112\n",
      "t50: 2.187237616112254\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 15 was 73.4%.\n",
      "current params: tensor([-0.6510,  0.2945,  0.7414], dtype=torch.float64)\n",
      "current ratio: -1.1387994253114837\n",
      "tensor(0.7342, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7342413515029205\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7342413515029205\n",
      "t50: 2.19495294599022\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 16 was 75.0%.\n",
      "current params: tensor([-0.6767,  0.2974,  0.7723], dtype=torch.float64)\n",
      "current ratio: -1.1412935912854878\n",
      "tensor(0.7504, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7504161602489532\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7504161602489532\n",
      "t50: 2.2029218200011567\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 17 was 76.3%.\n",
      "current params: tensor([-0.6994,  0.2939,  0.8015], dtype=torch.float64)\n",
      "current ratio: -1.1458899204739241\n",
      "tensor(0.7636, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7635663660023034\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7635663660023034\n",
      "t50: 2.221379201402314\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 18 was 78.8%.\n",
      "current params: tensor([-0.7373,  0.3274,  0.8364], dtype=torch.float64)\n",
      "current ratio: -1.1344698468184495\n",
      "tensor(0.7886, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7885726520109434\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7885726520109434\n",
      "t50: 2.2011738828251266\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 19 was 80.0%.\n",
      "current params: tensor([-0.7583,  0.3221,  0.8640], dtype=torch.float64)\n",
      "current ratio: -1.1393000231891028\n",
      "tensor(0.8003, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8002613383265604\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8002613383265604\n",
      "t50: 2.2253308539039693\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 20 was 81.1%.\n",
      "current params: tensor([-0.7794,  0.3166,  0.8911], dtype=torch.float64)\n",
      "current ratio: -1.1433303636122223\n",
      "tensor(0.8116, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.811578690919632\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.811578690919632\n",
      "t50: 2.2490428187681584\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 21 was 82.2%.\n",
      "current params: tensor([-0.8008,  0.3122,  0.9179], dtype=torch.float64)\n",
      "current ratio: -1.146199979101368\n",
      "tensor(0.8229, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8229410781251997\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8229410781251997\n",
      "t50: 2.272686331349512\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 22 was 83.4%.\n",
      "current params: tensor([-0.8227,  0.3091,  0.9445], dtype=torch.float64)\n",
      "current ratio: -1.148053158113\n",
      "tensor(0.8343, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8343268890112916\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8343268890112916\n",
      "t50: 2.299568291431706\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 23 was 85.6%.\n",
      "current params: tensor([-0.8580,  0.3404,  0.9766], dtype=torch.float64)\n",
      "current ratio: -1.1382812390831683\n",
      "tensor(0.8561, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8560770471732632\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8560770471732632\n",
      "t50: 2.293137202997452\n",
      "t85: 23.19053010392828\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 24 was 86.5%.\n",
      "current params: tensor([-0.8777,  0.3338,  1.0017], dtype=torch.float64)\n",
      "current ratio: -1.1412705975850397\n",
      "tensor(0.8652, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8652087479493149\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8652087479493149\n",
      "t50: 2.3207184312623856\n",
      "t85: 18.155951085760556\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 25 was 88.3%.\n",
      "current params: tensor([-0.9102,  0.3603,  1.0321], dtype=torch.float64)\n",
      "current ratio: -1.1339656733126904\n",
      "tensor(0.8839, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8838538145422115\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8838538145422115\n",
      "t50: 2.323603458852165\n",
      "t85: 14.343653710333069\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 26 was 89.1%.\n",
      "current params: tensor([-0.9286,  0.3518,  1.0560], dtype=torch.float64)\n",
      "current ratio: -1.1372347781070506\n",
      "tensor(0.8915, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8915486891597744\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8915486891597744\n",
      "t50: 2.3552008365491734\n",
      "t85: 13.548026188716666\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 27 was 89.0%.\n",
      "current params: tensor([-0.9350,  0.3157,  1.0743], dtype=torch.float64)\n",
      "current ratio: -1.1490433383135572\n",
      "tensor(0.8903, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8902559118792714\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8902559118792714\n",
      "t50: 2.4090862383043627\n",
      "t85: 13.84988560700759\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 28 was 90.0%.\n",
      "current params: tensor([-0.9572,  0.3165,  1.0994], dtype=torch.float64)\n",
      "current ratio: -1.1485476693063827\n",
      "tensor(0.9001, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9000622452278938\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9000622452278938\n",
      "t50: 2.4370456687913555\n",
      "t85: 13.032296195832503\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 29 was 90.6%.\n",
      "current params: tensor([-0.9751,  0.3073,  1.1223], dtype=torch.float64)\n",
      "current ratio: -1.1508887796938627\n",
      "tensor(0.9065, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9065218615915164\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9065218615915164\n",
      "t50: 2.470378858505095\n",
      "t85: 12.676793731665919\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 30 was 91.2%.\n",
      "current params: tensor([-0.9935,  0.2989,  1.1451], dtype=torch.float64)\n",
      "current ratio: -1.15259999103167\n",
      "tensor(0.9130, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9129690607140326\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9129690607140326\n",
      "t50: 2.503938396948911\n",
      "t85: 12.3957426947287\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 31 was 91.9%.\n",
      "current params: tensor([-1.0130,  0.2934,  1.1682], dtype=torch.float64)\n",
      "current ratio: -1.153225136610993\n",
      "tensor(0.9199, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9198740361273046\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9198740361273046\n",
      "t50: 2.5383831166765134\n",
      "t85: 12.107637054450928\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 32 was 93.2%.\n",
      "current params: tensor([-1.0423,  0.3134,  1.1955], dtype=torch.float64)\n",
      "current ratio: -1.1469700061210535\n",
      "tensor(0.9329, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9328881265108707\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9328881265108707\n",
      "t50: 2.553512512564096\n",
      "t85: 11.5660225644548\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 33 was 93.9%.\n",
      "current params: tensor([-1.0624,  0.3104,  1.2185], dtype=torch.float64)\n",
      "current ratio: -1.1469485029720872\n",
      "tensor(0.9394, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9394149059919432\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9394149059919432\n",
      "t50: 2.5847253573403433\n",
      "t85: 11.419112500699558\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 34 was 94.4%.\n",
      "current params: tensor([-1.0808,  0.3035,  1.2405], dtype=torch.float64)\n",
      "current ratio: -1.1477504611610436\n",
      "tensor(0.9447, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9446710431969098\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9446710431969098\n",
      "t50: 2.6236657097349054\n",
      "t85: 11.344142808977008\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 35 was 94.8%.\n",
      "current params: tensor([-1.0974,  0.2921,  1.2615], dtype=torch.float64)\n",
      "current ratio: -1.1494876048827936\n",
      "tensor(0.9486, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9485513195910831\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9485513195910831\n",
      "t50: 2.66398412721966\n",
      "t85: 11.374267097573213\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 36 was 95.8%.\n",
      "current params: tensor([-1.1254,  0.3107,  1.2875], dtype=torch.float64)\n",
      "current ratio: -1.144057241269187\n",
      "tensor(0.9585, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9585112276813657\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9585112276813657\n",
      "t50: 2.683756013990057\n",
      "t85: 11.177555790694024\n",
      "t95: 51.728933558528865\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 37 was 96.1%.\n",
      "current params: tensor([-1.1414,  0.2987,  1.3079], dtype=torch.float64)\n",
      "current ratio: -1.1458027070959818\n",
      "tensor(0.9616, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9615639707419734\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9615639707419734\n",
      "t50: 2.726061179619756\n",
      "t85: 11.229513786375225\n",
      "t95: 48.18833999609215\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 38 was 95.9%.\n",
      "current params: tensor([-1.1475,  0.2634,  1.3235], dtype=torch.float64)\n",
      "current ratio: -1.1533856066259944\n",
      "tensor(0.9596, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9595762964875678\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9595762964875678\n",
      "t50: 2.7823056315234296\n",
      "t85: 11.50901628127738\n",
      "t95: 51.38529530387397\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 39 was 96.2%.\n",
      "current params: tensor([-1.1639,  0.2528,  1.3437], dtype=torch.float64)\n",
      "current ratio: -1.1544829376002244\n",
      "tensor(0.9626, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9626372644905393\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9626372644905393\n",
      "t50: 2.8250744375511396\n",
      "t85: 11.566592981982813\n",
      "t95: 48.264101485839305\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 40 was 96.5%.\n",
      "current params: tensor([-1.1794,  0.2401,  1.3633], dtype=torch.float64)\n",
      "current ratio: -1.1559395017815635\n",
      "tensor(0.9651, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9650950115519139\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9650950115519139\n",
      "t50: 2.870263131766767\n",
      "t85: 11.666471484975343\n",
      "t95: 46.09123922263205\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 41 was 96.7%.\n",
      "current params: tensor([-1.1947,  0.2270,  1.3826], dtype=torch.float64)\n",
      "current ratio: -1.1573389688889015\n",
      "tensor(0.9673, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9673114914150455\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9673114914150455\n",
      "t50: 2.918769634700397\n",
      "t85: 11.768259851438236\n",
      "t95: 44.45052676996149\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 42 was 96.8%.\n",
      "current params: tensor([-1.2099,  0.2136,  1.4018], dtype=torch.float64)\n",
      "current ratio: -1.158643331304297\n",
      "tensor(0.9689, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9688556402776105\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9688556402776105\n",
      "t50: 2.9640374917088224\n",
      "t85: 11.916842113240156\n",
      "t95: 43.16760437262375\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 43 was 96.7%.\n",
      "current params: tensor([-1.2159,  0.1801,  1.4167], dtype=torch.float64)\n",
      "current ratio: -1.1651433723844413\n",
      "tensor(0.9672, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9672339633498678\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9672339633498678\n",
      "t50: 3.0285563511638447\n",
      "t85: 12.19750618001111\n",
      "t95: 45.87587221768037\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 44 was 96.5%.\n",
      "current params: tensor([-1.2219,  0.1482,  1.4315], dtype=torch.float64)\n",
      "current ratio: -1.1714925852634008\n",
      "tensor(0.9655, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9654542315890123\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9654542315890123\n",
      "t50: 3.08985877181877\n",
      "t85: 12.485262357705881\n",
      "t95: 48.02652485186072\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 45 was 96.7%.\n",
      "current params: tensor([-1.2370,  0.1357,  1.4501], dtype=torch.float64)\n",
      "current ratio: -1.1723392287896974\n",
      "tensor(0.9671, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9670605228321267\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9670605228321267\n",
      "t50: 3.1389299960491113\n",
      "t85: 12.587384753452943\n",
      "t95: 47.33159468366616\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 46 was 96.5%.\n",
      "current params: tensor([-1.2430,  0.1052,  1.4647], dtype=torch.float64)\n",
      "current ratio: -1.1783552857223958\n",
      "tensor(0.9656, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9655537212073143\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9655537212073143\n",
      "t50: 3.200906449904515\n",
      "t85: 12.880777516351882\n",
      "t95: 49.5286882708927\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 47 was 96.3%.\n",
      "current params: tensor([-1.2491,  0.0759,  1.4792], dtype=torch.float64)\n",
      "current ratio: -1.1842318132770349\n",
      "tensor(0.9638, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9637955107183241\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9637955107183241\n",
      "t50: 3.266870674388762\n",
      "t85: 13.18121058440532\n",
      "t95: 51.98690355137677\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 48 was 96.5%.\n",
      "current params: tensor([-1.2640,  0.0640,  1.4975], dtype=torch.float64)\n",
      "current ratio: -1.1846652646446871\n",
      "tensor(0.9654, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9654071943181343\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9654071943181343\n",
      "t50: 3.3165560521947044\n",
      "t85: 13.330596644973943\n",
      "t95: 51.05689784056519\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 49 was 96.3%.\n",
      "current params: tensor([-1.2701,  0.0358,  1.5117], dtype=torch.float64)\n",
      "current ratio: -1.1902406641506738\n",
      "tensor(0.9639, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9639213320542371\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9639213320542371\n",
      "t50: 3.3834783185802757\n",
      "t85: 13.637764926863165\n",
      "t95: 53.55591735031689\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 50 was 96.5%.\n",
      "current params: tensor([-1.2846,  0.0233,  1.5296], dtype=torch.float64)\n",
      "current ratio: -1.1906982241876067\n",
      "tensor(0.9655, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.965517986502747\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.965517986502747\n",
      "t50: 3.436895682683773\n",
      "t85: 13.834415330969179\n",
      "t95: 52.629267253159576\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 51 was 96.6%.\n",
      "current params: tensor([-1.2991,  0.0110,  1.5473], dtype=torch.float64)\n",
      "current ratio: -1.1910430775682663\n",
      "tensor(0.9668, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9667830020049408\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9667830020049408\n",
      "t50: 3.4907470212534313\n",
      "t85: 13.983682578119117\n",
      "t95: 51.89804313003862\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 52 was 96.5%.\n",
      "current params: tensor([-1.3052, -0.0162,  1.5613], dtype=torch.float64)\n",
      "current ratio: -1.1961994850879543\n",
      "tensor(0.9653, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9653447769711536\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9653447769711536\n",
      "t50: 3.559329418905884\n",
      "t85: 14.298828701983785\n",
      "t95: 54.241769731872814\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 53 was 96.6%.\n",
      "current params: tensor([-1.3194, -0.0287,  1.5787], dtype=torch.float64)\n",
      "current ratio: -1.1964634617041876\n",
      "tensor(0.9664, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9663527618879044\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9663527618879044\n",
      "t50: 3.617577462800796\n",
      "t85: 14.498683858768759\n",
      "t95: 53.4942979071255\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 54 was 96.4%.\n",
      "current params: tensor([-1.3255, -0.0550,  1.5925], dtype=torch.float64)\n",
      "current ratio: -1.2013697907082255\n",
      "tensor(0.9650, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9649813084740739\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9649813084740739\n",
      "t50: 3.683222135080742\n",
      "t85: 14.822001629408978\n",
      "t95: 56.95559242135846\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 55 was 96.3%.\n",
      "current params: tensor([-1.3316, -0.0806,  1.6062], dtype=torch.float64)\n",
      "current ratio: -1.2061648884006329\n",
      "tensor(0.9635, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.963474086314989\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.963474086314989\n",
      "t50: 3.7535042334164936\n",
      "t85: 15.1524716383342\n",
      "t95: 59.72872292273433\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 56 was 96.4%.\n",
      "current params: tensor([-1.3459, -0.0925,  1.6233], dtype=torch.float64)\n",
      "current ratio: -1.2061195668803262\n",
      "tensor(0.9646, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9645753577562404\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9645753577562404\n",
      "t50: 3.8130119712628137\n",
      "t85: 15.355469088889679\n",
      "t95: 58.64515085915509\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 57 was 96.3%.\n",
      "current params: tensor([-1.3520, -0.1174,  1.6368], dtype=torch.float64)\n",
      "current ratio: -1.2106867430105304\n",
      "tensor(0.9632, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9632372627194904\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9632372627194904\n",
      "t50: 3.884557005041097\n",
      "t85: 15.694182697449445\n",
      "t95: 61.47041735974156\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 58 was 96.4%.\n",
      "current params: tensor([-1.3660, -0.1293,  1.6536], dtype=torch.float64)\n",
      "current ratio: -1.2105664841526904\n",
      "tensor(0.9641, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9641164839994149\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9641164839994149\n",
      "t50: 3.9491167109743275\n",
      "t85: 15.898196669054553\n",
      "t95: 61.522258777118566\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 59 was 96.2%.\n",
      "current params: tensor([-1.3721, -0.1535,  1.6670], dtype=torch.float64)\n",
      "current ratio: -1.2149202187053205\n",
      "tensor(0.9628, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9628190623683172\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9628190623683172\n",
      "t50: 4.022039162549341\n",
      "t85: 16.245240394149747\n",
      "t95: 64.53228002814657\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 60 was 96.1%.\n",
      "current params: tensor([-1.3783, -0.1771,  1.6804], dtype=torch.float64)\n",
      "current ratio: -1.2191755312545487\n",
      "tensor(0.9614, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9614220855664409\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9614220855664409\n",
      "t50: 4.095634410922209\n",
      "t85: 16.599696211917784\n",
      "t95: 67.90740457800416\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 61 was 96.4%.\n",
      "current params: tensor([-1.3998, -0.1765,  1.7001], dtype=torch.float64)\n",
      "current ratio: -1.2144985739497882\n",
      "tensor(0.9650, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9649885437414505\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9649885437414505\n",
      "t50: 4.148163670053783\n",
      "t85: 16.71357510896136\n",
      "t95: 63.39416976437814\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 62 was 96.3%.\n",
      "current params: tensor([-1.4059, -0.1997,  1.7133], dtype=torch.float64)\n",
      "current ratio: -1.2185803739435648\n",
      "tensor(0.9637, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9637192805692191\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9637192805692191\n",
      "t50: 4.222638477513592\n",
      "t85: 17.069563655264194\n",
      "t95: 66.31031250378497\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 63 was 96.1%.\n",
      "current params: tensor([-1.4121, -0.2225,  1.7264], dtype=torch.float64)\n",
      "current ratio: -1.2225719069140284\n",
      "tensor(0.9619, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9619108700354287\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9619108700354287\n",
      "t50: 4.297819212975381\n",
      "t85: 17.432988715756398\n",
      "t95: 69.54748101883725\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 64 was 96.0%.\n",
      "current params: tensor([-1.4182, -0.2447,  1.7394], dtype=torch.float64)\n",
      "current ratio: -1.2264709690779427\n",
      "tensor(0.9607, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9606847561788494\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9606847561788494\n",
      "t50: 4.378571668936452\n",
      "t85: 17.803855936703776\n",
      "t95: 73.17290725532595\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 65 was 95.9%.\n",
      "current params: tensor([-1.4244, -0.2665,  1.7524], dtype=torch.float64)\n",
      "current ratio: -1.2302827236205833\n",
      "tensor(0.9595, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9594749698215385\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9594749698215385\n",
      "t50: 4.455282674647948\n",
      "t85: 18.1824169031433\n",
      "t95: 79.18244039108036\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 66 was 95.8%.\n",
      "current params: tensor([-1.4306, -0.2879,  1.7653], dtype=torch.float64)\n",
      "current ratio: -1.2340085706309027\n",
      "tensor(0.9581, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9580928739586241\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9580928739586241\n",
      "t50: 4.5327609005140825\n",
      "t85: 18.568812347659534\n",
      "t95: 84.23605111096856\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 67 was 96.1%.\n",
      "current params: tensor([-1.4518, -0.2879,  1.7844], dtype=torch.float64)\n",
      "current ratio: -1.2291097958393533\n",
      "tensor(0.9615, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.961527432994573\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.961527432994573\n",
      "t50: 4.592136230724699\n",
      "t85: 18.665866909820938\n",
      "t95: 76.32667284027647\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 68 was 96.0%.\n",
      "current params: tensor([-1.4579, -0.3091,  1.7972], dtype=torch.float64)\n",
      "current ratio: -1.2326990520019325\n",
      "tensor(0.9603, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9603334526669022\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9603334526669022\n",
      "t50: 4.675880831611835\n",
      "t85: 19.053729050811654\n",
      "t95: 80.41608898448204\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 69 was 95.8%.\n",
      "current params: tensor([-1.4641, -0.3298,  1.8099], dtype=torch.float64)\n",
      "current ratio: -1.2362091300611464\n",
      "tensor(0.9586, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9586068579536441\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9586068579536441\n",
      "t50: 4.755310973815099\n",
      "t85: 19.449471151265758\n",
      "t95: 85.07956209466836\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 70 was 95.7%.\n",
      "current params: tensor([-1.4703, -0.3502,  1.8226], dtype=torch.float64)\n",
      "current ratio: -1.239637857800606\n",
      "tensor(0.9574, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9574453990981917\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9574453990981917\n",
      "t50: 4.835518595222989\n",
      "t85: 19.853106072273015\n",
      "t95: 90.4867890089468\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 71 was 95.6%.\n",
      "current params: tensor([-1.4765, -0.3702,  1.8352], dtype=torch.float64)\n",
      "current ratio: -1.2429899778416889\n",
      "tensor(0.9563, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9562963767722414\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9562963767722414\n",
      "t50: 4.922035226020467\n",
      "t85: 20.26491788198112\n",
      "t95: 96.90463560983576\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 72 was 95.5%.\n",
      "current params: tensor([-1.4827, -0.3900,  1.8478], dtype=torch.float64)\n",
      "current ratio: -1.246266699872557\n",
      "tensor(0.9550, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9550497508441932\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9550497508441932\n",
      "t50: 5.004007966388053\n",
      "t85: 20.685065816991404\n",
      "t95: 104.77335864279937\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 73 was 95.5%.\n",
      "current params: tensor([-1.4963, -0.4002,  1.8633], dtype=torch.float64)\n",
      "current ratio: -1.2452848107226147\n",
      "tensor(0.9559, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9558710399822532\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9558710399822532\n",
      "t50: 5.082284577810502\n",
      "t85: 20.972743759457178\n",
      "t95: 101.93440704251648\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 74 was 95.4%.\n",
      "current params: tensor([-1.5025, -0.4196,  1.8758], dtype=torch.float64)\n",
      "current ratio: -1.2484228858564062\n",
      "tensor(0.9547, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9547414113809449\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9547414113809449\n",
      "t50: 5.165992769296827\n",
      "t85: 21.403794357828684\n",
      "t95: 110.51779042922054\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 75 was 95.3%.\n",
      "current params: tensor([-1.5088, -0.4388,  1.8883], dtype=torch.float64)\n",
      "current ratio: -1.2514901234461715\n",
      "tensor(0.9536, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.953588692658648\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.953588692658648\n",
      "t50: 5.256440604596823\n",
      "t85: 21.84351468410466\n",
      "t95: 121.75182683966362\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 76 was 95.4%.\n",
      "current params: tensor([-1.5224, -0.4488,  1.9036], dtype=torch.float64)\n",
      "current ratio: -1.2503821025369666\n",
      "tensor(0.9543, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9543062827177304\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9543062827177304\n",
      "t50: 5.33704052170531\n",
      "t85: 22.13788459762785\n",
      "t95: 116.61650372055135\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 77 was 95.3%.\n",
      "current params: tensor([-1.5286, -0.4677,  1.9159], dtype=torch.float64)\n",
      "current ratio: -1.2533218011690084\n",
      "tensor(0.9532, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9531938529011234\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9531938529011234\n",
      "t50: 5.423517844357737\n",
      "t85: 22.58886246227388\n",
      "t95: 129.1285195367149\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 78 was 95.2%.\n",
      "current params: tensor([-1.5349, -0.4863,  1.9282], dtype=torch.float64)\n",
      "current ratio: -1.2561948735365551\n",
      "tensor(0.9521, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9520912691418654\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9520912691418654\n",
      "t50: 5.517063960387791\n",
      "t85: 23.048860236909956\n",
      "t95: 147.82972433880292\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 79 was 95.0%.\n",
      "current params: tensor([-1.5413, -0.5047,  1.9404], dtype=torch.float64)\n",
      "current ratio: -1.25900236449316\n",
      "tensor(0.9509, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9509323756695133\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9509323756695133\n",
      "t50: 5.605493486862018\n",
      "t85: 23.518061690216154\n",
      "t95: 185.05652368557426\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 80 was 95.1%.\n",
      "current params: tensor([-1.5548, -0.5144,  1.9555], dtype=torch.float64)\n",
      "current ratio: -1.257684625582349\n",
      "tensor(0.9517, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9516721573834747\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9516721573834747\n",
      "t50: 5.695829802195078\n",
      "t85: 23.821988931304627\n",
      "t95: 147.52908046331018\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 81 was 95.0%.\n",
      "current params: tensor([-1.5612, -0.5326,  1.9677], dtype=torch.float64)\n",
      "current ratio: -1.2603778935778929\n",
      "tensor(0.9506, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9505841658080121\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9505841658080121\n",
      "t50: 5.786233443996012\n",
      "t85: 24.30297165329851\n",
      "t95: 178.86611463513464\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 82 was 94.9%.\n",
      "current params: tensor([-1.5675, -0.5505,  1.9798], dtype=torch.float64)\n",
      "current ratio: -1.2630094141025394\n",
      "tensor(0.9495, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9495049438606437\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9495049438606437\n",
      "t50: 5.884176420168716\n",
      "t85: 24.79353338284214\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 83 was 94.8%.\n",
      "current params: tensor([-1.5739, -0.5682,  1.9919], dtype=torch.float64)\n",
      "current ratio: -1.2655801532453996\n",
      "tensor(0.9484, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9483946027879795\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9483946027879795\n",
      "t50: 5.983329085623625\n",
      "t85: 25.293873131100305\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 84 was 94.9%.\n",
      "current params: tensor([-1.5875, -0.5777,  2.0067], dtype=torch.float64)\n",
      "current ratio: -1.2640877433272748\n",
      "tensor(0.9491, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9490960892817425\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9490960892817425\n",
      "t50: 6.0709903565869645\n",
      "t85: 25.6992736715702\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 85 was 94.8%.\n",
      "current params: tensor([-1.5939, -0.5952,  2.0187], dtype=torch.float64)\n",
      "current ratio: -1.2665561752721768\n",
      "tensor(0.9480, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.948029349170003\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.948029349170003\n",
      "t50: 6.172413318647398\n",
      "t85: 26.21429465873319\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 86 was 94.6%.\n",
      "current params: tensor([-1.6003, -0.6125,  2.0307], dtype=torch.float64)\n",
      "current ratio: -1.2689673162164563\n",
      "tensor(0.9470, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9469705712230702\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9469705712230702\n",
      "t50: 6.268074924631558\n",
      "t85: 26.739564383870462\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 87 was 94.5%.\n",
      "current params: tensor([-1.6067, -0.6296,  2.0426], dtype=torch.float64)\n",
      "current ratio: -1.2713220603618178\n",
      "tensor(0.9459, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9459194345980622\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9459194345980622\n",
      "t50: 6.371905608427556\n",
      "t85: 27.275299755097638\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 88 was 94.4%.\n",
      "current params: tensor([-1.6131, -0.6466,  2.0545], dtype=torch.float64)\n",
      "current ratio: -1.273621290136235\n",
      "tensor(0.9448, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9448254164577176\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9448254164577176\n",
      "t50: 6.47702940253142\n",
      "t85: 27.821724025433365\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 89 was 94.5%.\n",
      "current params: tensor([-1.6268, -0.6557,  2.0691], dtype=torch.float64)\n",
      "current ratio: -1.27190702062729\n",
      "tensor(0.9455, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9455360924120106\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9455360924120106\n",
      "t50: 6.56956454699353\n",
      "t85: 28.147416586085917\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 90 was 94.4%.\n",
      "current params: tensor([-1.6332, -0.6725,  2.0809], dtype=torch.float64)\n",
      "current ratio: -1.2741172604917865\n",
      "tensor(0.9445, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9444952267874257\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9444952267874257\n",
      "t50: 6.677087521422807\n",
      "t85: 28.707010050239127\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 91 was 94.3%.\n",
      "current params: tensor([-1.6397, -0.6891,  2.0927], dtype=torch.float64)\n",
      "current ratio: -1.276275112881381\n",
      "tensor(0.9435, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9434614449571987\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9434614449571987\n",
      "t50: 6.785951068689403\n",
      "t85: 29.27773212064708\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 92 was 94.2%.\n",
      "current params: tensor([-1.6462, -0.7055,  2.1045], dtype=torch.float64)\n",
      "current ratio: -1.2783813953654999\n",
      "tensor(0.9424, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9424345299713011\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9424345299713011\n",
      "t50: 6.896175129491662\n",
      "t85: 29.85982295237871\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 93 was 94.1%.\n",
      "current params: tensor([-1.6527, -0.7218,  2.1162], dtype=torch.float64)\n",
      "current ratio: -1.2804369150053514\n",
      "tensor(0.9414, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9414142703292363\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9414142703292363\n",
      "t50: 6.999910060723548\n",
      "t85: 30.453529589316346\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 94 was 94.0%.\n",
      "current params: tensor([-1.6593, -0.7379,  2.1279], dtype=torch.float64)\n",
      "current ratio: -1.2824424683301552\n",
      "tensor(0.9404, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9403683865276196\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9403683865276196\n",
      "t50: 7.1127804086125925\n",
      "t85: 31.05910615501144\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 95 was 94.1%.\n",
      "current params: tensor([-1.6730, -0.7467,  2.1422], dtype=torch.float64)\n",
      "current ratio: -1.2804881207744576\n",
      "tensor(0.9411, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9410640978370844\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9410640978370844\n",
      "t50: 7.219719141636973\n",
      "t85: 31.39840716665519\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 96 was 94.0%.\n",
      "current params: tensor([-1.6795, -0.7627,  2.1539], dtype=torch.float64)\n",
      "current ratio: -1.282418725393988\n",
      "tensor(0.9401, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9400519016636537\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9400519016636537\n",
      "t50: 7.3352922666860065\n",
      "t85: 32.018183635703366\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 97 was 93.9%.\n",
      "current params: tensor([-1.6861, -0.7785,  2.1655], dtype=torch.float64)\n",
      "current ratio: -1.2843021147481948\n",
      "tensor(0.9390, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9390459562438955\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9390459562438955\n",
      "t50: 7.452315877771708\n",
      "t85: 32.65031790936654\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 98 was 93.8%.\n",
      "current params: tensor([-1.6927, -0.7942,  2.1770], dtype=torch.float64)\n",
      "current ratio: -1.2861390271249025\n",
      "tensor(0.9380, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9380461136779985\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9380461136779985\n",
      "t50: 7.570810787418772\n",
      "t85: 33.29508277278995\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 99 was 93.7%.\n",
      "current params: tensor([-1.6993, -0.8098,  2.1886], dtype=torch.float64)\n",
      "current ratio: -1.2879301913920809\n",
      "tensor(0.9371, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9370522345372345\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9370522345372345\n",
      "t50: 7.682130554872603\n",
      "t85: 33.952758801524894\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 100 was 93.6%.\n",
      "current params: tensor([-1.7059, -0.8253,  2.2001], dtype=torch.float64)\n",
      "current ratio: -1.2896763270723703\n",
      "tensor(0.9361, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9360641877342912\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9360641877342912\n",
      "t50: 7.803485337562284\n",
      "t85: 34.62363460574254\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 101 was 93.5%.\n",
      "current params: tensor([-1.7126, -0.8406,  2.2116], dtype=torch.float64)\n",
      "current ratio: -1.2913781444262682\n",
      "tensor(0.9351, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9350818503767384\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9350818503767384\n",
      "t50: 7.926372893723761\n",
      "t85: 35.30800708787188\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 102 was 93.4%.\n",
      "current params: tensor([-1.7192, -0.8558,  2.2230], dtype=torch.float64)\n",
      "current ratio: -1.2930363445430562\n",
      "tensor(0.9341, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9341051075466646\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9341051075466646\n",
      "t50: 8.050814752944222\n",
      "t85: 36.00618171416858\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 103 was 93.3%.\n",
      "current params: tensor([-1.7259, -0.8709,  2.2345], dtype=torch.float64)\n",
      "current ratio: -1.294651619437889\n",
      "tensor(0.9331, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9331338519307948\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9331338519307948\n",
      "t50: 8.176832651100888\n",
      "t85: 36.718472800768815\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 104 was 93.2%.\n",
      "current params: tensor([-1.7326, -0.8858,  2.2459], dtype=torch.float64)\n",
      "current ratio: -1.2962246521524892\n",
      "tensor(0.9322, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9321679832158515\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9321679832158515\n",
      "t50: 8.295045926684661\n",
      "t85: 37.44520381485072\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 105 was 93.1%.\n",
      "current params: tensor([-1.7394, -0.9007,  2.2573], dtype=torch.float64)\n",
      "current ratio: -1.2977561168557274\n",
      "tensor(0.9312, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9311821841460832\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9311821841460832\n",
      "t50: 8.424125896842247\n",
      "t85: 38.1867076915507\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 106 was 93.1%.\n",
      "current params: tensor([-1.7533, -0.9088,  2.2712], dtype=torch.float64)\n",
      "current ratio: -1.2953733014045103\n",
      "tensor(0.9319, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9319017202838974\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9319017202838974\n",
      "t50: 8.545895586597954\n",
      "t85: 38.545420631759995\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 107 was 93.0%.\n",
      "current params: tensor([-1.7601, -0.9236,  2.2825], dtype=torch.float64)\n",
      "current ratio: -1.2968517945515965\n",
      "tensor(0.9309, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9309405431941267\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9309405431941267\n",
      "t50: 8.678046310911334\n",
      "t85: 39.303345250388894\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 108 was 92.9%.\n",
      "current params: tensor([-1.7668, -0.9383,  2.2938], dtype=torch.float64)\n",
      "current ratio: -1.2982909219958987\n",
      "tensor(0.9300, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9299844576304634\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9299844576304634\n",
      "t50: 8.811872387191862\n",
      "t85: 40.07664475785374\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 109 was 92.9%.\n",
      "current params: tensor([-1.7736, -0.9528,  2.3051], dtype=torch.float64)\n",
      "current ratio: -1.29969130790803\n",
      "tensor(0.9290, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9290333718119258\n",
      "Penalty:  tensor(0.2530, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.6759920982208955\n",
      "t50: 8.947396836521357\n",
      "t85: 40.86567612831791\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 110 was 92.7%.\n",
      "current params: tensor([-1.7804, -0.9673,  2.2053], dtype=torch.float64)\n",
      "current ratio: -1.2386429273993078\n",
      "tensor(0.9271, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9270916934593864\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9270916934593864\n",
      "t50: 8.721268700685059\n",
      "t85: 40.011748851502865\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 111 was 92.6%.\n",
      "current params: tensor([-1.7872, -0.9816,  2.2058], dtype=torch.float64)\n",
      "current ratio: -1.234281126909918\n",
      "tensor(0.9261, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9260608435283889\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9260608435283889\n",
      "t50: 8.815263630455183\n",
      "t85: 40.618837226367994\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 112 was 92.5%.\n",
      "current params: tensor([-1.7939, -0.9959,  2.2064], dtype=torch.float64)\n",
      "current ratio: -1.2299418785351692\n",
      "tensor(0.9250, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9250363478842439\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9250363478842439\n",
      "t50: 8.920348743154534\n",
      "t85: 41.4083032443903\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 113 was 92.4%.\n",
      "current params: tensor([-1.8008, -1.0101,  2.2070], dtype=torch.float64)\n",
      "current ratio: -1.2256250437607155\n",
      "tensor(0.9240, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9240181401669467\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9240181401669467\n",
      "t50: 9.016311543386578\n",
      "t85: 42.0423159455085\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 114 was 92.3%.\n",
      "current params: tensor([-1.8076, -1.0242,  2.2077], dtype=torch.float64)\n",
      "current ratio: -1.2213304871910735\n",
      "tensor(0.9230, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.923006154810008\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.923006154810008\n",
      "t50: 9.113219288398232\n",
      "t85: 42.68833823915912\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 115 was 92.2%.\n",
      "current params: tensor([-1.8144, -1.0382,  2.2083], dtype=torch.float64)\n",
      "current ratio: -1.2170580763917944\n",
      "tensor(0.9220, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9220003259888826\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9220003259888826\n",
      "t50: 9.22163505004162\n",
      "t85: 43.3466545039328\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 116 was 92.0%.\n",
      "current params: tensor([-1.8213, -1.0521,  2.2089], dtype=torch.float64)\n",
      "current ratio: -1.212807681727861\n",
      "tensor(0.9205, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9204885754123485\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9204885754123485\n",
      "t50: 9.320594849210918\n",
      "t85: 44.017558588467516\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 117 was 91.9%.\n",
      "current params: tensor([-1.8282, -1.0659,  2.2095], dtype=torch.float64)\n",
      "current ratio: -1.2085791229763139\n",
      "tensor(0.9195, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9194959428960768\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9194959428960768\n",
      "t50: 9.420504058812465\n",
      "t85: 44.70108245036303\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 118 was 91.8%.\n",
      "current params: tensor([-1.8351, -1.0797,  2.2101], dtype=torch.float64)\n",
      "current ratio: -1.2043723255615522\n",
      "tensor(0.9185, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9185091810130865\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9185091810130865\n",
      "t50: 9.521411729560278\n",
      "t85: 45.39780195345777\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 119 was 91.7%.\n",
      "current params: tensor([-1.8420, -1.0933,  2.2107], dtype=torch.float64)\n",
      "current ratio: -1.2001871666922068\n",
      "tensor(0.9175, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9175282179243756\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9175282179243756\n",
      "t50: 9.634400606762398\n",
      "t85: 46.108041373173116\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 120 was 91.6%.\n",
      "current params: tensor([-1.8489, -1.1069,  2.2113], dtype=torch.float64)\n",
      "current ratio: -1.1960235257363403\n",
      "tensor(0.9166, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9165529823993116\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9165529823993116\n",
      "t50: 9.737474324786824\n",
      "t85: 46.83213613174706\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 121 was 91.5%.\n",
      "current params: tensor([-1.8558, -1.1204,  2.2120], dtype=torch.float64)\n",
      "current ratio: -1.1918812841096622\n",
      "tensor(0.9156, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9155834038854962\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9155834038854962\n",
      "t50: 9.841588143820232\n",
      "t85: 47.57043328882364\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 122 was 91.4%.\n",
      "current params: tensor([-1.8628, -1.1339,  2.2126], dtype=torch.float64)\n",
      "current ratio: -1.1877603251708024\n",
      "tensor(0.9146, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9146194126021479\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9146194126021479\n",
      "t50: 9.946755688989809\n",
      "t85: 48.32329206194771\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 123 was 91.4%.\n",
      "current params: tensor([-1.8698, -1.1472,  2.2132], dtype=torch.float64)\n",
      "current ratio: -1.1836605341235726\n",
      "tensor(0.9142, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9141720829285834\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9141720829285834\n",
      "t50: 10.052990698664779\n",
      "t85: 49.09108437905798\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 124 was 91.3%.\n",
      "current params: tensor([-1.8768, -1.1605,  2.2138], dtype=torch.float64)\n",
      "current ratio: -1.1795818636901199\n",
      "tensor(0.9132, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9132182789777822\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9132182789777822\n",
      "t50: 10.160345513112485\n",
      "t85: 49.8745226452208\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 125 was 91.2%.\n",
      "current params: tensor([-1.8838, -1.1738,  2.2145], dtype=torch.float64)\n",
      "current ratio: -1.1755241429783005\n",
      "tensor(0.9123, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9122698805945528\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9122698805945528\n",
      "t50: 10.280681817498623\n",
      "t85: 50.67369161374381\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 126 was 91.1%.\n",
      "current params: tensor([-1.8908, -1.1869,  2.2151], dtype=torch.float64)\n",
      "current ratio: -1.1714872621131645\n",
      "tensor(0.9113, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9113268147799289\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9113268147799289\n",
      "t50: 10.390380132143138\n",
      "t85: 51.48900559085768\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 127 was 91.0%.\n",
      "current params: tensor([-1.8979, -1.2001,  2.2157], dtype=torch.float64)\n",
      "current ratio: -1.1674711126587096\n",
      "tensor(0.9104, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9103890111808814\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9103890111808814\n",
      "t50: 10.5012038151944\n",
      "t85: 52.320894201899236\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 128 was 90.9%.\n",
      "current params: tensor([-1.9050, -1.2131,  2.2164], dtype=torch.float64)\n",
      "current ratio: -1.1634755875674936\n",
      "tensor(0.9095, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9094564021021315\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9094564021021315\n",
      "t50: 10.613167253236314\n",
      "t85: 53.16980314492186\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 129 was 90.8%.\n",
      "current params: tensor([-1.9121, -1.2261,  2.2170], dtype=torch.float64)\n",
      "current ratio: -1.1595005811345374\n",
      "tensor(0.9085, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9085289223984317\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9085289223984317\n",
      "t50: 10.726284966502503\n",
      "t85: 53.77779697336607\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 130 was 90.7%.\n",
      "current params: tensor([-1.9192, -1.2390,  2.2177], dtype=torch.float64)\n",
      "current ratio: -1.155545988953765\n",
      "tensor(0.9076, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9076065092983181\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9076065092983181\n",
      "t50: 10.84057161204227\n",
      "t85: 54.65533405810024\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 131 was 90.6%.\n",
      "current params: tensor([-1.9263, -1.2519,  2.2183], dtype=torch.float64)\n",
      "current ratio: -1.151611707875878\n",
      "tensor(0.9067, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.906689102198386\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.906689102198386\n",
      "t50: 10.956041986837462\n",
      "t85: 55.55109364305952\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 132 was 90.5%.\n",
      "current params: tensor([-1.9334, -1.2647,  2.2190], dtype=torch.float64)\n",
      "current ratio: -1.1476976359670648\n",
      "tensor(0.9058, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9057766424547967\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9057766424547967\n",
      "t50: 11.072711030877677\n",
      "t85: 56.46558344415059\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 133 was 90.4%.\n",
      "current params: tensor([-1.9406, -1.2774,  2.2196], dtype=torch.float64)\n",
      "current ratio: -1.1438036724682992\n",
      "tensor(0.9049, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9048690731876594\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9048690731876594\n",
      "t50: 11.190593830198239\n",
      "t85: 57.399331208006174\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 134 was 90.3%.\n",
      "current params: tensor([-1.9477, -1.2901,  2.2203], dtype=torch.float64)\n",
      "current ratio: -1.1399297177552015\n",
      "tensor(0.9040, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9039663391063902\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9039663391063902\n",
      "t50: 11.3097056198871\n",
      "t85: 58.35288578333513\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 135 was 90.3%.\n",
      "current params: tensor([-1.9549, -1.3027,  2.2209], dtype=torch.float64)\n",
      "current ratio: -1.1360756732985517\n",
      "tensor(0.9031, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9030683863584169\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9030683863584169\n",
      "t50: 11.430061787063737\n",
      "t85: 59.32681826772323\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 136 was 90.2%.\n",
      "current params: tensor([-1.9621, -1.3153,  2.2216], dtype=torch.float64)\n",
      "current ratio: -1.13224144162559\n",
      "tensor(0.9022, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9021751624008856\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9021751624008856\n",
      "t50: 11.551677873834231\n",
      "t85: 60.01384572201507\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 137 was 90.1%.\n",
      "current params: tensor([-1.9694, -1.3278,  2.2223], dtype=torch.float64)\n",
      "current ratio: -1.1284269262822493\n",
      "tensor(0.9018, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9017965159633853\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9017965159633853\n",
      "t50: 11.674569580227914\n",
      "t85: 61.021751112073574\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 138 was 90.0%.\n",
      "current params: tensor([-1.9766, -1.3403,  2.2229], dtype=torch.float64)\n",
      "current ratio: -1.1246321220597035\n",
      "tensor(0.9009, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9009118712650407\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9009118712650407\n",
      "t50: 11.798797083068523\n",
      "t85: 62.052046064454785\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 139 was 90.0%.\n",
      "current params: tensor([-1.9839, -1.3527,  2.2236], dtype=torch.float64)\n",
      "current ratio: -1.1208568498981368\n",
      "tensor(0.9000, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.9000317751412493\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.9000317751412493\n",
      "t50: 11.924332351841857\n",
      "t85: 63.104934812986684\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 140 was 89.9%.\n",
      "current params: tensor([-1.9911, -1.3651,  2.2243], dtype=torch.float64)\n",
      "current ratio: -1.1171010155132395\n",
      "tensor(0.8992, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8991561857877488\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8991561857877488\n",
      "t50: 12.051191588889044\n",
      "t85: 64.1811042075434\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 141 was 89.8%.\n",
      "current params: tensor([-1.9984, -1.3775,  2.2250], dtype=torch.float64)\n",
      "current ratio: -1.1133645255911122\n",
      "tensor(0.8983, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8982850610417954\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8982850610417954\n",
      "t50: 12.179391165950886\n",
      "t85: 64.93148666713722\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 142 was 89.7%.\n",
      "current params: tensor([-2.0057, -1.3897,  2.2256], dtype=torch.float64)\n",
      "current ratio: -1.1096472877400292\n",
      "tensor(0.8974, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8974183586229924\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8974183586229924\n",
      "t50: 12.308947627065544\n",
      "t85: 66.04622060269581\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 143 was 89.6%.\n",
      "current params: tensor([-2.0131, -1.4020,  2.2263], dtype=torch.float64)\n",
      "current ratio: -1.1059492104470678\n",
      "tensor(0.8966, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8965560363095777\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8965560363095777\n",
      "t50: 12.439877691458772\n",
      "t85: 67.18608202582169\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 144 was 89.6%.\n",
      "current params: tensor([-2.0204, -1.4142,  2.2270], dtype=torch.float64)\n",
      "current ratio: -1.1022702030387224\n",
      "tensor(0.8962, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8962072399759802\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8962072399759802\n",
      "t50: 12.572198256433468\n",
      "t85: 68.35186860069442\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 145 was 89.5%.\n",
      "current params: tensor([-2.0278, -1.4263,  2.2277], dtype=torch.float64)\n",
      "current ratio: -1.0986102742327322\n",
      "tensor(0.8954, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8953527811103638\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8953527811103638\n",
      "t50: 12.70597404059881\n",
      "t85: 69.15719854396596\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 146 was 89.4%.\n",
      "current params: tensor([-2.0351, -1.4384,  2.2284], dtype=torch.float64)\n",
      "current ratio: -1.094969241046877\n",
      "tensor(0.8945, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8945025488435383\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8945025488435383\n",
      "t50: 12.826020644567123\n",
      "t85: 70.36630118898097\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 147 was 89.3%.\n",
      "current params: tensor([-2.0425, -1.4505,  2.2291], dtype=torch.float64)\n",
      "current ratio: -1.091347014495518\n",
      "tensor(0.8937, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8936565095491474\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8936565095491474\n",
      "t50: 12.962487961958002\n",
      "t85: 71.60350102877675\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 148 was 89.2%.\n",
      "current params: tensor([-2.0499, -1.4625,  2.2298], dtype=torch.float64)\n",
      "current ratio: -1.087743506405716\n",
      "tensor(0.8928, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8928146282471177\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8928146282471177\n",
      "t50: 13.100413502685393\n",
      "t85: 72.45126981818377\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 149 was 89.2%.\n",
      "current params: tensor([-2.0574, -1.4745,  2.2305], dtype=torch.float64)\n",
      "current ratio: -1.0841586293674494\n",
      "tensor(0.8925, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8924854568809839\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8924854568809839\n",
      "t50: 13.239815083515149\n",
      "t85: 73.7345899398541\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 150 was 89.1%.\n",
      "current params: tensor([-2.0648, -1.4865,  2.2312], dtype=torch.float64)\n",
      "current ratio: -1.0805923992132727\n",
      "tensor(0.8917, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8916509955956452\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8916509955956452\n",
      "t50: 13.380760855206816\n",
      "t85: 75.04906156216852\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 151 was 89.0%.\n",
      "current params: tensor([-2.0723, -1.4984,  2.2319], dtype=torch.float64)\n",
      "current ratio: -1.0770446316378774\n",
      "tensor(0.8908, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8908205552724557\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8908205552724557\n",
      "t50: 13.523219251146635\n",
      "t85: 75.9427600609301\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 152 was 88.9%.\n",
      "current params: tensor([-2.0797, -1.5103,  2.2326], dtype=torch.float64)\n",
      "current ratio: -1.0735152406846817\n",
      "tensor(0.8900, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8899941073458859\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8899941073458859\n",
      "t50: 13.667208693851125\n",
      "t85: 77.30714189381041\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 153 was 88.9%.\n",
      "current params: tensor([-2.0872, -1.5221,  2.2333], dtype=torch.float64)\n",
      "current ratio: -1.0700041411308159\n",
      "tensor(0.8892, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8891716212603883\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8891716212603883\n",
      "t50: 13.796343329936672\n",
      "t85: 78.70473975422559\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 154 was 88.8%.\n",
      "current params: tensor([-2.0947, -1.5339,  2.2341], dtype=torch.float64)\n",
      "current ratio: -1.0665112484352883\n",
      "tensor(0.8889, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8888611432858924\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8888611432858924\n",
      "t50: 13.943260113675679\n",
      "t85: 79.64685123878222\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 155 was 88.8%.\n",
      "current params: tensor([-2.1023, -1.5457,  2.2348], dtype=torch.float64)\n",
      "current ratio: -1.063036585455671\n",
      "tensor(0.8880, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8880456532872908\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8880456532872908\n",
      "t50: 14.091815016695135\n",
      "t85: 81.09928606902493\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 156 was 88.7%.\n",
      "current params: tensor([-2.1098, -1.5574,  2.2355], dtype=torch.float64)\n",
      "current ratio: -1.0595799655888214\n",
      "tensor(0.8872, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8872340061136798\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8872340061136798\n",
      "t50: 14.241974875444312\n",
      "t85: 82.58800709519252\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 157 was 88.6%.\n",
      "current params: tensor([-2.1174, -1.5691,  2.2362], dtype=torch.float64)\n",
      "current ratio: -1.056141305615157\n",
      "tensor(0.8864, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8864261748691533\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8864261748691533\n",
      "t50: 14.393759145339713\n",
      "t85: 83.58273733835807\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 158 was 88.5%.\n",
      "current params: tensor([-2.1249, -1.5807,  2.2370], dtype=torch.float64)\n",
      "current ratio: -1.0527205229530512\n",
      "tensor(0.8856, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8856221309229881\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8856221309229881\n",
      "t50: 14.529835548414246\n",
      "t85: 85.13030082413904\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 159 was 88.5%.\n",
      "current params: tensor([-2.1325, -1.5924,  2.2377], dtype=torch.float64)\n",
      "current ratio: -1.049317535615021\n",
      "tensor(0.8853, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8853294076993934\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8853294076993934\n",
      "t50: 14.68472593081091\n",
      "t85: 86.71758634585157\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 160 was 88.4%.\n",
      "current params: tensor([-2.1401, -1.6040,  2.2384], dtype=torch.float64)\n",
      "current ratio: -1.0459323724076237\n",
      "tensor(0.8845, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8845319694858085\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8845319694858085\n",
      "t50: 14.841353680459337\n",
      "t85: 87.7688447244741\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 161 was 88.3%.\n",
      "current params: tensor([-2.1477, -1.6155,  2.2392], dtype=torch.float64)\n",
      "current ratio: -1.0425648448692393\n",
      "tensor(0.8837, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8837382144910015\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8837382144910015\n",
      "t50: 14.999683937350314\n",
      "t85: 89.42117679276502\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 162 was 88.2%.\n",
      "current params: tensor([-2.1554, -1.6270,  2.2399], dtype=torch.float64)\n",
      "current ratio: -1.0392148722282113\n",
      "tensor(0.8829, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8829481160042835\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8829481160042835\n",
      "t50: 15.141594035367076\n",
      "t85: 90.50839848455492\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 163 was 88.2%.\n",
      "current params: tensor([-2.1630, -1.6385,  2.2407], dtype=torch.float64)\n",
      "current ratio: -1.0358823742574579\n",
      "tensor(0.8827, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8826687483956641\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8826687483956641\n",
      "t50: 15.303179951936128\n",
      "t85: 92.22839574943364\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 164 was 88.1%.\n",
      "current params: tensor([-2.1707, -1.6500,  2.2414], dtype=torch.float64)\n",
      "current ratio: -1.0325673833366313\n",
      "tensor(0.8819, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8818849785070235\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8818849785070235\n",
      "t50: 15.466586298640237\n",
      "t85: 93.99562018722474\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 165 was 88.1%.\n",
      "current params: tensor([-2.1784, -1.6615,  2.2422], dtype=torch.float64)\n",
      "current ratio: -1.0292697104221251\n",
      "tensor(0.8811, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8811047698389423\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8811047698389423\n",
      "t50: 15.61302435692637\n",
      "t85: 95.14646878308034\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 166 was 88.0%.\n",
      "current params: tensor([-2.1861, -1.6729,  2.2429], dtype=torch.float64)\n",
      "current ratio: -1.0259892764206011\n",
      "tensor(0.8803, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8803280967019237\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8803280967019237\n",
      "t50: 15.779802020737327\n",
      "t85: 96.98794449702032\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 167 was 88.0%.\n",
      "current params: tensor([-2.1938, -1.6842,  2.2437], dtype=torch.float64)\n",
      "current ratio: -1.0227260027330163\n",
      "tensor(0.8801, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8800615689420765\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8800615689420765\n",
      "t50: 15.94840450007037\n",
      "t85: 98.17921583272087\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 168 was 87.9%.\n",
      "current params: tensor([-2.2015, -1.6956,  2.2444], dtype=torch.float64)\n",
      "current ratio: -1.0194799247818136\n",
      "tensor(0.8793, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8792909638511471\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8792909638511471\n",
      "t50: 16.099532040919385\n",
      "t85: 100.10027398509085\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 169 was 87.8%.\n",
      "current params: tensor([-2.2093, -1.7069,  2.2452], dtype=torch.float64)\n",
      "current ratio: -1.016250853221739\n",
      "tensor(0.8785, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8785238063894769\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8785238063894769\n",
      "t50: 16.271685406466784\n",
      "t85: 101.3349130041558\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 170 was 87.7%.\n",
      "current params: tensor([-2.2171, -1.7182,  2.2460], dtype=torch.float64)\n",
      "current ratio: -1.013038710515284\n",
      "tensor(0.8778, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8777600714065563\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8777600714065563\n",
      "t50: 16.445728111833755\n",
      "t85: 103.33916592240882\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 171 was 87.7%.\n",
      "current params: tensor([-2.2248, -1.7295,  2.2467], dtype=torch.float64)\n",
      "current ratio: -1.0098434195719495\n",
      "tensor(0.8775, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8775058866920609\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8775058866920609\n",
      "t50: 16.601649499539416\n",
      "t85: 104.6180781554168\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 172 was 87.6%.\n",
      "current params: tensor([-2.2326, -1.7407,  2.2475], dtype=torch.float64)\n",
      "current ratio: -1.0066650182431314\n",
      "tensor(0.8767, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8767479754177582\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8767479754177582\n",
      "t50: 16.77936876504003\n",
      "t85: 106.7117338387914\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 173 was 87.5%.\n",
      "current params: tensor([-2.2405, -1.7519,  2.2483], dtype=torch.float64)\n",
      "current ratio: -1.0035033173076933\n",
      "tensor(0.8760, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8759934047711405\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8759934047711405\n",
      "t50: 16.959044099074475\n",
      "t85: 108.03826077054589\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 174 was 87.5%.\n",
      "current params: tensor([-2.2483, -1.7631,  2.2491], dtype=torch.float64)\n",
      "current ratio: -1.0003582406711324\n",
      "tensor(0.8752, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8752421499626147\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8752421499626147\n",
      "t50: 17.1199923086012\n",
      "t85: 110.22565141948664\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 175 was 87.4%.\n",
      "current params: tensor([-2.2561, -1.7743,  2.2499], dtype=torch.float64)\n",
      "current ratio: -0.9972297126441528\n",
      "tensor(0.8750, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8749998334065724\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8749998334065724\n",
      "t50: 17.303408933519606\n",
      "t85: 111.60086192115047\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 176 was 87.4%.\n",
      "current params: tensor([-2.2640, -1.7854,  2.2507], dtype=torch.float64)\n",
      "current ratio: -0.9941177728834494\n",
      "tensor(0.8743, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8742541729468598\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8742541729468598\n",
      "t50: 17.488915870310834\n",
      "t85: 113.88928293362345\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 177 was 87.3%.\n",
      "current params: tensor([-2.2719, -1.7966,  2.2515], dtype=torch.float64)\n",
      "current ratio: -0.9910222327680319\n",
      "tensor(0.8735, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8735117514524746\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8735117514524746\n",
      "t50: 17.655069315464985\n",
      "t85: 115.31699467874118\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 178 was 87.2%.\n",
      "current params: tensor([-2.2797, -1.8077,  2.2523], dtype=torch.float64)\n",
      "current ratio: -0.987943017545611\n",
      "tensor(0.8728, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8727725444818081\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8727725444818081\n",
      "t50: 17.84444971721031\n",
      "t85: 117.71170153160483\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 179 was 87.2%.\n",
      "current params: tensor([-2.2876, -1.8187,  2.2531], dtype=torch.float64)\n",
      "current ratio: -0.9848800528337005\n",
      "tensor(0.8725, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8725416441010873\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8725416441010873\n",
      "t50: 18.014058629218646\n",
      "t85: 119.19319429765184\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 180 was 87.1%.\n",
      "current params: tensor([-2.2956, -1.8298,  2.2539], dtype=torch.float64)\n",
      "current ratio: -0.9818333795083595\n",
      "tensor(0.8718, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8718078155554448\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8718078155554448\n",
      "t50: 18.207469156869106\n",
      "t85: 121.70285130942091\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 181 was 87.1%.\n",
      "current params: tensor([-2.3035, -1.8408,  2.2547], dtype=torch.float64)\n",
      "current ratio: -0.9788028100151596\n",
      "tensor(0.8711, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8710771288408522\n",
      "Penalty:  tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8701618116492741\n",
      "t50: 18.403023681281198\n",
      "t85: 123.24252240585405\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 182 was 86.5%.\n",
      "current params: tensor([-2.2818, -1.8518,  2.2555], dtype=torch.float64)\n",
      "current ratio: -0.9884859264888958\n",
      "tensor(0.8654, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8654394340042525\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8654394340042525\n",
      "t50: 18.457564989575484\n",
      "t85: 135.78379127970956\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 183 was 86.4%.\n",
      "current params: tensor([-2.2895, -1.8627,  2.2563], dtype=torch.float64)\n",
      "current ratio: -0.9854835687378565\n",
      "tensor(0.8647, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8647016913684779\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8647016913684779\n",
      "t50: 18.65374678134006\n",
      "t85: 137.64622098901282\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 184 was 86.3%.\n",
      "current params: tensor([-2.2973, -1.8736,  2.2571], dtype=torch.float64)\n",
      "current ratio: -0.982496423234927\n",
      "tensor(0.8640, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8639670332380497\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8639670332380497\n",
      "t50: 18.828968154936813\n",
      "t85: 141.03968526011718\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 185 was 86.3%.\n",
      "current params: tensor([-2.3051, -1.8844,  2.2579], dtype=torch.float64)\n",
      "current ratio: -0.9795244287058258\n",
      "tensor(0.8637, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8637399244448478\n",
      "Penalty:  tensor(0.0025, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8611976323460085\n",
      "t50: 19.02924969485187\n",
      "t85: 142.99372630440914\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 186 was 85.7%.\n",
      "current params: tensor([-2.2837, -1.8953,  2.2588], dtype=torch.float64)\n",
      "current ratio: -0.9890836324592266\n",
      "tensor(0.8576, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8575950485665418\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8575950485665418\n",
      "t50: 19.094577073637566\n",
      "t85: 162.52378574855953\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 187 was 85.7%.\n",
      "current params: tensor([-2.2913, -1.9060,  2.2596], dtype=torch.float64)\n",
      "current ratio: -0.9861373801003185\n",
      "tensor(0.8574, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8573613577394882\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8573613577394882\n",
      "t50: 19.295699875937835\n",
      "t85: 167.72093809857057\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 188 was 85.6%.\n",
      "current params: tensor([-2.2990, -1.9167,  2.2604], dtype=torch.float64)\n",
      "current ratio: -0.9832055970648858\n",
      "tensor(0.8566, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8566253121724139\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8566253121724139\n",
      "t50: 19.49912274031808\n",
      "t85: 170.4922206879343\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 189 was 85.5%.\n",
      "current params: tensor([-2.3067, -1.9275,  2.2612], dtype=torch.float64)\n",
      "current ratio: -0.9802881117005883\n",
      "tensor(0.8559, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.855892252798776\n",
      "Penalty:  tensor(0.0041, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8517839783842266\n",
      "t50: 19.68031481308366\n",
      "t85: 176.2452425996315\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 190 was 85.0%.\n",
      "current params: tensor([-2.2855, -1.9381,  2.2621], dtype=torch.float64)\n",
      "current ratio: -0.989734542351573\n",
      "tensor(0.8503, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8502511733889092\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8502511733889092\n",
      "t50: 19.781712950982325\n",
      "t85: 222.3363453104696\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 191 was 84.9%.\n",
      "current params: tensor([-2.2931, -1.9487,  2.2629], dtype=torch.float64)\n",
      "current ratio: -0.9868406444941136\n",
      "tensor(0.8495, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8495121532727415\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8495121532727415\n",
      "t50: 19.96335390197328\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 192 was 84.8%.\n",
      "current params: tensor([-2.3006, -1.9593,  2.2637], dtype=torch.float64)\n",
      "current ratio: -0.9839603156504193\n",
      "tensor(0.8488, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8487760620117016\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8487760620117016\n",
      "t50: 20.172089977729254\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 193 was 84.8%.\n",
      "current params: tensor([-2.3082, -1.9699,  2.2646], dtype=torch.float64)\n",
      "current ratio: -0.9810935141907664\n",
      "tensor(0.8485, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8485467071385571\n",
      "Penalty:  tensor(0.0056, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8429328756078908\n",
      "t50: 20.357502844559846\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 194 was 84.2%.\n",
      "current params: tensor([-2.2873, -1.9804,  2.2654], dtype=torch.float64)\n",
      "current ratio: -0.9904374631562263\n",
      "tensor(0.8424, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8424012102567273\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8424012102567273\n",
      "t50: 20.47204814436843\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 195 was 84.2%.\n",
      "current params: tensor([-2.2947, -1.9909,  2.2663], dtype=torch.float64)\n",
      "current ratio: -0.9875923543788323\n",
      "tensor(0.8422, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.842166250875809\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.842166250875809\n",
      "t50: 20.658079439236882\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 196 was 84.1%.\n",
      "current params: tensor([-2.3022, -2.0013,  2.2671], dtype=torch.float64)\n",
      "current ratio: -0.9847602384471448\n",
      "tensor(0.8414, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8414294600919273\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8414294600919273\n",
      "t50: 20.872538357135557\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 197 was 84.0%.\n",
      "current params: tensor([-2.3097, -2.0117,  2.2680], dtype=torch.float64)\n",
      "current ratio: -0.9819409535838118\n",
      "tensor(0.8407, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8406955080272754\n",
      "Penalty:  tensor(0.0071, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.833636010251345\n",
      "t50: 21.089369198260314\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 198 was 83.5%.\n",
      "current params: tensor([-2.2890, -2.0221,  2.2688], dtype=torch.float64)\n",
      "current ratio: -0.9911915257464435\n",
      "tensor(0.8351, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8350549516331975\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8350549516331975\n",
      "t50: 21.190907808620207\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 199 was 83.4%.\n",
      "current params: tensor([-2.2963, -2.0324,  2.2697], dtype=torch.float64)\n",
      "current ratio: -0.9883922692850118\n",
      "tensor(0.8343, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8343159265723074\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8343159265723074\n",
      "t50: 21.409252668544386\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 200 was 83.3%.\n",
      "current params: tensor([-2.3037, -2.0427,  2.2705], dtype=torch.float64)\n",
      "current ratio: -0.9856052362714375\n",
      "tensor(0.8336, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.833579683825817\n",
      "Penalty:  tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.832479580422143\n",
      "t50: 21.602115972988873\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 201 was 82.7%.\n",
      "current params: tensor([-2.2831, -2.0530,  2.2714], dtype=torch.float64)\n",
      "current ratio: -0.9948896844297086\n",
      "tensor(0.8279, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8279248613453416\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8279248613453416\n",
      "t50: 21.71354345651947\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 202 was 82.7%.\n",
      "current params: tensor([-2.2903, -2.0632,  2.2722], dtype=torch.float64)\n",
      "current ratio: -0.9921217502547348\n",
      "tensor(0.8272, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8271837194309113\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8271837194309113\n",
      "t50: 21.935660704270035\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 203 was 82.6%.\n",
      "current params: tensor([-2.2975, -2.0734,  2.2731], dtype=torch.float64)\n",
      "current ratio: -0.9893654693075374\n",
      "tensor(0.8264, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8264453034477475\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8264453034477475\n",
      "t50: 22.160230018304375\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 204 was 82.6%.\n",
      "current params: tensor([-2.3048, -2.0835,  2.2740], dtype=torch.float64)\n",
      "current ratio: -0.9866208205387212\n",
      "tensor(0.8262, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8262125898654915\n",
      "Penalty:  tensor(0.0022, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8239851435370993\n",
      "t50: 22.358013036278535\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 205 was 82.0%.\n",
      "current params: tensor([-2.2844, -2.0937,  2.2749], dtype=torch.float64)\n",
      "current ratio: -0.9958286480022069\n",
      "tensor(0.8201, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8200561470279477\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8200561470279477\n",
      "t50: 22.513409424482607\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 206 was 81.9%.\n",
      "current params: tensor([-2.2915, -2.1038,  2.2757], dtype=torch.float64)\n",
      "current ratio: -0.9931017569412194\n",
      "tensor(0.8193, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8193158943285348\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8193158943285348\n",
      "t50: 22.712281686389847\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 207 was 81.9%.\n",
      "current params: tensor([-2.2987, -2.1138,  2.2766], dtype=torch.float64)\n",
      "current ratio: -0.9903859718963371\n",
      "tensor(0.8191, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8190810653654234\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8190810653654234\n",
      "t50: 22.943443253745762\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 208 was 81.8%.\n",
      "current params: tensor([-2.3059, -2.1239,  2.2775], dtype=torch.float64)\n",
      "current ratio: -0.9876814162725813\n",
      "tensor(0.8183, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8183451568737229\n",
      "Penalty:  tensor(0.0033, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8150555615066731\n",
      "t50: 23.14652276736392\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 209 was 81.2%.\n",
      "current params: tensor([-2.2856, -2.1339,  2.2784], dtype=torch.float64)\n",
      "current ratio: -0.9968198460012173\n",
      "tensor(0.8122, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8121918369029926\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8121918369029926\n",
      "t50: 23.31853110826908\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 210 was 81.1%.\n",
      "current params: tensor([-2.2927, -2.1438,  2.2792], dtype=torch.float64)\n",
      "current ratio: -0.9941317214208353\n",
      "tensor(0.8120, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8119551963196483\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8119551963196483\n",
      "t50: 23.52278638892704\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 211 was 81.1%.\n",
      "current params: tensor([-2.2998, -2.1538,  2.2801], dtype=torch.float64)\n",
      "current ratio: -0.9914543417398158\n",
      "tensor(0.8112, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8112176514892304\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8112176514892304\n",
      "t50: 23.76102915155008\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 212 was 81.0%.\n",
      "current params: tensor([-2.3069, -2.1637,  2.2810], dtype=torch.float64)\n",
      "current ratio: -0.9887875551731167\n",
      "tensor(0.8105, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8104826913863526\n",
      "Penalty:  tensor(0.0043, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8061972606877539\n",
      "t50: 24.00190535963557\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 213 was 80.4%.\n",
      "current params: tensor([-2.2868, -2.1736,  2.2819], dtype=torch.float64)\n",
      "current ratio: -0.9978634045131328\n",
      "tensor(0.8048, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8048353010070545\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8048353010070545\n",
      "t50: 24.159422018262088\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 214 was 80.4%.\n",
      "current params: tensor([-2.2938, -2.1834,  2.2828], dtype=torch.float64)\n",
      "current ratio: -0.9952121358064426\n",
      "tensor(0.8041, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8040963975150421\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8040963975150421\n",
      "t50: 24.369464848468425\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 215 was 80.3%.\n",
      "current params: tensor([-2.3008, -2.1932,  2.2837], dtype=torch.float64)\n",
      "current ratio: -0.9925710069344315\n",
      "tensor(0.8034, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8033600250635662\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.8033600250635662\n",
      "t50: 24.615103080207447\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 216 was 80.2%.\n",
      "current params: tensor([-2.3078, -2.2030,  2.2846], dtype=torch.float64)\n",
      "current ratio: -0.9899400105058804\n",
      "tensor(0.8026, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.8026261788178125\n",
      "Penalty:  tensor(0.0052, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7974125023199627\n",
      "t50: 24.86345780034015\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 217 was 79.6%.\n",
      "current params: tensor([-2.2879, -2.2128,  2.2855], dtype=torch.float64)\n",
      "current ratio: -0.9989596272523077\n",
      "tensor(0.7970, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7969825441215438\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7969825441215438\n",
      "t50: 25.038439606425253\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 218 was 79.6%.\n",
      "current params: tensor([-2.2948, -2.2225,  2.2864], dtype=torch.float64)\n",
      "current ratio: -0.9963432111802442\n",
      "tensor(0.7962, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7962450034792391\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7962450034792391\n",
      "t50: 25.25448428752176\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 219 was 79.5%.\n",
      "current params: tensor([-2.3017, -2.2322,  2.2873], dtype=torch.float64)\n",
      "current ratio: -0.9937365044519835\n",
      "tensor(0.7955, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7955099371008232\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7955099371008232\n",
      "t50: 25.507953942248477\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 220 was 79.5%.\n",
      "current params: tensor([-2.3087, -2.2419,  2.2882], dtype=torch.float64)\n",
      "current ratio: -0.9911395032485609\n",
      "tensor(0.7953, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7952791523925973\n",
      "Penalty:  tensor(0.0061, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7892063935612759\n",
      "t50: 25.764228570439343\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 221 was 78.9%.\n",
      "current params: tensor([-2.2889, -2.2516,  2.2891], dtype=torch.float64)\n",
      "current ratio: -1.0001089763296602\n",
      "tensor(0.7891, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7891369635877168\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7891369635877168\n",
      "t50: 25.958155070563897\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 222 was 78.8%.\n",
      "current params: tensor([-2.2957, -2.2612,  2.2900], dtype=torch.float64)\n",
      "current ratio: -0.9975257228503837\n",
      "tensor(0.7884, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7884009077569997\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7884009077569997\n",
      "t50: 26.18053344160439\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 223 was 78.7%.\n",
      "current params: tensor([-2.3026, -2.2708,  2.2910], dtype=torch.float64)\n",
      "current ratio: -0.9949517785430347\n",
      "tensor(0.7877, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7876672717594353\n",
      "Penalty:  tensor(5.5456e-06, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7876617261574871\n",
      "t50: 26.442299244344678\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 224 was 78.2%.\n",
      "current params: tensor([-2.2828, -2.2804,  2.2919], dtype=torch.float64)\n",
      "current ratio: -1.003978399543406\n",
      "tensor(0.7820, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7820183881432666\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7820183881432666\n",
      "t50: 26.651729658339168\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 225 was 78.1%.\n",
      "current params: tensor([-2.2896, -2.2899,  2.2928], dtype=torch.float64)\n",
      "current ratio: -1.001272512740455\n",
      "tensor(0.7813, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7812813507136923\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7812813507136923\n",
      "t50: 26.916777238541485\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 226 was 78.0%.\n",
      "current params: tensor([-2.2963, -2.2994,  2.2937], dtype=torch.float64)\n",
      "current ratio: -0.9975412893515788\n",
      "tensor(0.7805, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7805466831915873\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7805466831915873\n",
      "t50: 27.145793164308948\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 227 was 78.0%.\n",
      "current params: tensor([-2.3031, -2.3089,  2.2947], dtype=torch.float64)\n",
      "current ratio: -0.9938459728705337\n",
      "tensor(0.7803, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7803157128829297\n",
      "Penalty:  tensor(0.0068, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7735068276845645\n",
      "t50: 27.416298812140422\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 228 was 78.1%.\n",
      "current params: tensor([-2.2835, -2.2832,  2.2956], dtype=torch.float64)\n",
      "current ratio: -1.0053117685709267\n",
      "tensor(0.7813, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7813209415941411\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7813209415941411\n",
      "t50: 26.778488107839017\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 229 was 78.1%.\n",
      "current params: tensor([-2.2902, -2.2924,  2.2965], dtype=torch.float64)\n",
      "current ratio: -1.0017820446901906\n",
      "tensor(0.7811, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7811079263165105\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7811079263165105\n",
      "t50: 27.002168344456024\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 230 was 78.0%.\n",
      "current params: tensor([-2.2969, -2.3017,  2.2975], dtype=torch.float64)\n",
      "current ratio: -0.9981718216347505\n",
      "tensor(0.7804, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7803946792035354\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7803946792035354\n",
      "t50: 27.267192834666798\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 231 was 77.9%.\n",
      "current params: tensor([-2.3037, -2.3109,  2.2984], dtype=torch.float64)\n",
      "current ratio: -0.9945945574245816\n",
      "tensor(0.7797, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7796835679805244\n",
      "Penalty:  tensor(0.0094, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7703203470254139\n",
      "t50: 27.495613391830755\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 232 was 78.1%.\n",
      "current params: tensor([-2.2841, -2.2859,  2.2994], dtype=torch.float64)\n",
      "current ratio: -1.0058728370860552\n",
      "tensor(0.7811, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7811343375725012\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7811343375725012\n",
      "t50: 26.867041373850768\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 233 was 78.0%.\n",
      "current params: tensor([-2.2908, -2.2950,  2.3003], dtype=torch.float64)\n",
      "current ratio: -1.0023431761748551\n",
      "tensor(0.7804, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7804387374584989\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7804387374584989\n",
      "t50: 27.126963307494094\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 234 was 78.0%.\n",
      "current params: tensor([-2.2975, -2.3040,  2.3013], dtype=torch.float64)\n",
      "current ratio: -0.9988442807950154\n",
      "tensor(0.7802, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.780246415776771\n",
      "Penalty:  tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7788788671575972\n",
      "t50: 27.35043681188619\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 235 was 78.6%.\n",
      "current params: tensor([-2.3042, -2.2794,  2.3023], dtype=torch.float64)\n",
      "current ratio: -0.9991601584510713\n",
      "tensor(0.7865, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7865303697816526\n",
      "Penalty:  tensor(0.0016, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7849250183507275\n",
      "t50: 26.77883485223166\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 236 was 78.0%.\n",
      "current params: tensor([-2.2848, -2.2882,  2.3032], dtype=torch.float64)\n",
      "current ratio: -1.0065570987679102\n",
      "tensor(0.7810, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7809972156813498\n",
      "Penalty:  tensor(0.0649, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7161175550460013\n",
      "t50: 26.989765215732056\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 237 was 77.9%.\n",
      "current params: tensor([-2.2914, -2.2970,  2.2149], dtype=torch.float64)\n",
      "current ratio: -0.9642618743274333\n",
      "tensor(0.7797, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7797458433441017\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7797458433441017\n",
      "t50: 26.244665591044534\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 238 was 77.9%.\n",
      "current params: tensor([-2.2981, -2.3058,  2.2154], dtype=torch.float64)\n",
      "current ratio: -0.9607816805911029\n",
      "tensor(0.7791, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7790679142084319\n",
      "Penalty:  tensor(0.0032, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7758321101330109\n",
      "t50: 26.490065727143424\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 239 was 78.5%.\n",
      "current params: tensor([-2.3047, -2.2818,  2.2159], dtype=torch.float64)\n",
      "current ratio: -0.9614396446579576\n",
      "tensor(0.7853, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7852871018638022\n",
      "Penalty:  tensor(0.0021, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7831460831828693\n",
      "t50: 25.898940560531372\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 240 was 77.9%.\n",
      "current params: tensor([-2.2854, -2.2905,  2.2163], dtype=torch.float64)\n",
      "current ratio: -0.9676240495053925\n",
      "tensor(0.7798, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7797720800833525\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7797720800833525\n",
      "t50: 26.097863701097936\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 241 was 77.9%.\n",
      "current params: tensor([-2.2920, -2.2991,  2.2168], dtype=torch.float64)\n",
      "current ratio: -0.9642044935768984\n",
      "tensor(0.7791, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7791081960831392\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7791081960831392\n",
      "t50: 26.30085304417574\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 242 was 77.8%.\n",
      "current params: tensor([-2.2986, -2.3077,  2.2173], dtype=torch.float64)\n",
      "current ratio: -0.9608111877477288\n",
      "tensor(0.7789, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7789477077699074\n",
      "Penalty:  tensor(0.0051, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7738500899795906\n",
      "t50: 26.543910257613856\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 243 was 78.5%.\n",
      "current params: tensor([-2.3052, -2.2842,  2.2177], dtype=torch.float64)\n",
      "current ratio: -0.9620376355697733\n",
      "tensor(0.7851, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7851100797648348\n",
      "Penalty:  tensor(0.0027, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7824552831326583\n",
      "t50: 25.99443469067612\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 244 was 77.9%.\n",
      "current params: tensor([-2.2861, -2.2927,  2.2182], dtype=torch.float64)\n",
      "current ratio: -0.9675072684882954\n",
      "tensor(0.7796, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7796241410749878\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7796241410749878\n",
      "t50: 26.15665055539804\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 245 was 77.8%.\n",
      "current params: tensor([-2.2926, -2.3011,  2.2187], dtype=torch.float64)\n",
      "current ratio: -0.9641650289384965\n",
      "tensor(0.7790, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7789735064243419\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7789735064243419\n",
      "t50: 26.395571607883905\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 246 was 77.8%.\n",
      "current params: tensor([-2.2992, -2.3096,  2.2192], dtype=torch.float64)\n",
      "current ratio: -0.9608474203214509\n",
      "tensor(0.7783, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7783244356797961\n",
      "Penalty:  tensor(0.0070, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7713459254042572\n",
      "t50: 26.598695200417797\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 247 was 78.4%.\n",
      "current params: tensor([-2.3058, -2.2866,  2.2196], dtype=torch.float64)\n",
      "current ratio: -0.962647616049711\n",
      "tensor(0.7849, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7849375115897924\n",
      "Penalty:  tensor(0.0032, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7817597165571004\n",
      "t50: 26.053433285830856\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 248 was 77.8%.\n",
      "current params: tensor([-2.2867, -2.2949,  2.2201], dtype=torch.float64)\n",
      "current ratio: -0.9674136837724668\n",
      "tensor(0.7790, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7789765890444508\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7789765890444508\n",
      "t50: 26.253609326353736\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 249 was 77.8%.\n",
      "current params: tensor([-2.2932, -2.3032,  2.2206], dtype=torch.float64)\n",
      "current ratio: -0.9641450097333409\n",
      "tensor(0.7788, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7788403594373838\n",
      "Penalty:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7782310820466851\n",
      "t50: 26.452948844773672\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 250 was 78.4%.\n",
      "current params: tensor([-2.2998, -2.2804,  2.2211], dtype=torch.float64)\n",
      "current ratio: -0.9658007137750767\n",
      "tensor(0.7849, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7849219932166307\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7849219932166307\n",
      "t50: 25.913142023272947\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 251 was 78.4%.\n",
      "current params: tensor([-2.3063, -2.2886,  2.2216], dtype=torch.float64)\n",
      "current ratio: -0.9632634229822684\n",
      "tensor(0.7843, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7842992738707071\n",
      "Penalty:  tensor(0.0037, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7805702122296299\n",
      "t50: 26.14648211819253\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 252 was 77.8%.\n",
      "current params: tensor([-2.2874, -2.2968,  2.2221], dtype=torch.float64)\n",
      "current ratio: -0.9674714328259315\n",
      "tensor(0.7789, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7788663309442146\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7788663309442146\n",
      "t50: 26.30970791402208\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 253 was 77.8%.\n",
      "current params: tensor([-2.2938, -2.3049,  2.2226], dtype=torch.float64)\n",
      "current ratio: -0.9642673617735371\n",
      "tensor(0.7782, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7782392442901184\n",
      "Penalty:  tensor(0.0024, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7758796634146582\n",
      "t50: 26.50737600301495\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 254 was 78.4%.\n",
      "current params: tensor([-2.3003, -2.2826,  2.2231], dtype=torch.float64)\n",
      "current ratio: -0.9664126053376964\n",
      "tensor(0.7843, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.784276941000293\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.784276941000293\n",
      "t50: 26.007978628603002\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 255 was 78.4%.\n",
      "current params: tensor([-2.3069, -2.2906,  2.2236], dtype=torch.float64)\n",
      "current ratio: -0.963891158612916\n",
      "tensor(0.7842, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7841666630125534\n",
      "Penalty:  tensor(0.0043, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7798759295740847\n",
      "t50: 26.202872673029404\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 256 was 77.8%.\n",
      "current params: tensor([-2.2880, -2.2987,  2.2241], dtype=torch.float64)\n",
      "current ratio: -0.9675473422483132\n",
      "tensor(0.7788, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7787572094991436\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7787572094991436\n",
      "t50: 26.366405965498128\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 257 was 77.8%.\n",
      "current params: tensor([-2.2945, -2.3067,  2.2246], dtype=torch.float64)\n",
      "current ratio: -0.9644025760037521\n",
      "tensor(0.7781, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7781399035244975\n",
      "Penalty:  tensor(0.0041, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7740236901523496\n",
      "t50: 26.60091736572899\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 258 was 78.4%.\n",
      "current params: tensor([-2.3009, -2.2847,  2.2251], dtype=torch.float64)\n",
      "current ratio: -0.9670374185058827\n",
      "tensor(0.7841, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.784136634246506\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.784136634246506\n",
      "t50: 26.066456647940345\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 259 was 78.4%.\n",
      "current params: tensor([-2.3075, -2.2926,  2.2256], dtype=torch.float64)\n",
      "current ratio: -0.9645312146333874\n",
      "tensor(0.7840, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7840351620562821\n",
      "Penalty:  tensor(0.0049, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7791717962403069\n",
      "t50: 26.260026025025947\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 260 was 77.8%.\n",
      "current params: tensor([-2.2887, -2.3006,  2.2261], dtype=torch.float64)\n",
      "current ratio: -0.9676346856052545\n",
      "tensor(0.7781, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7781458208004703\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7781458208004703\n",
      "t50: 26.462083625830996\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 261 was 77.8%.\n",
      "current params: tensor([-2.2951, -2.3085,  2.2266], dtype=torch.float64)\n",
      "current ratio: -0.9645475702129078\n",
      "tensor(0.7780, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7780398146114443\n",
      "Penalty:  tensor(0.0059, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7721523725494956\n",
      "t50: 26.657138086322426\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 262 was 78.3%.\n",
      "current params: tensor([-2.3016, -2.2868,  2.2272], dtype=torch.float64)\n",
      "current ratio: -0.9676753059830348\n",
      "tensor(0.7840, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7839987289611982\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7839987289611982\n",
      "t50: 26.16273317276061\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 263 was 78.3%.\n",
      "current params: tensor([-2.3080, -2.2947,  2.2277], dtype=torch.float64)\n",
      "current ratio: -0.9651837252268217\n",
      "tensor(0.7834, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.783403430036205\n",
      "Penalty:  tensor(0.0054, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7779562837315324\n",
      "t50: 26.355413452158118\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 264 was 77.8%.\n",
      "current params: tensor([-2.2894, -2.3025,  2.2282], dtype=torch.float64)\n",
      "current ratio: -0.9677388464608311\n",
      "tensor(0.7780, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7780369734117493\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7780369734117493\n",
      "t50: 26.520430296544582\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 265 was 77.7%.\n",
      "current params: tensor([-2.2958, -2.3103,  2.2287], dtype=torch.float64)\n",
      "current ratio: -0.9647021917399033\n",
      "tensor(0.7779, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7779389173668061\n",
      "Penalty:  tensor(0.0077, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7702646115765627\n",
      "t50: 26.71431401006117\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 266 was 78.3%.\n",
      "current params: tensor([-2.3022, -2.2889,  2.2293], dtype=torch.float64)\n",
      "current ratio: -0.9683264263032014\n",
      "tensor(0.7834, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7833610113700631\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7833610113700631\n",
      "t50: 26.222530089040788\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 267 was 78.3%.\n",
      "current params: tensor([-2.3086, -2.2967,  2.2298], dtype=torch.float64)\n",
      "current ratio: -0.9658488180896088\n",
      "tensor(0.7833, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7832754584669445\n",
      "Penalty:  tensor(0.0060, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7772331829984388\n",
      "t50: 26.414071166184304\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 268 was 77.7%.\n",
      "current params: tensor([-2.2900, -2.3044,  2.2303], dtype=torch.float64)\n",
      "current ratio: -0.967856150201189\n",
      "tensor(0.7779, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7779279991991507\n",
      "Penalty:  tensor(0.0018, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7761029172052963\n",
      "t50: 26.579631598638972\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 269 was 78.3%.\n",
      "current params: tensor([-2.2964, -2.2832,  2.2309], dtype=torch.float64)\n",
      "current ratio: -0.9714559729831622\n",
      "tensor(0.7838, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7838340459635005\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7838340459635005\n",
      "t50: 26.091515892468475\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 270 was 78.3%.\n",
      "current params: tensor([-2.3028, -2.2908,  2.2314], dtype=torch.float64)\n",
      "current ratio: -0.9689852309248406\n",
      "tensor(0.7833, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7832524438796039\n",
      "Penalty:  tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.782998205643844\n",
      "t50: 26.280329426965626\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 271 was 77.7%.\n",
      "current params: tensor([-2.2842, -2.2985,  2.2320], dtype=torch.float64)\n",
      "current ratio: -0.9710696031992405\n",
      "tensor(0.7779, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7779132392653727\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7779132392653727\n",
      "t50: 26.443464785175607\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 272 was 77.7%.\n",
      "current params: tensor([-2.2906, -2.3060,  2.2325], dtype=torch.float64)\n",
      "current ratio: -0.9681088376016415\n",
      "tensor(0.7778, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7778287882995524\n",
      "Penalty:  tensor(0.0035, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7743712870880496\n",
      "t50: 26.67194806880052\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 273 was 78.3%.\n",
      "current params: tensor([-2.2969, -2.2851,  2.2331], dtype=torch.float64)\n",
      "current ratio: -0.9721992517645113\n",
      "tensor(0.7832, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7832023042048712\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7832023042048712\n",
      "t50: 26.147543298373574\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 274 was 78.3%.\n",
      "current params: tensor([-2.3033, -2.2926,  2.2336], dtype=torch.float64)\n",
      "current ratio: -0.9697415294391183\n",
      "tensor(0.7831, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.783129331253047\n",
      "Penalty:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7824147490395861\n",
      "t50: 26.372973169982743\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 275 was 77.7%.\n",
      "current params: tensor([-2.2848, -2.3002,  2.2342], dtype=torch.float64)\n",
      "current ratio: -0.9712916445154667\n",
      "tensor(0.7778, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7778069436113287\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7778069436113287\n",
      "t50: 26.537391439559723\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 276 was 77.7%.\n",
      "current params: tensor([-2.2911, -2.3077,  2.2347], dtype=torch.float64)\n",
      "current ratio: -0.9683721891339622\n",
      "tensor(0.7772, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7772269698528486\n",
      "Penalty:  tensor(0.0051, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7721213401076407\n",
      "t50: 26.726794685387386\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 277 was 78.3%.\n",
      "current params: tensor([-2.2974, -2.2870,  2.2353], dtype=torch.float64)\n",
      "current ratio: -0.972957709817221\n",
      "tensor(0.7831, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7830741702439608\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7830741702439608\n",
      "t50: 26.241591428201705\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 278 was 78.3%.\n",
      "current params: tensor([-2.3038, -2.2945,  2.2358], dtype=torch.float64)\n",
      "current ratio: -0.9705126074704674\n",
      "tensor(0.7830, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7830067579788605\n",
      "Penalty:  tensor(0.0012, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7818227757795322\n",
      "t50: 26.428933499066808\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 279 was 77.7%.\n",
      "current params: tensor([-2.2853, -2.3019,  2.2364], dtype=torch.float64)\n",
      "current ratio: -0.9715270135133477\n",
      "tensor(0.7777, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7777001569048035\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7777001569048035\n",
      "t50: 26.593972583281808\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 280 was 77.7%.\n",
      "current params: tensor([-2.2916, -2.3094,  2.2370], dtype=torch.float64)\n",
      "current ratio: -0.968646242458725\n",
      "tensor(0.7771, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7771257934977895\n",
      "Penalty:  tensor(0.0068, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7703556863275736\n",
      "t50: 26.782606165862095\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 281 was 78.2%.\n",
      "current params: tensor([-2.2979, -2.2889,  2.2375], dtype=torch.float64)\n",
      "current ratio: -0.9737317354068457\n",
      "tensor(0.7829, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7829461917701185\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7829461917701185\n",
      "t50: 26.299182168707254\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 282 was 78.2%.\n",
      "current params: tensor([-2.3042, -2.2963,  2.2381], dtype=torch.float64)\n",
      "current ratio: -0.9712988114386651\n",
      "tensor(0.7824, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7823821515041071\n",
      "Penalty:  tensor(0.0017, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7807194131463988\n",
      "t50: 26.4858437573732\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 283 was 77.7%.\n",
      "current params: tensor([-2.2858, -2.3037,  2.2387], dtype=torch.float64)\n",
      "current ratio: -0.9717754970161541\n",
      "tensor(0.7771, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7770911627524001\n",
      "Penalty:  tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7759634111404562\n",
      "t50: 26.651452848525903\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 284 was 78.2%.\n",
      "current params: tensor([-2.2921, -2.2833,  2.2393], dtype=torch.float64)\n",
      "current ratio: -0.9769434797470823\n",
      "tensor(0.7829, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7828992220544134\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7828992220544134\n",
      "t50: 26.170986278507897\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 285 was 78.2%.\n",
      "current params: tensor([-2.2984, -2.2906,  2.2399], dtype=torch.float64)\n",
      "current ratio: -0.9745160970156728\n",
      "tensor(0.7823, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7823398246625024\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7823398246625024\n",
      "t50: 26.35520590041622\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 286 was 78.2%.\n",
      "current params: tensor([-2.3048, -2.2980,  2.2404], dtype=torch.float64)\n",
      "current ratio: -0.9720948642293943\n",
      "tensor(0.7823, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7822830682387597\n",
      "Penalty:  tensor(0.0022, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7801159845305596\n",
      "t50: 26.579090736577864\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 287 was 77.7%.\n",
      "current params: tensor([-2.2864, -2.3053,  2.2410], dtype=torch.float64)\n",
      "current ratio: -0.972125253750998\n",
      "tensor(0.7770, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7770063917447936\n",
      "Penalty:  tensor(0.0027, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7743063367107004\n",
      "t50: 26.746053601784002\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 288 was 78.2%.\n",
      "current params: tensor([-2.2927, -2.2851,  2.2416], dtype=torch.float64)\n",
      "current ratio: -0.9777320577519201\n",
      "tensor(0.7823, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7822886688081083\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7822886688081083\n",
      "t50: 26.265928187064077\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 289 was 78.2%.\n",
      "current params: tensor([-2.2990, -2.2924,  2.2422], dtype=torch.float64)\n",
      "current ratio: -0.9753160767969499\n",
      "tensor(0.7822, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7822361862613559\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7822361862613559\n",
      "t50: 26.449786114326926\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 290 was 78.2%.\n",
      "current params: tensor([-2.3053, -2.2996,  2.2428], dtype=torch.float64)\n",
      "current ratio: -0.9729062315656409\n",
      "tensor(0.7822, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7821838663931153\n",
      "Penalty:  tensor(0.0027, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7795020191591281\n",
      "t50: 26.63545142273525\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 291 was 77.6%.\n",
      "current params: tensor([-2.2870, -2.3069,  2.2434], dtype=torch.float64)\n",
      "current ratio: -0.972486538344428\n",
      "tensor(0.7769, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7769206748703839\n",
      "Penalty:  tensor(0.0043, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.772631278561454\n",
      "t50: 26.80309206239097\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 292 was 78.2%.\n",
      "current params: tensor([-2.2932, -2.2869,  2.2440], dtype=torch.float64)\n",
      "current ratio: -0.9785369490847848\n",
      "tensor(0.7822, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7821804103206352\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7821804103206352\n",
      "t50: 26.324317881543294\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 293 was 78.2%.\n",
      "current params: tensor([-2.2995, -2.2941,  2.2446], dtype=torch.float64)\n",
      "current ratio: -0.9761320897436416\n",
      "tensor(0.7821, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7821320217970661\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7821320217970661\n",
      "t50: 26.50771258584847\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 294 was 78.1%.\n",
      "current params: tensor([-2.3058, -2.3013,  2.2452], dtype=torch.float64)\n",
      "current ratio: -0.9737332730313988\n",
      "tensor(0.7816, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7815821513994294\n",
      "Penalty:  tensor(0.0032, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7783747928217528\n",
      "t50: 26.692910083197727\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 295 was 77.6%.\n",
      "current params: tensor([-2.2876, -2.3085,  2.2458], dtype=torch.float64)\n",
      "current ratio: -0.9728594533705212\n",
      "tensor(0.7768, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7768339960875068\n",
      "Penalty:  tensor(0.0059, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7709376855075631\n",
      "t50: 26.861171068427158\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 296 was 78.2%.\n",
      "current params: tensor([-2.2938, -2.2887,  2.2465], dtype=torch.float64)\n",
      "current ratio: -0.9793583752139942\n",
      "tensor(0.7821, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.782072535310163\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.782072535310163\n",
      "t50: 26.383598459792413\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 297 was 78.2%.\n",
      "current params: tensor([-2.3001, -2.2959,  2.2471], dtype=torch.float64)\n",
      "current ratio: -0.9769642962050518\n",
      "tensor(0.7820, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7820279109367233\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7820279109367233\n",
      "t50: 26.566602999515954\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 298 was 78.1%.\n",
      "current params: tensor([-2.3063, -2.3030,  2.2477], dtype=torch.float64)\n",
      "current ratio: -0.9745761725646211\n",
      "tensor(0.7815, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7814818027045201\n",
      "Penalty:  tensor(0.0042, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7773267660350036\n",
      "t50: 26.751406504308065\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 299 was 78.2%.\n",
      "current params: tensor([-2.2882, -2.2833,  2.2483], dtype=torch.float64)\n",
      "current ratio: -0.9825810562635163\n",
      "tensor(0.7820, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7820257920000251\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7820257920000251\n",
      "t50: 26.26091210689976\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 300 was 78.1%.\n",
      "current params: tensor([-2.2944, -2.2904,  2.2490], dtype=torch.float64)\n",
      "current ratio: -0.98019160882171\n",
      "tensor(0.7820, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7819845834052233\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7819845834052233\n",
      "t50: 26.441796905302223\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 301 was 78.1%.\n",
      "current params: tensor([-2.3006, -2.2975,  2.2496], dtype=torch.float64)\n",
      "current ratio: -0.9778080064808359\n",
      "tensor(0.7814, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7814418569673578\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7814418569673578\n",
      "t50: 26.624458423774744\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 302 was 78.1%.\n",
      "current params: tensor([-2.3069, -2.3045,  2.2502], dtype=torch.float64)\n",
      "current ratio: -0.9754301803581009\n",
      "tensor(0.7814, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7814015942724398\n",
      "Penalty:  tensor(0.0062, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7751517219010681\n",
      "t50: 26.847224454049343\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 303 was 78.1%.\n",
      "current params: tensor([-2.2888, -2.2850,  2.2509], dtype=torch.float64)\n",
      "current ratio: -0.9834208014954635\n",
      "tensor(0.7819, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7819345923264\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7819345923264\n",
      "t50: 26.358070483464314\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 304 was 78.1%.\n",
      "current params: tensor([-2.2950, -2.2920,  2.2515], dtype=torch.float64)\n",
      "current ratio: -0.981041609242304\n",
      "tensor(0.7814, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7813949995044084\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7813949995044084\n",
      "t50: 26.538945997188527\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 305 was 78.1%.\n",
      "current params: tensor([-2.3012, -2.2991,  2.2521], dtype=torch.float64)\n",
      "current ratio: -0.9786680852729566\n",
      "tensor(0.7814, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7813578716181611\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7813578716181611\n",
      "t50: 26.72150728715618\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 306 was 78.1%.\n",
      "current params: tensor([-2.3075, -2.3061,  2.2528], dtype=torch.float64)\n",
      "current ratio: -0.9763003563242002\n",
      "tensor(0.7813, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7813208536189902\n",
      "Penalty:  tensor(0.0084, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.772945854881113\n",
      "t50: 26.90586329181491\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 307 was 78.1%.\n",
      "current params: tensor([-2.2894, -2.2868,  2.2534], dtype=torch.float64)\n",
      "current ratio: -0.984277831334849\n",
      "tensor(0.7818, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7818435790283855\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7818435790283855\n",
      "t50: 26.418611367665747\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 308 was 78.1%.\n",
      "current params: tensor([-2.2956, -2.2937,  2.2541], dtype=torch.float64)\n",
      "current ratio: -0.981908611180672\n",
      "tensor(0.7813, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7813070260224176\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7813070260224176\n",
      "t50: 26.599271899786043\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 309 was 78.1%.\n",
      "current params: tensor([-2.3018, -2.3007,  2.2548], dtype=torch.float64)\n",
      "current ratio: -0.9795449841121306\n",
      "tensor(0.7813, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7812729076072334\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7812729076072334\n",
      "t50: 26.781616472514713\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 310 was 78.1%.\n",
      "current params: tensor([-2.3081, -2.3077,  2.2554], dtype=torch.float64)\n",
      "current ratio: -0.9771870801152291\n",
      "tensor(0.7812, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7812388892700305\n",
      "Penalty:  tensor(0.0105, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7707010937072903\n",
      "t50: 26.96575378585054\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 311 was 78.1%.\n",
      "current params: tensor([-2.2901, -2.2885,  2.2561], dtype=torch.float64)\n",
      "current ratio: -0.9851524728495479\n",
      "tensor(0.7813, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7812503179535186\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7812503179535186\n",
      "t50: 26.480260818654934\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 312 was 78.1%.\n",
      "current params: tensor([-2.2963, -2.2954,  2.2567], dtype=torch.float64)\n",
      "current ratio: -0.9827928575754116\n",
      "tensor(0.7812, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7812188902983535\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7812188902983535\n",
      "t50: 26.660674559726054\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 313 was 78.1%.\n",
      "current params: tensor([-2.3025, -2.3023,  2.2574], dtype=torch.float64)\n",
      "current ratio: -0.9804388683060963\n",
      "tensor(0.7812, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7811875524057933\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7811875524057933\n",
      "t50: 26.842860750969688\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 314 was 78.0%.\n",
      "current params: tensor([-2.3087, -2.3093,  2.2581], dtype=torch.float64)\n",
      "current ratio: -0.9778378165905978\n",
      "tensor(0.7807, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7806547896645607\n",
      "Penalty:  tensor(0.0127, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7679220362642942\n",
      "t50: 27.026838336448943\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 315 was 78.1%.\n",
      "current params: tensor([-2.2907, -2.2902,  2.2588], dtype=torch.float64)\n",
      "current ratio: -0.9860447509502137\n",
      "tensor(0.7812, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7811593436086999\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7811593436086999\n",
      "t50: 26.542883171610775\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 316 was 78.1%.\n",
      "current params: tensor([-2.2969, -2.2971,  2.2594], dtype=torch.float64)\n",
      "current ratio: -0.9836061776309893\n",
      "tensor(0.7811, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7811305057460588\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7811305057460588\n",
      "t50: 26.72319322223176\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 317 was 78.0%.\n",
      "current params: tensor([-2.3031, -2.3040,  2.2601], dtype=torch.float64)\n",
      "current ratio: -0.9809633777303425\n",
      "tensor(0.7806, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7806002140001967\n",
      "Penalty:  tensor(0.0019, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7786917162168744\n",
      "t50: 26.90527495130271\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 318 was 78.1%.\n",
      "current params: tensor([-2.2851, -2.2850,  2.2608], dtype=torch.float64)\n",
      "current ratio: -0.9893720794298777\n",
      "tensor(0.7811, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7810980403925514\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7810980403925514\n",
      "t50: 26.423278165939625\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 319 was 78.1%.\n",
      "current params: tensor([-2.2912, -2.2918,  2.2615], dtype=torch.float64)\n",
      "current ratio: -0.9867869688660563\n",
      "tensor(0.7811, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7810715721411701\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7810715721411701\n",
      "t50: 26.639885667008322\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 320 was 78.1%.\n",
      "current params: tensor([-2.2974, -2.2986,  2.2622], dtype=torch.float64)\n",
      "current ratio: -0.984156615756445\n",
      "tensor(0.7810, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7810451771679132\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7810451771679132\n",
      "t50: 26.82034990598638\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 321 was 78.0%.\n",
      "current params: tensor([-2.3036, -2.3055,  2.2629], dtype=torch.float64)\n",
      "current ratio: -0.9815359302867039\n",
      "tensor(0.7805, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7805173354531831\n",
      "Penalty:  tensor(0.0039, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7766565510538219\n",
      "t50: 27.002587595487253\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 322 was 78.1%.\n",
      "current params: tensor([-2.2856, -2.2866,  2.2636], dtype=torch.float64)\n",
      "current ratio: -0.9899473397458021\n",
      "tensor(0.7810, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7810069999864899\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7810069999864899\n",
      "t50: 26.52132554106339\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 323 was 78.0%.\n",
      "current params: tensor([-2.2918, -2.2934,  2.2643], dtype=torch.float64)\n",
      "current ratio: -0.9873289372516819\n",
      "tensor(0.7810, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7809828009704325\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7809828009704325\n",
      "t50: 26.699974480555934\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 324 was 78.0%.\n",
      "current params: tensor([-2.2979, -2.3002,  2.2650], dtype=torch.float64)\n",
      "current ratio: -0.9847199756419406\n",
      "tensor(0.7805, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7804571293413991\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7804571293413991\n",
      "t50: 26.880377781568367\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 325 was 78.0%.\n",
      "current params: tensor([-2.3041, -2.3070,  2.2657], dtype=torch.float64)\n",
      "current ratio: -0.9821230114784026\n",
      "tensor(0.7804, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7804337471122443\n",
      "Penalty:  tensor(0.0058, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7745920612388549\n",
      "t50: 27.062463974001865\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 326 was 78.0%.\n",
      "current params: tensor([-2.2862, -2.2882,  2.2664], dtype=torch.float64)\n",
      "current ratio: -0.990485186315291\n",
      "tensor(0.7809, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.780915726902707\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.780915726902707\n",
      "t50: 26.582524163329936\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 327 was 78.0%.\n",
      "current params: tensor([-2.2923, -2.2950,  2.2672], dtype=torch.float64)\n",
      "current ratio: -0.9878874664455328\n",
      "tensor(0.7804, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7803920708609143\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7803920708609143\n",
      "t50: 26.76115981562701\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 328 was 78.0%.\n",
      "current params: tensor([-2.2984, -2.3017,  2.2679], dtype=torch.float64)\n",
      "current ratio: -0.9853015243897102\n",
      "tensor(0.7804, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7803707099976259\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7803707099976259\n",
      "t50: 26.941460737433545\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 329 was 78.0%.\n",
      "current params: tensor([-2.3046, -2.3085,  2.2686], dtype=torch.float64)\n",
      "current ratio: -0.9827247723659355\n",
      "tensor(0.7803, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7803494111134611\n",
      "Penalty:  tensor(0.0079, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7724973970412107\n",
      "t50: 27.123534092664304\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 330 was 78.0%.\n",
      "current params: tensor([-2.2867, -2.2899,  2.2693], dtype=torch.float64)\n",
      "current ratio: -0.9910404739488797\n",
      "tensor(0.7803, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7803225726456848\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7803225726456848\n",
      "t50: 26.644808971941373\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 331 was 78.0%.\n",
      "current params: tensor([-2.2928, -2.2966,  2.2701], dtype=torch.float64)\n",
      "current ratio: -0.9884651132284996\n",
      "tensor(0.7803, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.780303089096219\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.780303089096219\n",
      "t50: 26.823388338527717\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 332 was 78.0%.\n",
      "current params: tensor([-2.2989, -2.3033,  2.2708], dtype=torch.float64)\n",
      "current ratio: -0.9858987443042614\n",
      "tensor(0.7803, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7802836609999417\n",
      "Penalty:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7795787319804841\n",
      "t50: 27.00372204525642\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 333 was 78.5%.\n",
      "current params: tensor([-2.3051, -2.2847,  2.2716], dtype=torch.float64)\n",
      "current ratio: -0.9854630046347491\n",
      "tensor(0.7854, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.785389675689165\n",
      "Penalty:  tensor(0.0025, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7829177514176127\n",
      "t50: 26.581861581765242\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 334 was 78.0%.\n",
      "current params: tensor([-2.2873, -2.2914,  2.2723], dtype=torch.float64)\n",
      "current ratio: -0.9916768434864552\n",
      "tensor(0.7802, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7802469579539235\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7802469579539235\n",
      "t50: 26.706559442149178\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 335 was 78.0%.\n",
      "current params: tensor([-2.2934, -2.2980,  2.2730], dtype=torch.float64)\n",
      "current ratio: -0.9891206507703352\n",
      "tensor(0.7802, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7802294208520434\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7802294208520434\n",
      "t50: 26.885190895767035\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 336 was 78.0%.\n",
      "current params: tensor([-2.2995, -2.3047,  2.2738], dtype=torch.float64)\n",
      "current ratio: -0.9865732608524653\n",
      "tensor(0.7802, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7802119342148913\n",
      "Penalty:  tensor(0.0022, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7780551057429498\n",
      "t50: 27.065577668268944\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 337 was 78.5%.\n",
      "current params: tensor([-2.3056, -2.2862,  2.2746], dtype=torch.float64)\n",
      "current ratio: -0.9865363973974569\n",
      "tensor(0.7853, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7853058159264615\n",
      "Penalty:  tensor(0.0030, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7822994170615133\n",
      "t50: 26.64409345478273\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 338 was 78.0%.\n",
      "current params: tensor([-2.2878, -2.2929,  2.2753], dtype=torch.float64)\n",
      "current ratio: -0.9923281999650685\n",
      "tensor(0.7802, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7801703618543862\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7801703618543862\n",
      "t50: 26.808029013068932\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 339 was 78.0%.\n",
      "current params: tensor([-2.2939, -2.2996,  2.2761], dtype=torch.float64)\n",
      "current ratio: -0.9897904438595069\n",
      "tensor(0.7802, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7801546327730443\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7801546327730443\n",
      "t50: 26.987017774597756\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 340 was 78.0%.\n",
      "current params: tensor([-2.3000, -2.3062,  2.2768], dtype=torch.float64)\n",
      "current ratio: -0.9872613144963807\n",
      "tensor(0.7801, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7801389496148038\n",
      "Penalty:  tensor(0.0036, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7765051709155376\n",
      "t50: 27.16776589079855\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 341 was 78.4%.\n",
      "current params: tensor([-2.3061, -2.2878,  2.2776], dtype=torch.float64)\n",
      "current ratio: -0.9876301811375661\n",
      "tensor(0.7847, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7847196974674201\n",
      "Penalty:  tensor(0.0036, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7811667885285629\n",
      "t50: 26.70758916266878\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 342 was 78.0%.\n",
      "current params: tensor([-2.2884, -2.2945,  2.2784], dtype=torch.float64)\n",
      "current ratio: -0.9929971259570893\n",
      "tensor(0.7801, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7800933596540411\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7800933596540411\n",
      "t50: 26.872321698613614\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 343 was 78.0%.\n",
      "current params: tensor([-2.2945, -2.3011,  2.2792], dtype=torch.float64)\n",
      "current ratio: -0.9904771015842804\n",
      "tensor(0.7801, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7800793066619807\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7800793066619807\n",
      "t50: 27.051446660156373\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 344 was 77.9%.\n",
      "current params: tensor([-2.3006, -2.3077,  2.2800], dtype=torch.float64)\n",
      "current ratio: -0.9879655401018261\n",
      "tensor(0.7796, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7795638766852657\n",
      "Penalty:  tensor(0.0051, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7744336320921984\n",
      "t50: 27.232332893500775\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 345 was 78.4%.\n",
      "current params: tensor([-2.3067, -2.2894,  2.2807], dtype=torch.float64)\n",
      "current ratio: -0.9887444574715507\n",
      "tensor(0.7846, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7846356230271329\n",
      "Penalty:  tensor(0.0041, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.780524053759029\n",
      "t50: 26.77221389703246\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 346 was 78.0%.\n",
      "current params: tensor([-2.2890, -2.2960,  2.2815], dtype=torch.float64)\n",
      "current ratio: -0.9936836777269283\n",
      "tensor(0.7800, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7800158955484815\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7800158955484815\n",
      "t50: 26.937848163559533\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 347 was 77.9%.\n",
      "current params: tensor([-2.2951, -2.3026,  2.2823], dtype=torch.float64)\n",
      "current ratio: -0.9911807757581276\n",
      "tensor(0.7795, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7795019724013724\n",
      "Penalty:  tensor(5.2013e-05, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7794499593406703\n",
      "t50: 27.117147950912894\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 348 was 78.4%.\n",
      "current params: tensor([-2.3012, -2.2843,  2.2831], dtype=torch.float64)\n",
      "current ratio: -0.9921567871041013\n",
      "tensor(0.7846, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7845720190780512\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7845720190780512\n",
      "t50: 26.696436122241206\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 349 was 78.4%.\n",
      "current params: tensor([-2.3073, -2.2909,  2.2839], dtype=torch.float64)\n",
      "current ratio: -0.989875549331908\n",
      "tensor(0.7846, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7845665874484262\n",
      "Penalty:  tensor(0.0047, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.779869813209949\n",
      "t50: 26.874754052846153\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 350 was 77.9%.\n",
      "current params: tensor([-2.2897, -2.2975,  2.2847], dtype=torch.float64)\n",
      "current ratio: -0.9944478232457074\n",
      "tensor(0.7795, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.779451965524718\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.779451965524718\n",
      "t50: 27.003101138811886\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 351 was 77.9%.\n",
      "current params: tensor([-2.2957, -2.3041,  2.2856], dtype=torch.float64)\n",
      "current ratio: -0.99196406899269\n",
      "tensor(0.7794, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7794417039504405\n",
      "Penalty:  tensor(0.0015, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7779604595478686\n",
      "t50: 27.182501637004403\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 352 was 78.4%.\n",
      "current params: tensor([-2.3018, -2.2858,  2.2864], dtype=torch.float64)\n",
      "current ratio: -0.9933011603938278\n",
      "tensor(0.7845, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7845014726154758\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7845014726154758\n",
      "t50: 26.761968457182775\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 353 was 78.4%.\n",
      "current params: tensor([-2.3079, -2.2924,  2.2872], dtype=torch.float64)\n",
      "current ratio: -0.9910277656697942\n",
      "tensor(0.7845, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7844974643716657\n",
      "Penalty:  tensor(0.0053, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7792020876037299\n",
      "t50: 26.940502259435725\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 354 was 77.9%.\n",
      "current params: tensor([-2.2903, -2.2990,  2.2880], dtype=torch.float64)\n",
      "current ratio: -0.9952303239817285\n",
      "tensor(0.7794, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7793891342715894\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7793891342715894\n",
      "t50: 27.06964819813588\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 355 was 77.9%.\n",
      "current params: tensor([-2.2963, -2.3055,  2.2888], dtype=torch.float64)\n",
      "current ratio: -0.9927626492217703\n",
      "tensor(0.7794, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7793803277203157\n",
      "Penalty:  tensor(0.0029, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7764444701720438\n",
      "t50: 27.249276194755772\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 356 was 78.4%.\n",
      "current params: tensor([-2.3024, -2.2874,  2.2897], dtype=torch.float64)\n",
      "current ratio: -0.9944672681129898\n",
      "tensor(0.7844, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7844302599923552\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7844302599923552\n",
      "t50: 26.828864046121463\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 357 was 78.4%.\n",
      "current params: tensor([-2.3085, -2.2939,  2.2905], dtype=torch.float64)\n",
      "current ratio: -0.9922016061934031\n",
      "tensor(0.7844, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7844275746968589\n",
      "Penalty:  tensor(0.0059, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7785198054264447\n",
      "t50: 27.00765084423011\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 358 was 77.9%.\n",
      "current params: tensor([-2.2909, -2.3005,  2.2914], dtype=torch.float64)\n",
      "current ratio: -0.9960288375879748\n",
      "tensor(0.7793, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7793252709748986\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7793252709748986\n",
      "t50: 27.13761467010585\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 359 was 77.9%.\n",
      "current params: tensor([-2.2970, -2.3070,  2.2922], dtype=torch.float64)\n",
      "current ratio: -0.9935767318512018\n",
      "tensor(0.7793, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7793178183436793\n",
      "Penalty:  tensor(0.0044, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7749014293883737\n",
      "t50: 27.317507555466104\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 360 was 78.4%.\n",
      "current params: tensor([-2.3030, -2.2890,  2.2930], dtype=torch.float64)\n",
      "current ratio: -0.99565547515088\n",
      "tensor(0.7844, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7843583272745458\n",
      "Penalty:  tensor(0.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7838986678180921\n",
      "t50: 26.897161238386268\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 361 was 77.9%.\n",
      "current params: tensor([-2.2854, -2.2955,  2.2939], dtype=torch.float64)\n",
      "current ratio: -0.9993092971894464\n",
      "tensor(0.7793, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7792537289187387\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7792537289187387\n",
      "t50: 27.06451030582865\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 362 was 77.9%.\n",
      "current params: tensor([-2.2914, -2.3020,  2.2947], dtype=torch.float64)\n",
      "current ratio: -0.9968643831584064\n",
      "tensor(0.7792, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7792474239239362\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7792474239239362\n",
      "t50: 27.243181556670347\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 363 was 77.9%.\n",
      "current params: tensor([-2.2975, -2.3085,  2.2956], dtype=torch.float64)\n",
      "current ratio: -0.9944272027937038\n",
      "tensor(0.7792, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7792411429373256\n",
      "Penalty:  tensor(0.0059, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7733702162388186\n",
      "t50: 27.423611450681946\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 364 was 78.4%.\n",
      "current params: tensor([-2.3035, -2.2905,  2.2965], dtype=torch.float64)\n",
      "current ratio: -0.9969371578038083\n",
      "tensor(0.7843, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7842727948739119\n",
      "Penalty:  tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7833308152695709\n",
      "t50: 26.963825926236936\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 365 was 77.9%.\n",
      "current params: tensor([-2.2859, -2.2970,  2.2973], dtype=torch.float64)\n",
      "current ratio: -1.0001472019843782\n",
      "tensor(0.7792, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7791734624556426\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7791734624556426\n",
      "t50: 27.13211067972764\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 366 was 77.9%.\n",
      "current params: tensor([-2.2919, -2.3035,  2.2982], dtype=torch.float64)\n",
      "current ratio: -0.9977168954009663\n",
      "tensor(0.7792, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7791682442018386\n",
      "Penalty:  tensor(0.0009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7782779086999942\n",
      "t50: 27.31108586698672\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 367 was 78.4%.\n",
      "current params: tensor([-2.2980, -2.2855,  2.2991], dtype=torch.float64)\n",
      "current ratio: -1.0004875623209726\n",
      "tensor(0.7842, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7841996572610899\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7841996572610899\n",
      "t50: 26.890843674710293\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 368 was 78.3%.\n",
      "current params: tensor([-2.3040, -2.2920,  2.3000], dtype=torch.float64)\n",
      "current ratio: -0.9982394219520497\n",
      "tensor(0.7837, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7836988947752566\n",
      "Penalty:  tensor(0.0014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7822498390456458\n",
      "t50: 27.06902263895511\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 369 was 77.9%.\n",
      "current params: tensor([-2.2865, -2.2984,  2.3009], dtype=torch.float64)\n",
      "current ratio: -1.0010608220198254\n",
      "tensor(0.7791, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7791068669733707\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7791068669733707\n",
      "t50: 27.199625642297196\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 370 was 77.8%.\n",
      "current params: tensor([-2.2925, -2.3049,  2.3018], dtype=torch.float64)\n",
      "current ratio: -0.998644841477049\n",
      "tensor(0.7786, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.778601438632029\n",
      "Penalty:  tensor(0.0023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7763077480040919\n",
      "t50: 27.378915852302157\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 371 was 78.3%.\n",
      "current params: tensor([-2.2985, -2.2870,  2.3027], dtype=torch.float64)\n",
      "current ratio: -1.0018068566042122\n",
      "tensor(0.7836, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7836249972924155\n",
      "Penalty:  tensor(0.0066, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7769797491510585\n",
      "t50: 26.958547989687645\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 372 was 78.3%.\n",
      "current params: tensor([-2.3046, -2.2934,  2.2160], dtype=torch.float64)\n",
      "current ratio: -0.9615853465980652\n",
      "tensor(0.7831, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7830724375064954\n",
      "Penalty:  tensor(0.0020, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7811047145867591\n",
      "t50: 26.194734745279433\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 373 was 77.7%.\n",
      "current params: tensor([-2.2870, -2.2999,  2.2165], dtype=torch.float64)\n",
      "current ratio: -0.9637411407209817\n",
      "tensor(0.7780, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7779709208341492\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7779709208341492\n",
      "t50: 26.31758083456927\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 374 was 77.7%.\n",
      "current params: tensor([-2.2930, -2.3063,  2.2169], dtype=torch.float64)\n",
      "current ratio: -0.96125042825692\n",
      "tensor(0.7780, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7779677849456361\n",
      "Penalty:  tensor(0.0037, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7742598961057807\n",
      "t50: 26.485668962005413\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 375 was 78.2%.\n",
      "current params: tensor([-2.2990, -2.2885,  2.2174], dtype=torch.float64)\n",
      "current ratio: -0.964503187385959\n",
      "tensor(0.7830, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7829814572117201\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7829814572117201\n",
      "t50: 26.074023948696833\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 376 was 78.2%.\n",
      "current params: tensor([-2.3050, -2.2949,  2.2178], dtype=torch.float64)\n",
      "current ratio: -0.9621769155495971\n",
      "tensor(0.7830, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.782984006101563\n",
      "Penalty:  tensor(0.0024, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7805422520486727\n",
      "t50: 26.24128418541027\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 377 was 77.7%.\n",
      "current params: tensor([-2.2875, -2.3013,  2.2183], dtype=torch.float64)\n",
      "current ratio: -0.9639225166068854\n",
      "tensor(0.7779, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7778869586946865\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7778869586946865\n",
      "t50: 26.402835635518954\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 378 was 77.7%.\n",
      "current params: tensor([-2.2935, -2.3077,  2.2188], dtype=torch.float64)\n",
      "current ratio: -0.9614433821417988\n",
      "tensor(0.7779, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.77788477405561\n",
      "Penalty:  tensor(0.0052, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7727337468892714\n",
      "t50: 26.571299988955094\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 379 was 78.2%.\n",
      "current params: tensor([-2.2995, -2.2900,  2.2192], dtype=torch.float64)\n",
      "current ratio: -0.9651008618283721\n",
      "tensor(0.7829, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7828907899075613\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7828907899075613\n",
      "t50: 26.12139803295261\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 380 was 78.2%.\n",
      "current params: tensor([-2.3055, -2.2964,  2.2197], dtype=torch.float64)\n",
      "current ratio: -0.9627798698580384\n",
      "tensor(0.7824, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7823923928951722\n",
      "Penalty:  tensor(0.0029, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7794665769418859\n",
      "t50: 26.288804845541552\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 381 was 77.7%.\n",
      "current params: tensor([-2.2880, -2.3028,  2.2202], dtype=torch.float64)\n",
      "current ratio: -0.9641107259874014\n",
      "tensor(0.7778, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7778020246767181\n",
      "Penalty:  tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7775657685552174\n",
      "t50: 26.450981917519922\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 382 was 78.2%.\n",
      "current params: tensor([-2.2940, -2.2850,  2.2206], dtype=torch.float64)\n",
      "current ratio: -0.9680260782588356\n",
      "tensor(0.7828, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7828085209041616\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7828085209041616\n",
      "t50: 26.039578353989643\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 383 was 78.2%.\n",
      "current params: tensor([-2.3000, -2.2914,  2.2211], dtype=torch.float64)\n",
      "current ratio: -0.9657056014787107\n",
      "tensor(0.7823, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.782310964866449\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.782310964866449\n",
      "t50: 26.205776706852753\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 384 was 78.2%.\n",
      "current params: tensor([-2.3060, -2.2978,  2.2216], dtype=torch.float64)\n",
      "current ratio: -0.9633897582791223\n",
      "tensor(0.7823, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7823158915037536\n",
      "Penalty:  tensor(0.0034, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7788824996750915\n",
      "t50: 26.373497114115363\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 385 was 77.7%.\n",
      "current params: tensor([-2.2886, -2.3042,  2.2221], dtype=torch.float64)\n",
      "current ratio: -0.9643560756927368\n",
      "tensor(0.7777, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7777298028943244\n",
      "Penalty:  tensor(0.0016, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7761040996013295\n",
      "t50: 26.498719945266828\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 386 was 78.2%.\n",
      "current params: tensor([-2.2945, -2.2865,  2.2226], dtype=torch.float64)\n",
      "current ratio: -0.9686372610762409\n",
      "tensor(0.7822, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7822272434957027\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7822272434957027\n",
      "t50: 26.087364104726362\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 387 was 78.2%.\n",
      "current params: tensor([-2.3005, -2.2928,  2.2230], dtype=torch.float64)\n",
      "current ratio: -0.9663218758132998\n",
      "tensor(0.7822, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7822329644995087\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7822329644995087\n",
      "t50: 26.25365669241598\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 388 was 78.2%.\n",
      "current params: tensor([-2.3065, -2.2992,  2.2235], dtype=torch.float64)\n",
      "current ratio: -0.9640111652375976\n",
      "tensor(0.7822, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7822387047096072\n",
      "Penalty:  tensor(0.0040, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7782867900538779\n",
      "t50: 26.421553658807856\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 389 was 77.7%.\n",
      "current params: tensor([-2.2891, -2.3056,  2.2240], dtype=torch.float64)\n",
      "current ratio: -0.9646087293373704\n",
      "tensor(0.7772, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7771550141074758\n",
      "Penalty:  tensor(0.0030, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7741203979474127\n",
      "t50: 26.54740058591064\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 390 was 78.2%.\n",
      "current params: tensor([-2.2951, -2.2880,  2.2245], dtype=torch.float64)\n",
      "current ratio: -0.9692602436951911\n",
      "tensor(0.7821, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7821478227023042\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7821478227023042\n",
      "t50: 26.135977114286366\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 391 was 78.2%.\n",
      "current params: tensor([-2.3011, -2.2943,  2.2250], dtype=torch.float64)\n",
      "current ratio: -0.9669499405180625\n",
      "tensor(0.7822, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7821543029886862\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7821543029886862\n",
      "t50: 26.302467181401404\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 392 was 78.2%.\n",
      "current params: tensor([-2.3071, -2.3007,  2.2255], dtype=torch.float64)\n",
      "current ratio: -0.9646442900093184\n",
      "tensor(0.7822, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7821608011249381\n",
      "Penalty:  tensor(0.0045, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7776791652313665\n",
      "t50: 26.470563996256555\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 393 was 77.7%.\n",
      "current params: tensor([-2.2897, -2.3071,  2.2260], dtype=torch.float64)\n",
      "current ratio: -0.9648688025985266\n",
      "tensor(0.7771, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7770810570968151\n",
      "Penalty:  tensor(0.0045, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7726176871700491\n",
      "t50: 26.5970469894851\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 394 was 78.2%.\n",
      "current params: tensor([-2.2956, -2.2895,  2.2265], dtype=torch.float64)\n",
      "current ratio: -0.969895352505562\n",
      "tensor(0.7821, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7820671843982586\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7820671843982586\n",
      "t50: 26.18560011877908\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 395 was 78.2%.\n",
      "current params: tensor([-2.3016, -2.2958,  2.2270], dtype=torch.float64)\n",
      "current ratio: -0.967590064690029\n",
      "tensor(0.7821, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7820743644683091\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7820743644683091\n",
      "t50: 26.352311511556074\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 396 was 78.2%.\n",
      "current params: tensor([-2.3076, -2.3021,  2.2275], dtype=torch.float64)\n",
      "current ratio: -0.9652894089963032\n",
      "tensor(0.7821, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7820815611317912\n",
      "Penalty:  tensor(0.0050, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7770586769816479\n",
      "t50: 26.520632290301304\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 397 was 77.7%.\n",
      "current params: tensor([-2.2902, -2.3085,  2.2280], dtype=torch.float64)\n",
      "current ratio: -0.9651340290769096\n",
      "tensor(0.7770, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7770056002676158\n",
      "Penalty:  tensor(0.0059, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7710873394285016\n",
      "t50: 26.686336958318357\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 398 was 78.1%.\n",
      "current params: tensor([-2.2962, -2.2910,  2.2285], dtype=torch.float64)\n",
      "current ratio: -0.9705427893844815\n",
      "tensor(0.7820, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7819852828839596\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7819852828839596\n",
      "t50: 26.236260383989684\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 399 was 78.1%.\n",
      "current params: tensor([-2.3022, -2.2973,  2.2291], dtype=torch.float64)\n",
      "current ratio: -0.968242456176208\n",
      "tensor(0.7820, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7819931061955451\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7819931061955451\n",
      "t50: 26.403216561212723\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 400 was 78.1%.\n",
      "current params: tensor([-2.3082, -2.3036,  2.2296], dtype=torch.float64)\n",
      "current ratio: -0.9659467359522378\n",
      "tensor(0.7815, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7814992457815468\n",
      "Penalty:  tensor(0.0066, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7748601165876794\n",
      "t50: 26.571785027950227\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 401 was 78.1%.\n",
      "current params: tensor([-2.2908, -2.2861,  2.2301], dtype=torch.float64)\n",
      "current ratio: -0.9734982702500268\n",
      "tensor(0.7819, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7819072698557394\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7819072698557394\n",
      "t50: 26.158439881455223\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 402 was 78.1%.\n",
      "current params: tensor([-2.2968, -2.2924,  2.2306], dtype=torch.float64)\n",
      "current ratio: -0.9711982929752748\n",
      "tensor(0.7819, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7819156934933983\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7819156934933983\n",
      "t50: 26.32429522118906\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 403 was 78.1%.\n",
      "current params: tensor([-2.3028, -2.2987,  2.2311], dtype=torch.float64)\n",
      "current ratio: -0.9689028772804935\n",
      "tensor(0.7814, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7814224097278734\n",
      "Penalty:  tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7812534598294124\n",
      "t50: 26.49175129741512\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 404 was 77.6%.\n",
      "current params: tensor([-2.2853, -2.3051,  2.2317], dtype=torch.float64)\n",
      "current ratio: -0.968164442592704\n",
      "tensor(0.7768, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.776848333670922\n",
      "Penalty:  tensor(0.0025, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7743804821581743\n",
      "t50: 26.618523001812502\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 405 was 78.1%.\n",
      "current params: tensor([-2.2913, -2.2875,  2.2322], dtype=torch.float64)\n",
      "current ratio: -0.9742282440077075\n",
      "tensor(0.7818, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7818234703292717\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7818234703292717\n",
      "t50: 26.206949605942693\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 406 was 78.1%.\n",
      "current params: tensor([-2.2972, -2.2938,  2.2327], dtype=torch.float64)\n",
      "current ratio: -0.9719331034013277\n",
      "tensor(0.7813, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7813307055956747\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7813307055956747\n",
      "t50: 26.373047684297084\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 407 was 78.1%.\n",
      "current params: tensor([-2.3032, -2.3001,  2.2333], dtype=torch.float64)\n",
      "current ratio: -0.9696424278023849\n",
      "tensor(0.7813, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7813403274685815\n",
      "Penalty:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7807349904378558\n",
      "t50: 26.540668345837286\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 408 was 77.6%.\n",
      "current params: tensor([-2.2858, -2.3064,  2.2338], dtype=torch.float64)\n",
      "current ratio: -0.9685099361634171\n",
      "tensor(0.7768, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7767695055649007\n",
      "Penalty:  tensor(0.0038, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7729218649210547\n",
      "t50: 26.66807771761749\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 409 was 78.1%.\n",
      "current params: tensor([-2.2917, -2.2890,  2.2343], dtype=torch.float64)\n",
      "current ratio: -0.9749725034657315\n",
      "tensor(0.7812, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7812371320317014\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7812371320317014\n",
      "t50: 26.256385325194284\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 410 was 78.1%.\n",
      "current params: tensor([-2.2977, -2.2952,  2.2349], dtype=torch.float64)\n",
      "current ratio: -0.9726820700737602\n",
      "tensor(0.7812, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7812472420330567\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7812472420330567\n",
      "t50: 26.42266717249448\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 411 was 78.1%.\n",
      "current params: tensor([-2.3036, -2.3015,  2.2354], dtype=torch.float64)\n",
      "current ratio: -0.9703961619166777\n",
      "tensor(0.7813, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7812573635478458\n",
      "Penalty:  tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7802062767543716\n",
      "t50: 26.590554942166587\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 412 was 77.6%.\n",
      "current params: tensor([-2.2862, -2.3078,  2.2360], dtype=torch.float64)\n",
      "current ratio: -0.9688645427088566\n",
      "tensor(0.7762, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7761880381995588\n",
      "Penalty:  tensor(0.0052, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7709411355182219\n",
      "t50: 26.71861486568738\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 413 was 78.1%.\n",
      "current params: tensor([-2.2922, -2.2904,  2.2365], dtype=torch.float64)\n",
      "current ratio: -0.9757312106202516\n",
      "tensor(0.7812, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7811523341818817\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7811523341818817\n",
      "t50: 26.306694475130655\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 414 was 78.1%.\n",
      "current params: tensor([-2.2981, -2.2967,  2.2371], dtype=torch.float64)\n",
      "current ratio: -0.9734455177527012\n",
      "tensor(0.7812, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7811629045863769\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7811629045863769\n",
      "t50: 26.47326029732345\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 415 was 78.1%.\n",
      "current params: tensor([-2.3041, -2.3030,  2.2377], dtype=torch.float64)\n",
      "current ratio: -0.9711643345255001\n",
      "tensor(0.7812, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7811734856242467\n",
      "Penalty:  tensor(0.0019, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7792791266403428\n",
      "t50: 26.641435416681954\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 416 was 78.1%.\n",
      "current params: tensor([-2.2867, -2.2856,  2.2382], dtype=torch.float64)\n",
      "current ratio: -0.9787859289979436\n",
      "tensor(0.7811, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7810676933011699\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7810676933011699\n",
      "t50: 26.229153550211684\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 417 was 78.1%.\n",
      "current params: tensor([-2.2927, -2.2918,  2.2388], dtype=torch.float64)\n",
      "current ratio: -0.9765004968050505\n",
      "tensor(0.7811, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7810786859909513\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7810786859909513\n",
      "t50: 26.394659664138405\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 418 was 78.1%.\n",
      "current params: tensor([-2.2986, -2.2981,  2.2393], dtype=torch.float64)\n",
      "current ratio: -0.9742195255819879\n",
      "tensor(0.7811, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7810896882436263\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7810896882436263\n",
      "t50: 26.56176397130572\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 419 was 78.1%.\n",
      "current params: tensor([-2.3046, -2.3043,  2.2399], dtype=torch.float64)\n",
      "current ratio: -0.9719430494864294\n",
      "tensor(0.7811, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7811007002903849\n",
      "Penalty:  tensor(0.0037, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7773736328823444\n",
      "t50: 26.692200933442805\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 420 was 78.0%.\n",
      "current params: tensor([-2.2872, -2.2870,  2.2405], dtype=torch.float64)\n",
      "current ratio: -0.9795654392256827\n",
      "tensor(0.7810, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7809923456834349\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7809923456834349\n",
      "t50: 26.280162985926143\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 421 was 78.1%.\n",
      "current params: tensor([-2.2931, -2.2932,  2.2411], dtype=torch.float64)\n",
      "current ratio: -0.9772634473211664\n",
      "tensor(0.7810, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7810037300976191\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7810037300976191\n",
      "t50: 26.44598257183466\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 422 was 78.1%.\n",
      "current params: tensor([-2.2991, -2.2994,  2.2416], dtype=torch.float64)\n",
      "current ratio: -0.9748596920960734\n",
      "tensor(0.7810, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7810151232993648\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7810151232993648\n",
      "t50: 26.61340407082435\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 423 was 78.0%.\n",
      "current params: tensor([-2.3051, -2.3057,  2.2422], dtype=torch.float64)\n",
      "current ratio: -0.9724623352488542\n",
      "tensor(0.7805, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7805249078629433\n",
      "Penalty:  tensor(0.0056, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7749293156161081\n",
      "t50: 26.782444507862884\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 424 was 78.0%.\n",
      "current params: tensor([-2.2877, -2.2884,  2.2428], dtype=torch.float64)\n",
      "current ratio: -0.9800755279034525\n",
      "tensor(0.7809, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7809162643766357\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7809162643766357\n",
      "t50: 26.33217472693141\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 425 was 78.0%.\n",
      "current params: tensor([-2.2937, -2.2946,  2.2434], dtype=torch.float64)\n",
      "current ratio: -0.9776738408195826\n",
      "tensor(0.7809, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7809280075295082\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7809280075295082\n",
      "t50: 26.498326875425974\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 426 was 78.0%.\n",
      "current params: tensor([-2.2996, -2.3009,  2.2440], dtype=torch.float64)\n",
      "current ratio: -0.9752784855378752\n",
      "tensor(0.7804, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.780438120180218\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.780438120180218\n",
      "t50: 26.66608487774469\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 427 was 78.0%.\n",
      "current params: tensor([-2.3056, -2.3071,  2.2446], dtype=torch.float64)\n",
      "current ratio: -0.9728918230470054\n",
      "tensor(0.7805, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7804505064113111\n",
      "Penalty:  tensor(0.0075, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7729616139209006\n",
      "t50: 26.835383837705596\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 428 was 78.0%.\n",
      "current params: tensor([-2.2883, -2.2898,  2.2452], dtype=torch.float64)\n",
      "current ratio: -0.9804885116720626\n",
      "tensor(0.7808, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7808394652582329\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7808394652582329\n",
      "t50: 26.38520923081121\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 429 was 78.0%.\n",
      "current params: tensor([-2.2942, -2.2961,  2.2458], dtype=torch.float64)\n",
      "current ratio: -0.9780950898440905\n",
      "tensor(0.7803, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7803498695418556\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7803498695418556\n",
      "t50: 26.55171398911579\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 430 was 78.0%.\n",
      "current params: tensor([-2.3001, -2.3023,  2.2464], dtype=torch.float64)\n",
      "current ratio: -0.97571029772019\n",
      "tensor(0.7804, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7803625637326248\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7803625637326248\n",
      "t50: 26.719747393029067\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 431 was 78.0%.\n",
      "current params: tensor([-2.3061, -2.3085,  2.2470], dtype=torch.float64)\n",
      "current ratio: -0.9733317851133149\n",
      "tensor(0.7804, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7803752649565061\n",
      "Penalty:  tensor(0.0094, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7709618229706655\n",
      "t50: 26.889407078786313\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 432 was 78.0%.\n",
      "current params: tensor([-2.2888, -2.2913,  2.2476], dtype=torch.float64)\n",
      "current ratio: -0.9809126286813755\n",
      "tensor(0.7803, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7802602412308175\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7802602412308175\n",
      "t50: 26.43929376143192\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 433 was 78.0%.\n",
      "current params: tensor([-2.2947, -2.2975,  2.2482], dtype=torch.float64)\n",
      "current ratio: -0.9785296458397371\n",
      "tensor(0.7803, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7802732068054412\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7802732068054412\n",
      "t50: 26.60609024074339\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 434 was 78.0%.\n",
      "current params: tensor([-2.3006, -2.3037,  2.2488], dtype=torch.float64)\n",
      "current ratio: -0.9761528818043124\n",
      "tensor(0.7803, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.780286178596615\n",
      "Penalty:  tensor(0.0011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7791362818104322\n",
      "t50: 26.77450027144344\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 435 was 78.5%.\n",
      "current params: tensor([-2.3066, -2.2865,  2.2494], dtype=torch.float64)\n",
      "current ratio: -0.9752083271846465\n",
      "tensor(0.7852, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7852312599513808\n",
      "Penalty:  tensor(0.0040, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.781225523843743\n",
      "t50: 26.370259469836483\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 436 was 78.0%.\n",
      "current params: tensor([-2.2893, -2.2927,  2.2500], dtype=torch.float64)\n",
      "current ratio: -0.981393175448731\n",
      "tensor(0.7802, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7801938859591996\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7801938859591996\n",
      "t50: 26.531378446720748\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 437 was 78.0%.\n",
      "current params: tensor([-2.2953, -2.2989,  2.2507], dtype=torch.float64)\n",
      "current ratio: -0.9790182595312039\n",
      "tensor(0.7802, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7802072041417201\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7802072041417201\n",
      "t50: 26.698794590984626\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 438 was 78.0%.\n",
      "current params: tensor([-2.3012, -2.3051,  2.2513], dtype=torch.float64)\n",
      "current ratio: -0.9766495219408472\n",
      "tensor(0.7802, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7802205278284652\n",
      "Penalty:  tensor(0.0025, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7776985146586047\n",
      "t50: 26.8678311162988\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 439 was 78.5%.\n",
      "current params: tensor([-2.3071, -2.2879,  2.2519], dtype=torch.float64)\n",
      "current ratio: -0.9760595369914556\n",
      "tensor(0.7852, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7851611883179822\n",
      "Penalty:  tensor(0.0046, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7806089688026372\n",
      "t50: 26.462332045894144\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 440 was 78.0%.\n",
      "current params: tensor([-2.2899, -2.2941,  2.2525], dtype=torch.float64)\n",
      "current ratio: -0.9818833164346492\n",
      "tensor(0.7801, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7801263352639589\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7801263352639589\n",
      "t50: 26.586662583406675\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 441 was 78.0%.\n",
      "current params: tensor([-2.2958, -2.3003,  2.2532], dtype=torch.float64)\n",
      "current ratio: -0.9795163361108248\n",
      "tensor(0.7801, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7801399710038476\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7801399710038476\n",
      "t50: 26.754475831023814\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 442 was 78.0%.\n",
      "current params: tensor([-2.3018, -2.3065,  2.2538], dtype=torch.float64)\n",
      "current ratio: -0.977155496314971\n",
      "tensor(0.7802, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7801536115681894\n",
      "Penalty:  tensor(0.0039, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7762336130278176\n",
      "t50: 26.923914178188763\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 443 was 78.4%.\n",
      "current params: tensor([-2.3077, -2.2893,  2.2545], dtype=torch.float64)\n",
      "current ratio: -0.9769267691545458\n",
      "tensor(0.7846, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7845883066484391\n",
      "Penalty:  tensor(0.0051, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7794772150557213\n",
      "t50: 26.518122689139055\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 444 was 78.0%.\n",
      "current params: tensor([-2.2905, -2.2955,  2.2551], dtype=torch.float64)\n",
      "current ratio: -0.9823855006520633\n",
      "tensor(0.7801, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7800581429686565\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7800581429686565\n",
      "t50: 26.64306920988361\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 445 was 78.0%.\n",
      "current params: tensor([-2.2964, -2.3017,  2.2558], dtype=torch.float64)\n",
      "current ratio: -0.9800263115099695\n",
      "tensor(0.7801, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7800720600859204\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7800720600859204\n",
      "t50: 26.81129940633225\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 446 was 78.0%.\n",
      "current params: tensor([-2.3023, -2.3079,  2.2564], dtype=torch.float64)\n",
      "current ratio: -0.9776732278479385\n",
      "tensor(0.7801, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7800859813796056\n",
      "Penalty:  tensor(0.0053, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7747473153704579\n",
      "t50: 26.981159650662274\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 447 was 78.4%.\n",
      "current params: tensor([-2.3083, -2.2908,  2.2571], dtype=torch.float64)\n",
      "current ratio: -0.9778102022583894\n",
      "tensor(0.7845, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7845165303099825\n",
      "Penalty:  tensor(0.0057, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7788339752582365\n",
      "t50: 26.575050439260032\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 448 was 77.9%.\n",
      "current params: tensor([-2.2911, -2.2970,  2.2577], dtype=torch.float64)\n",
      "current ratio: -0.9828975533137709\n",
      "tensor(0.7800, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7799887023428248\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7799887023428248\n",
      "t50: 26.700707735134813\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 449 was 78.0%.\n",
      "current params: tensor([-2.2970, -2.3032,  2.2584], dtype=torch.float64)\n",
      "current ratio: -0.980546045526235\n",
      "tensor(0.7800, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7800028686526855\n",
      "Penalty:  tensor(0.0006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7793970561084946\n",
      "t50: 26.86937480031493\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 450 was 78.4%.\n",
      "current params: tensor([-2.3029, -2.2860,  2.2590], dtype=torch.float64)\n",
      "current ratio: -0.9809502054394329\n",
      "tensor(0.7844, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7844362862414749\n",
      "Penalty:  tensor(0.0003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7841023138965819\n",
      "t50: 26.464072598203643\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 451 was 77.9%.\n",
      "current params: tensor([-2.2856, -2.2922,  2.2597], dtype=torch.float64)\n",
      "current ratio: -0.9858337052456999\n",
      "tensor(0.7799, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7799034620941007\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7799034620941007\n",
      "t50: 26.588259792860033\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 452 was 77.9%.\n",
      "current params: tensor([-2.2915, -2.2984,  2.2604], dtype=torch.float64)\n",
      "current ratio: -0.983483738083469\n",
      "tensor(0.7799, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7799178536642505\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7799178536642505\n",
      "t50: 26.75571801609881\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 453 was 77.9%.\n",
      "current params: tensor([-2.2974, -2.3045,  2.2611], dtype=torch.float64)\n",
      "current ratio: -0.9811397894397377\n",
      "tensor(0.7794, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7794307187500125\n",
      "Penalty:  tensor(0.0019, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7774895464358244\n",
      "t50: 26.92479906476186\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 454 was 78.4%.\n",
      "current params: tensor([-2.3034, -2.2874,  2.2617], dtype=torch.float64)\n",
      "current ratio: -0.9819236882864044\n",
      "tensor(0.7844, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7843623589644821\n",
      "Penalty:  tensor(0.0008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7835698055678842\n",
      "t50: 26.519088349516917\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 455 was 77.9%.\n",
      "current params: tensor([-2.2861, -2.2936,  2.2624], dtype=torch.float64)\n",
      "current ratio: -0.9864260119551101\n",
      "tensor(0.7798, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7798316304436548\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7798316304436548\n",
      "t50: 26.682303570917934\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 456 was 77.9%.\n",
      "current params: tensor([-2.2920, -2.2997,  2.2631], dtype=torch.float64)\n",
      "current ratio: -0.9840835515227943\n",
      "tensor(0.7793, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7793446817679666\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7793446817679666\n",
      "t50: 26.85043305262709\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 457 was 77.9%.\n",
      "current params: tensor([-2.2979, -2.3059,  2.2638], dtype=torch.float64)\n",
      "current ratio: -0.9817493947511546\n",
      "tensor(0.7794, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.779359900957501\n",
      "Penalty:  tensor(0.0033, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7760697894257904\n",
      "t50: 26.981275505353924\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 458 was 78.4%.\n",
      "current params: tensor([-2.3038, -2.2888,  2.2645], dtype=torch.float64)\n",
      "current ratio: -0.9829156629305983\n",
      "tensor(0.7843, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7842877697076668\n",
      "Penalty:  tensor(0.0013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7830262903762003\n",
      "t50: 26.613047605610166\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 459 was 77.9%.\n",
      "current params: tensor([-2.2866, -2.2949,  2.2652], dtype=torch.float64)\n",
      "current ratio: -0.987032369847193\n",
      "tensor(0.7793, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7792574985529486\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7792574985529486\n",
      "t50: 26.739215214840186\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 460 was 77.9%.\n",
      "current params: tensor([-2.2925, -2.3011,  2.2659], dtype=torch.float64)\n",
      "current ratio: -0.9846996212464193\n",
      "tensor(0.7793, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7792728867965155\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7792728867965155\n",
      "t50: 26.907709092824593\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 461 was 77.9%.\n",
      "current params: tensor([-2.2984, -2.3073,  2.2666], dtype=torch.float64)\n",
      "current ratio: -0.9823728046238689\n",
      "tensor(0.7793, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7792882762933534\n",
      "Penalty:  tensor(0.0047, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7746295880703256\n",
      "t50: 27.07783767285708\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 462 was 78.4%.\n",
      "current params: tensor([-2.3043, -2.2902,  2.2673], dtype=torch.float64)\n",
      "current ratio: -0.983926457643571\n",
      "tensor(0.7842, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7842124876363421\n",
      "Penalty:  tensor(0.0017, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.782471494436157\n",
      "t50: 26.670397050622682\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 463 was 77.9%.\n",
      "current params: tensor([-2.2871, -2.2964,  2.2680], dtype=torch.float64)\n",
      "current ratio: -0.9876529590965684\n",
      "tensor(0.7792, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7791841795450186\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7791841795450186\n",
      "t50: 26.797278702277083\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 464 was 77.9%.\n",
      "current params: tensor([-2.2930, -2.3025,  2.2687], dtype=torch.float64)\n",
      "current ratio: -0.9853274999632481\n",
      "tensor(0.7792, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7791997107170965\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7791997107170965\n",
      "t50: 26.966237375298483\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 465 was 77.9%.\n",
      "current params: tensor([-2.2989, -2.3087,  2.2694], dtype=torch.float64)\n",
      "current ratio: -0.9830079472399057\n",
      "tensor(0.7792, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7792152426023923\n",
      "Penalty:  tensor(0.0061, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7731622466319127\n",
      "t50: 27.13683628321174\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 466 was 78.3%.\n",
      "current params: tensor([-2.3048, -2.2916,  2.2701], dtype=torch.float64)\n",
      "current ratio: -0.9849565241578901\n",
      "tensor(0.7836, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7836343144223932\n",
      "Penalty:  tensor(0.0022, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7814029013887551\n",
      "t50: 26.728994771550706\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 467 was 77.9%.\n",
      "current params: tensor([-2.2876, -2.2978,  2.2709], dtype=torch.float64)\n",
      "current ratio: -0.9882879150218142\n",
      "tensor(0.7791, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7791100555512913\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7791100555512913\n",
      "t50: 26.856524321692373\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 468 was 77.9%.\n",
      "current params: tensor([-2.2935, -2.3039,  2.2716], dtype=torch.float64)\n",
      "current ratio: -0.9859696518839507\n",
      "tensor(0.7791, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7791257001653349\n",
      "Penalty:  tensor(0.0013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7777884874198282\n",
      "t50: 27.025966629902005\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 469 was 78.3%.\n",
      "current params: tensor([-2.2994, -2.2868,  2.2723], dtype=torch.float64)\n",
      "current ratio: -0.9882281301256908\n",
      "tensor(0.7835, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7835480401473522\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7835480401473522\n",
      "t50: 26.61886801981519\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 470 was 78.3%.\n",
      "current params: tensor([-2.3053, -2.2930,  2.2731], dtype=torch.float64)\n",
      "current ratio: -0.9860021603593426\n",
      "tensor(0.7836, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7835689210859057\n",
      "Penalty:  tensor(0.0027, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7808233318761996\n",
      "t50: 26.787657505035963\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 471 was 77.9%.\n",
      "current params: tensor([-2.2881, -2.2991,  2.2738], dtype=torch.float64)\n",
      "current ratio: -0.9889809756839337\n",
      "tensor(0.7790, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7790466569449699\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7790466569449699\n",
      "t50: 26.915903984683403\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 472 was 77.9%.\n",
      "current params: tensor([-2.2940, -2.3053,  2.2745], dtype=torch.float64)\n",
      "current ratio: -0.9866699491806783\n",
      "tensor(0.7791, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7790624711433365\n",
      "Penalty:  tensor(0.0027, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7763759132577109\n",
      "t50: 27.085831816174643\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 473 was 78.3%.\n",
      "current params: tensor([-2.2999, -2.2882,  2.2753], dtype=torch.float64)\n",
      "current ratio: -0.9892889700947544\n",
      "tensor(0.7835, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7834813669191045\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7834813669191045\n",
      "t50: 26.67832648160317\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 474 was 78.3%.\n",
      "current params: tensor([-2.3059, -2.2944,  2.2760], dtype=torch.float64)\n",
      "current ratio: -0.9870675718735967\n",
      "tensor(0.7835, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7835023769823821\n",
      "Penalty:  tensor(0.0033, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7802306906133596\n",
      "t50: 26.847610017947094\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 475 was 77.8%.\n",
      "current params: tensor([-2.2887, -2.3005,  2.2768], dtype=torch.float64)\n",
      "current ratio: -0.9896872101409699\n",
      "tensor(0.7790, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7789820416876111\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7789820416876111\n",
      "t50: 27.015484571341208\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 476 was 77.8%.\n",
      "current params: tensor([-2.2946, -2.3067,  2.2776], dtype=torch.float64)\n",
      "current ratio: -0.9873833572357976\n",
      "tensor(0.7785, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.778496570745678\n",
      "Penalty:  tensor(0.0041, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7744350656435742\n",
      "t50: 27.14702183300963\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 477 was 78.3%.\n",
      "current params: tensor([-2.3005, -2.2896,  2.2783], dtype=torch.float64)\n",
      "current ratio: -0.9903697948440817\n",
      "tensor(0.7834, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7834140969675291\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7834140969675291\n",
      "t50: 26.77714542019357\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 478 was 78.3%.\n",
      "current params: tensor([-2.3064, -2.2958,  2.2791], dtype=torch.float64)\n",
      "current ratio: -0.9881529845395347\n",
      "tensor(0.7834, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7834352172144442\n",
      "Penalty:  tensor(0.0038, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7796253236816155\n",
      "t50: 26.908804443557578\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 479 was 77.8%.\n",
      "current params: tensor([-2.2892, -2.3019,  2.2798], dtype=torch.float64)\n",
      "current ratio: -0.9904090896032065\n",
      "tensor(0.7784, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7784153137556082\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7784153137556082\n",
      "t50: 27.07752329690526\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 480 was 77.8%.\n",
      "current params: tensor([-2.2951, -2.3081,  2.2806], dtype=torch.float64)\n",
      "current ratio: -0.9881146648608355\n",
      "tensor(0.7784, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7784320136191688\n",
      "Penalty:  tensor(0.0055, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.77298087003226\n",
      "t50: 27.248644753730726\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 481 was 78.3%.\n",
      "current params: tensor([-2.3010, -2.2911,  2.2814], dtype=torch.float64)\n",
      "current ratio: -0.9914709485261832\n",
      "tensor(0.7833, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7833462509674713\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7833462509674713\n",
      "t50: 26.839172991430626\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 482 was 78.3%.\n",
      "current params: tensor([-2.3070, -2.2972,  2.2822], dtype=torch.float64)\n",
      "current ratio: -0.9892587456063651\n",
      "tensor(0.7834, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7833674549198565\n",
      "Penalty:  tensor(0.0044, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7790069588265275\n",
      "t50: 27.00974471464484\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 483 was 77.8%.\n",
      "current params: tensor([-2.2898, -2.3034,  2.2830], dtype=torch.float64)\n",
      "current ratio: -0.9911469577290422\n",
      "tensor(0.7783, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7783493879728971\n",
      "Penalty:  tensor(0.0008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7775814424633216\n",
      "t50: 27.140847725453604\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 484 was 78.3%.\n",
      "current params: tensor([-2.2957, -2.2863,  2.2837], dtype=torch.float64)\n",
      "current ratio: -0.9948011519835946\n",
      "tensor(0.7833, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7832670186751355\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7832670186751355\n",
      "t50: 26.73208450979817\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 485 was 78.3%.\n",
      "current params: tensor([-2.3016, -2.2924,  2.2845], dtype=torch.float64)\n",
      "current ratio: -0.9925892134105203\n",
      "tensor(0.7833, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7832883713699147\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7832883713699147\n",
      "t50: 26.901537580444636\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 486 was 78.3%.\n",
      "current params: tensor([-2.3075, -2.2986,  2.2853], dtype=torch.float64)\n",
      "current ratio: -0.9903816610469528\n",
      "tensor(0.7833, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7833097177504833\n",
      "Penalty:  tensor(0.0049, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7783731730289084\n",
      "t50: 27.072643889040354\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 487 was 77.8%.\n",
      "current params: tensor([-2.2904, -2.3047,  2.2861], dtype=torch.float64)\n",
      "current ratio: -0.9919418264541917\n",
      "tensor(0.7783, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.778293612827104\n",
      "Penalty:  tensor(0.0021, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7761680361803274\n",
      "t50: 27.20450291130332\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 488 was 78.3%.\n",
      "current params: tensor([-2.2963, -2.2877,  2.2869], dtype=torch.float64)\n",
      "current ratio: -0.9959357351411045\n",
      "tensor(0.7832, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7832080066491497\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7832080066491497\n",
      "t50: 26.795283149152503\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 489 was 78.3%.\n",
      "current params: tensor([-2.3022, -2.2938,  2.2878], dtype=torch.float64)\n",
      "current ratio: -0.9937284727251027\n",
      "tensor(0.7832, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7832294785629533\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7832294785629533\n",
      "t50: 26.96528387937295\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 490 was 78.2%.\n",
      "current params: tensor([-2.3081, -2.3000,  2.2886], dtype=torch.float64)\n",
      "current ratio: -0.9915255959238842\n",
      "tensor(0.7827, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7827495126348132\n",
      "Penalty:  tensor(0.0055, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7772233736045445\n",
      "t50: 27.136944407860252\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 491 was 77.8%.\n",
      "current params: tensor([-2.2910, -2.3061,  2.2894], dtype=torch.float64)\n",
      "current ratio: -0.9927537697178673\n",
      "tensor(0.7782, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7782373306621679\n",
      "Penalty:  tensor(0.0035, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7747338495848536\n",
      "t50: 27.269493401047693\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 492 was 78.3%.\n",
      "current params: tensor([-2.2969, -2.2891,  2.2902], dtype=torch.float64)\n",
      "current ratio: -0.997091509609391\n",
      "tensor(0.7831, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7831485754287296\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7831485754287296\n",
      "t50: 26.859794357046244\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 493 was 78.2%.\n",
      "current params: tensor([-2.3028, -2.2952,  2.2910], dtype=torch.float64)\n",
      "current ratio: -0.9948889502637462\n",
      "tensor(0.7827, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7826686939275551\n",
      "Penalty:  tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7824559950838984\n",
      "t50: 27.03036214567764\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 494 was 77.8%.\n",
      "current params: tensor([-2.2856, -2.3014,  2.2919], dtype=torch.float64)\n",
      "current ratio: -0.9958719779312663\n",
      "tensor(0.7782, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7781510776697859\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7781510776697859\n",
      "t50: 27.16147171153334\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 495 was 77.8%.\n",
      "current params: tensor([-2.2915, -2.3075,  2.2927], dtype=torch.float64)\n",
      "current ratio: -0.9936000070867748\n",
      "tensor(0.7782, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7781681994407675\n",
      "Penalty:  tensor(0.0049, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7733094486706198\n",
      "t50: 27.333132530453558\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 496 was 78.2%.\n",
      "current params: tensor([-2.2973, -2.2905,  2.2935], dtype=torch.float64)\n",
      "current ratio: -0.998333838011438\n",
      "tensor(0.7826, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7825750521813859\n",
      "Penalty:  tensor(0., dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7825750521813859\n",
      "t50: 26.92292665507858\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 497 was 78.2%.\n",
      "current params: tensor([-2.3033, -2.2966,  2.2944], dtype=torch.float64)\n",
      "current ratio: -0.9961358299468306\n",
      "tensor(0.7826, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7825972265610683\n",
      "Penalty:  tensor(0.0007, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7819178651537513\n",
      "t50: 27.09397379215571\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 498 was 77.8%.\n",
      "current params: tensor([-2.2861, -2.3027,  2.2952], dtype=torch.float64)\n",
      "current ratio: -0.9967294759079685\n",
      "tensor(0.7781, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "current yield : 0.7780811315430302\n",
      "Penalty:  tensor(0.0002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "Loss:  -0.7779239902554458\n",
      "t50: 27.265190846190706\n",
      "t85: -1\n",
      "t95: -1\n",
      "t99: -1\n",
      "Using CPU\n",
      "Yield on sim. iteration 499 was 78.2%.\n",
      "optimization complete\n",
      "Final params: tensor([-2.2861, -2.3027,  2.2952], dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<KineticAssembly_AD.vectorized_rxn_net.VectorizedRxnNet at 0x1f354211be0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optim.rn.update_reaction_net(rn)\n",
    "optim.optimize(conc_scale=1e-1,conc_thresh=1e-1,mod_bool=True,mod_factor=10,max_thresh=10,max_yield=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for complete analysis: 1058.0169\n"
     ]
    }
   ],
   "source": [
    "t2 = time_mod.perf_counter()\n",
    "print(\"Time taken for complete analysis: %.4f\" %(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track the yield over optim iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optim.plot_yield()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all the parameter values\n",
    "\n",
    "### This can be stored in a file for later analysis or used to find the best parameter value depending upon a condition. For e.g. the values that give a minimum error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "yields= []\n",
    "final_params=[]\n",
    "\n",
    "final_t50 = []\n",
    "final_t85 = []\n",
    "final_t95 = []\n",
    "final_t99 = []\n",
    "\n",
    "for i in range(len(optim.final_yields)):\n",
    "    yields.append(optim.final_yields[i])\n",
    "    final_params.append(optim.final_solns[i])\n",
    "    \n",
    "    #Storing the different time points it reaches a particular yield threshold\n",
    "    if optim.final_t85[i] == -1:\n",
    "        final_t85.append(1) \n",
    "    else:\n",
    "        final_t85.append(optim.final_t85[i]) \n",
    "    if optim.final_t95[i] == -1:\n",
    "        final_t95.append(1)\n",
    "    else:\n",
    "        final_t95.append(optim.final_t95[i])\n",
    "\n",
    "\n",
    "final_yield_arr = np.array(yields)\n",
    "\n",
    "#final_param_arr = np.array(final_params)\n",
    "param_np_list = [p.detach().cpu().numpy() for p in final_params]\n",
    "final_param_arr = np.stack(param_np_list, axis=0)\n",
    "final_t85 = np.array(final_t85)\n",
    "final_t95 = np.array(final_t95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the ratio of ktri vs kdim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mask_r = final_yield_arr > 0.75\n",
    "\n",
    "#Calculate the ratio\n",
    "ratio = np.exp(final_param_arr[:,1])/np.exp(final_param_arr[:,0])\n",
    "ratio_masked = ratio[mask_r]\n",
    "\n",
    "#Normalize the time scale (t = t*conc*max_rate)\n",
    "conc=vec_rn.initial_copies[0].item()\n",
    "scale_time = final_t95[mask_r]*conc*np.max(final_param_arr[mask_r],axis=1)\n",
    "#Calculate the y_per_time\n",
    "y_per_time = 0.95/scale_time\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(4,3))\n",
    "ax.plot(ratio_masked,y_per_time,linestyle='',marker='o')\n",
    "ax.set_ylabel(\"Efficiency\",fontsize=20)\n",
    "ax.set_xlabel(\"Ratio\",fontsize=20)\n",
    "ax.tick_params(labelsize=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio with maximum efficiency:  3.59785130085109\n",
      "Optimal Rates:  [-0.9285657313030267, -0.9285657313030267, -0.9285657313030267, -0.9285657313030267, 0.35177107509198097, 0.35177107509198097, 0.35177107509198097, 0.35177107509198097, 1.0559972433962088, 0.35177107509198097, 0.35177107509198097, 1.0559972433962088, 0.35177107509198097, 0.35177107509198097, 1.0559972433962088, 1.0559972433962088]\n"
     ]
    }
   ],
   "source": [
    "max_indx = np.argmax(y_per_time)\n",
    "max_ratio = ratio[max_indx]\n",
    "max_rates = final_param_arr[max_indx]\n",
    "print(\"Ratio with maximum efficiency: \",max_ratio)\n",
    "\n",
    "reaction_rates = np.zeros(rn._rxn_count)\n",
    "counter=0\n",
    "for cls,uids in vec_rn.rxn_class.items():\n",
    "    for rid in uids:\n",
    "        reaction_rates[rid]=max_rates[counter]\n",
    "    counter+=1\n",
    "\n",
    "print(\"Optimal Rates: \",list(reaction_rates))\n",
    "\n",
    "#THE RATES BELOW ARE THE ONES TO PASTE INTO KINETIC_SIMULATION NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
