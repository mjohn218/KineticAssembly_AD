{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pressed-interim",
   "metadata": {},
   "source": [
    "## Import necessary modules\n",
    "\n",
    "Every Jupyter Notebook requires the path to the KineticAssembly_AD modules (.py files in the root directory) to be mentioned. This can be done by adding the path to the 'PATH' variable of the system environment. \n",
    "\n",
    "Additonal modules are also imported which are required to run any analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efcef298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure jupyter path is correct for loading local moudules\n",
    "import sys\n",
    "path_to_repo = \"C:\\\\Users\\\\denys\\\\AMGEN\\\\\"\n",
    "#Insert your path here\n",
    "# path_to_repo=\"\"\n",
    "sys.path.append(path_to_repo)\n",
    "\n",
    "\n",
    "import copy\n",
    "from KineticAssembly_AD import ReactionNetwork, VectorizedRxnNet, VecSim    #Import required python modules explicitly\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch import DoubleTensor as Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-phone",
   "metadata": {},
   "source": [
    "## Setup Reaction Network\n",
    "Before we begin to run a simulation, we need to create a Reaction Network that stores all the parameters required to run a simulation and other routines. The Reaction Network can be created by reading an input file. More information on how to create an input file can be found in the User Guide. \n",
    "\n",
    "Here a simple trimer model is used to run a simulation.\n",
    "#### Read the corresponding input file and call the ReactionNetwork class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c1a4dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['default_assoc', 1.0]\n",
      "['rxn_coupling', True]\n",
      "True\n",
      "['monomer_add_only', False]\n",
      "[(0, {'struct': <networkx.classes.graph.Graph object at 0x000001B89B59F898>, 'copies': tensor([100.], dtype=torch.float64), 'subunits': 1}), (1, {'struct': <networkx.classes.graph.Graph object at 0x000001B89ADDF518>, 'copies': tensor([100.], dtype=torch.float64), 'subunits': 1}), (2, {'struct': <networkx.classes.graph.Graph object at 0x000001B8939D7860>, 'copies': tensor([100.], dtype=torch.float64), 'subunits': 1}), (3, {'struct': <networkx.classes.graph.Graph object at 0x000001B8939D7550>, 'copies': tensor([100.], dtype=torch.float64), 'subunits': 1})]\n",
      "New node added - Node index: 4 ; Node label: AM \n",
      "New node added - Node index: 5 ; Node label: AB \n",
      "New node added - Node index: 6 ; Node label: AS \n",
      "New node added - Node index: 7 ; Node label: BM \n",
      "New node added - Node index: 8 ; Node label: MS \n",
      "New node added - Node index: 9 ; Node label: ABM \n",
      "New node added - Node index: 10 ; Node label: AMS \n",
      "New node added - Node index: 11 ; Node label: BS \n",
      "New node added - Node index: 12 ; Node label: ABS \n",
      "New node added - Node index: 13 ; Node label: BMS \n",
      "New node added - Node index: 14 ; Node label: ABMS \n",
      "The number of bonds formed are not compensated by the number of edges\n",
      "This could be possible due to presence of a repeating subunit\n",
      "SOurce1:  2 10\n",
      "The common reactant is:  B\n",
      "Edge added between:  2 14\n",
      "The number of bonds formed are not compensated by the number of edges\n",
      "This could be possible due to presence of a repeating subunit\n",
      "SOurce1:  3 9\n",
      "The common reactant is:  S\n",
      "Edge added between:  3 14\n",
      "The number of bonds formed are not compensated by the number of edges\n",
      "This could be possible due to presence of a repeating subunit\n",
      "SOurce1:  4 11\n",
      "The number of bonds formed are not compensated by the number of edges\n",
      "This could be possible due to presence of a repeating subunit\n",
      "SOurce1:  5 8\n",
      "The number of bonds formed are not compensated by the number of edges\n",
      "This could be possible due to presence of a repeating subunit\n",
      "SOurce1:  6 7\n",
      "The number of bonds formed are not compensated by the number of edges\n",
      "This could be possible due to presence of a repeating subunit\n",
      "SOurce1:  12 1\n",
      "The common reactant is:  M\n",
      "Edge added between:  1 14\n",
      "The number of bonds formed are not compensated by the number of edges\n",
      "This could be possible due to presence of a repeating subunit\n",
      "SOurce1:  13 0\n",
      "The common reactant is:  A\n",
      "Edge added between:  0 14\n",
      "Coupling Reaction ID:  {5: [0, 3], 6: [0, 4], 8: [1, 3], 9: [1, 7], 10: [3, 7], 11: [1, 3, 7], 12: [2, 4], 13: [2, 7], 14: [4, 7], 15: [2, 4, 7], 16: [1, 2, 3, 4], 17: [0, 2, 3, 7], 18: [0, 1, 4, 7], 19: [0, 1], 20: [0, 2], 21: [1, 2], 22: [3, 4], 23: [0, 3, 4], 24: [0, 1, 2]}\n",
      "Reaction Network Completed\n"
     ]
    }
   ],
   "source": [
    "base_input = './tetramer_diversification.pwr'\n",
    "rn = ReactionNetwork(base_input, one_step=True)\n",
    "rn.resolve_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-nevada",
   "metadata": {},
   "source": [
    "## Checking reaction network\n",
    "\n",
    "The ReactionNetwork is a networkx object which creates a graph network with each node as species that can be present in the system according to the binding rules given in the input file. Each node has a unique index number that can be used to access attributes stored for that species. Each edge represents a reaction and is associated with a unique reaction_id, on and off rates and the dG value for that reaction.\n",
    "\n",
    "\n",
    "After creating a Reaction Network we can looping over all network nodes to check if all species are created\n",
    "Creating a dictionary for later reference. This dictionary holds the reactants as keys and values as the reaction index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf4082db",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'reaction_network'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22832\\2423278059.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mreaction_network\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgtostr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Species present in the Reaction Network: \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'reaction_network'"
     ]
    }
   ],
   "source": [
    "uid_dict = {}\n",
    "sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "from reaction_network import gtostr\n",
    "\n",
    "print(\"Species present in the Reaction Network: \")\n",
    "print(\"%3s  %2s  %2s\" %(\"Index\",\"--\",'Species'))\n",
    "\n",
    "nodes_list = []\n",
    "for n in rn.network.nodes():\n",
    "    print(\"%3d  %4s  %-6s\" %(n,\"--\",gtostr(rn.network.nodes[n]['struct'])))\n",
    "    nodes_list.append(gtostr(rn.network.nodes[n]['struct']))\n",
    "    for k,v in rn.network[n].items():\n",
    "        uid = v['uid']\n",
    "        r1 = set(gtostr(rn.network.nodes[n]['struct']))\n",
    "        p = set(gtostr(rn.network.nodes[k]['struct']))\n",
    "        r2 = p-r1\n",
    "        reactants = (r1,r2)\n",
    "        uid_dict[(n,k)] = uid\n",
    "\n",
    "print()\n",
    "print(\"Total Number of Reactions: \",rn._rxn_count)\n",
    "print(\"Total Number of Species: \",len(rn.network.nodes()))\n",
    "        \n",
    "# Dictionary that stores source,destination of an edge and maps it to its unique id\n",
    "#Key : (First Reactant, Product)\n",
    "#Value : (Reaction_id)\n",
    "print()\n",
    "print(uid_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-drive",
   "metadata": {},
   "source": [
    "## Set Initial conditions for the association rates\n",
    "The next step is to define the initial conditions for the simulation. The initial concentrations are specified from the input file. However, the initial value of the association rates can be specified either through the input file \n",
    "\n",
    "From the user_input file, currently the code only allows 1 value to be read (from default_assoc parameter).\n",
    "\n",
    "To set starting rates to different values the next code block takes in a list/array of all rxn rates and updates them in the reaction network object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7c027d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define an empty torch tensor with length equal to number of reactions\n",
    "new_kon = torch.zeros([rn._rxn_count], requires_grad=True).double()\n",
    "\n",
    "#To set individual rates to different values, we need to create an list/array with different values.\n",
    "length = rn._rxn_count\n",
    "min_val = 0.1\n",
    "max_val = 3.0\n",
    "init_val = []\n",
    "\n",
    "for i in range(length):\n",
    "    # Linearly interpolate the current maximum from min_val up to max_val\n",
    "    current_max = min_val + (i / (length - 1)) * (max_val - min_val)\n",
    "    # Draw a random float uniformly between min_val and current_max\n",
    "    val = np.random.uniform(min_val, current_max)\n",
    "    init_val.append(val)\n",
    "\n",
    "#Else we could assign all initial values to be equal to 1; performs bad for lower indeces\n",
    "#init_val = 1\n",
    "\n",
    "\n",
    "new_kon = new_kon + Tensor(init_val)\n",
    "#for init_val = 1\n",
    "#new_kon = new_kon + Tensor([init_val])\n",
    "print(\"Len of new_kon\", len(new_kon))\n",
    "update_kon_dict = {}\n",
    "for edge in rn.network.edges:\n",
    "    update_kon_dict[edge] = new_kon[uid_dict[edge]]\n",
    "\n",
    "nx.set_edge_attributes(rn.network,update_kon_dict,'k_on')\n",
    "for edge in rn.network.edges:\n",
    "    print(rn.network.get_edge_data(edge[0],edge[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-baking",
   "metadata": {},
   "source": [
    "## Define the VectorizedRxnNet class\n",
    "This takes the corresponding rxn network class as input and creates tensor arrays for all parameters values required for running a simulation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2031d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_rn = VectorizedRxnNet(rn, dev='cpu')\n",
    "\n",
    "vec_rn.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-sleeping",
   "metadata": {},
   "source": [
    "## Create the VecSim class to run a simulation\n",
    "Takes the VecRxnNet  object as input. The simulation runtime is set by the 'runtime' argument. (Units - sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-queens",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = 1e4\n",
    "sim = VecSim(vec_rn, runtime, device='cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-gothic",
   "metadata": {},
   "source": [
    "# Run a Simulation\n",
    "### Input Parameters: \n",
    "conc_scale: Controls the conc step at each iteration. Since the numerical integration is not performed over fixed time steps but over fixed conc. steps. For e.g. for a value of 1uM, at each iteration step a total of app. 1uM is reacted (includes all species). Can be run using the default value. A general rule is use conc_scale = 0.01 * Max_yield\n",
    "\n",
    "conc_thresh: This can be used to periodically decrease the conc_scale parameter. After each iteration if the conc_scale is greater than the conc_thresh, then the conc_scale is decreased by mod_factor. Can be run using the default value. \n",
    "\n",
    "mod_bool: This argument is necessary to fix the mass balance criteria. Sometimes if the conc_scale is large, then the simulation can lead to a higher consumption of a particular species which is very low in conc, and create more of this species out of nothing. Default value:True\n",
    "\n",
    "verbose : Print output and progress of simulation\n",
    "\n",
    "yield_species : The species whose yield is to be tracked and returned to the optimzer. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sim.simulate(conc_scale=1e-2,conc_thresh=1e-2,mod_bool=True,verbose=True,yield_species=11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b3cd01-87a2-4872-b702-8a0a06edbcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vec_rn.max_subunits)\n",
    "print(vec_rn.copies_vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-seventh",
   "metadata": {},
   "source": [
    "## Plot the conc. of all species vs time\n",
    "\n",
    "We can plot the concentration vs time data for all species. We can choose which species concentration is to be plotted using the nodes_list variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dcd1a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "#Choose which species needs to be plotted\n",
    "#nodes_list = ['A','B','C', 'D', 'AB','BC','AC','ABCD']\n",
    "\n",
    "sim.plot_observable(nodes_list, ax=ax,legend=False,seed=201,lw=5)\n",
    "ax.set_title(\"runtime: \" + str(runtime) + \" seconds\")\n",
    "handles,labels = ax.get_legend_handles_labels()\n",
    "ax.set_xscale(\"log\")\n",
    "fig.legend(handles,nodes_list,loc='upper center',fancybox=True,ncol=3,fontsize='small',markerscale=1.0)\n",
    "ax.grid(which=\"major\",axis=\"both\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-simpson",
   "metadata": {},
   "source": [
    "### The following code can be used to store the Conc vs Time data for the final complex\n",
    "\n",
    "It can also be modified to store data for all species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-waterproof",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time_interval(time,conc,time_int=0.1):\n",
    "    start_time=time[0]\n",
    "    time_array = []\n",
    "    conc_array = []\n",
    "    for i in range(len(time)):\n",
    "        new_time=time[i]\n",
    "        ts = new_time/start_time\n",
    "        if ts>=time_int:\n",
    "            time_array.append(time[i])\n",
    "            conc_array.append(conc[i])\n",
    "            start_time=new_time\n",
    "    return(time_array,conc_array)\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-nomination",
   "metadata": {},
   "source": [
    "Before storing, the data can be smoothened by selecting only a fixed separation of time intervals to store. Its useful in removing rougher area which can arise due to a high conc_scale parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "time_arr = np.array(sim.steps)\n",
    "complx_conc = np.array(sim.observables[6][1]) # chnage first bracket based on #mer and based on topology\n",
    "\n",
    "sel_time = (time_arr >= 1e-4)\n",
    "sel_indx = np.argwhere(sel_time)[0][0]\n",
    "\n",
    "filter_time,filter_conc = convert_time_interval(time_arr[sel_indx-1:],complx_conc[sel_indx-1:],time_int=1.15)\n",
    "final_time = np.concatenate((time_arr[:sel_indx],filter_time[:]))\n",
    "final_conc = np.concatenate((complx_conc[:sel_indx],filter_conc[:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16415489",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"Conc_Profile_Trimer\",\"w\") as fl:\n",
    "    fl.write(\"#Timestep\\tConc\\n\")\n",
    "    for i in range(len(final_time)):\n",
    "        fl.write(\"%10.9f\\t%10.9f \\n\" %(final_time[i],final_conc[i]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3babb5",
   "metadata": {},
   "source": [
    "### Calculate Trapping Factor\n",
    "\n",
    "Using the cleaned up time and concentration data, we calculate the differential - dC/dln(t) and plot it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a6a530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_slope(time,conc,mode='delta'):\n",
    "        #There are 3 modes to calc slopes\n",
    "        #Mode 1 - \"delta\" : mode which is just ratio of finite differences\n",
    "        #Mode 2 - \"log\" : Gradient calc using numpy gradient function, but time is in logspace\n",
    "        #Mode 3: \"regular\" : Gradient calc with time in normal space\n",
    "\n",
    "        if mode==\"delta\":\n",
    "            slopes=[]\n",
    "            for i in range(len(time)-1):\n",
    "                delta_c = conc[i+1]=conc[i]\n",
    "                delta_t = np.log(time[i+1]-time[i])\n",
    "\n",
    "                s = delta_c/delta_t\n",
    "                slope.append(s)\n",
    "\n",
    "            return(slopes)\n",
    "        elif mode=='log':\n",
    "            l_grad = np.gradient(conc,np.log(time))\n",
    "            return(l_grad)\n",
    "        elif mode == \"regular\":\n",
    "            grad = np.gradient(conc,time)\n",
    "            return(grad)\n",
    "\n",
    "        \n",
    "#Uncleaned version - GRAD1\n",
    "l_grad = calc_slope(final_time,final_conc,mode='log')\n",
    "fig,ax2 = plt.subplots()\n",
    "\n",
    "ax2.plot(final_time,l_grad,linestyle='-',marker='.',alpha=0.6,color='crimson')\n",
    "ax2.set_xscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5c2696",
   "metadata": {},
   "source": [
    "**Isolate the inflection points**\n",
    "\n",
    "I do this by visualizing and handpicking the ranges that cover the two peaks. (I tried automating this but sometimes the numerical instabilities does not give the correct ratios)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a62cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_l_grad = l_grad\n",
    "actual_time = final_time\n",
    "\n",
    "#Finding time points by just visual picking\n",
    "first_peak_mask = actual_time<1\n",
    "first_peak_indx = np.argmax(actual_l_grad[first_peak_mask])\n",
    "first_peak = actual_time[first_peak_mask][first_peak_indx]\n",
    "\n",
    "second_regime_mask = (actual_time>1)\n",
    "second_peak_indx = np.argmax(actual_l_grad[second_regime_mask])\n",
    "second_peak = actual_time[second_regime_mask][second_peak_indx]\n",
    "\n",
    "\n",
    "valley_mask = (actual_time>first_peak) & (actual_time<second_peak)\n",
    "min_grad = np.argmin(actual_l_grad[valley_mask])\n",
    "# third_regime_mask = (actual_time>1e6)\n",
    "# third_peak_indx = np.argmax(actual_l_grad[third_regime_mask])\n",
    "# third_peak = actual_time[third_regime_mask][third_peak_indx]\n",
    "\n",
    "# time_bounds = [first_peak,second_peak,third_peak]\n",
    "time_bounds = [first_peak,second_peak]\n",
    "\n",
    "\n",
    "print('t1 = %.3f s | t2 = %.3f s' %(time_bounds[0],time_bounds[1]))\n",
    "lag_time = np.log(time_bounds[1]/time_bounds[0])\n",
    "\n",
    "print(\"Trapping Factor: \",lag_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280b760b",
   "metadata": {},
   "source": [
    "For higher order N-mers, follow the same routine. In case of multiple peaks, which can happend due to multiple steady-state trapped regimes, the trapping factor is calculate as the ratio between the time_point of the lastpeak to the first peak. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jhu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
