{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Framework using auto-diff to optimize binding rates\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary modules\n",
    "\n",
    "Every Jupyter Notebook requires the path to the KineticAssembly_AD modules (.py files in the root directory) to be mentioned. This can be done by adding the path to the 'PATH' variable of the system environment. \n",
    "\n",
    "Additonal modules are also imported which are required to run any analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# make sure jupyter path is correct for loading local moudules\n",
    "import sys\n",
    "path_to_repo=\"C:\\\\Users\\\\denys\\\\AMGEN\\\\\"   \n",
    "#Insert your path here\n",
    "# path_to_repo=\"\"\n",
    "sys.path.append(path_to_repo)\n",
    "\n",
    "import copy\n",
    "from KineticAssembly_AD import ReactionNetwork, VectorizedRxnNet, VecSim, Optimizer, EquilibriumSolver\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch import DoubleTensor as Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Reaction Network\n",
    "Before we begin to run the optimization routine, we need to create a Reaction Network that stores all the parameters required to run a simulation and other routines. The Reaction Network can be created by reading an input file. More information on how to create an input file can be found in the User Guide. \n",
    "\n",
    "Here a simple trimer model is used to run a simulation.\n",
    "#### Read the corresponding input file and call the ReactionNetwork class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['default_assoc', 1.0]\n",
      "[(0, {'struct': <networkx.classes.graph.Graph object at 0x00000169946356D8>, 'copies': tensor([100.], dtype=torch.float64), 'subunits': 1}), (1, {'struct': <networkx.classes.graph.Graph object at 0x00000169946176A0>, 'copies': tensor([100.], dtype=torch.float64), 'subunits': 1}), (2, {'struct': <networkx.classes.graph.Graph object at 0x000001698C4562E8>, 'copies': tensor([100.], dtype=torch.float64), 'subunits': 1}), (3, {'struct': <networkx.classes.graph.Graph object at 0x000001698C4567F0>, 'copies': tensor([100.], dtype=torch.float64), 'subunits': 1}), (4, {'struct': <networkx.classes.graph.Graph object at 0x000001698C456828>, 'copies': tensor([100.], dtype=torch.float64), 'subunits': 1})]\n",
      "New node added - Node index: 5 ; Node label: AB \n",
      "New node added - Node index: 6 ; Node label: AE \n",
      "New node added - Node index: 7 ; Node label: BC \n",
      "New node added - Node index: 8 ; Node label: ABE \n",
      "New node added - Node index: 9 ; Node label: CD \n",
      "New node added - Node index: 10 ; Node label: ABC \n",
      "New node added - Node index: 11 ; Node label: ABCE \n",
      "New node added - Node index: 12 ; Node label: DE \n",
      "New node added - Node index: 13 ; Node label: ADE \n",
      "New node added - Node index: 14 ; Node label: BCD \n",
      "New node added - Node index: 15 ; Node label: ABDE \n",
      "New node added - Node index: 16 ; Node label: ABCD \n",
      "New node added - Node index: 17 ; Node label: ABCDE \n",
      "New node added - Node index: 18 ; Node label: CDE \n",
      "New node added - Node index: 19 ; Node label: BCDE \n",
      "New node added - Node index: 20 ; Node label: ACDE \n",
      "Reaction Network Completed\n"
     ]
    }
   ],
   "source": [
    "base_input = './fivemer.pwr'\n",
    "rn = ReactionNetwork(base_input, one_step=True)\n",
    "rn.resolve_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking reaction network\n",
    "\n",
    "The ReactionNetwork is a networkx object which creates a graph network with each node as species that can be present in the system according to the binding rules given in the input file. Each node has a unique index number that can be used to access attributes stored for that species. Each edge represents a reaction and is associated with a unique reaction_id, on and off rates and the dG value for that reaction.\n",
    "\n",
    "\n",
    "After creating a Reaction Network we can looping over all network nodes to check if all species are created\n",
    "Creating a dictionary for later reference. This dictionary holds the reactants as keys and values as the reaction index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Species present in the Reaction Network: \n",
      "Index  --  Species\n",
      "  0    --  A     \n",
      "  1    --  B     \n",
      "  2    --  C     \n",
      "  3    --  D     \n",
      "  4    --  E     \n",
      "  5    --  AB    \n",
      "  6    --  AE    \n",
      "  7    --  BC    \n",
      "  8    --  ABE   \n",
      "  9    --  CD    \n",
      " 10    --  ABC   \n",
      " 11    --  ABCE  \n",
      " 12    --  DE    \n",
      " 13    --  ADE   \n",
      " 14    --  BCD   \n",
      " 15    --  ABDE  \n",
      " 16    --  ABCD  \n",
      " 17    --  ABCDE \n",
      " 18    --  CDE   \n",
      " 19    --  BCDE  \n",
      " 20    --  ACDE  \n",
      "\n",
      "Total Number of Reactions:  40\n",
      "Total Number of Species:  21\n",
      "\n",
      "{(0, 5): 0, (0, 6): 1, (0, 10): 24, (0, 13): 30, (0, 16): 34, (0, 20): 36, (0, 17): 38, (1, 5): 0, (1, 7): 2, (1, 8): 3, (1, 14): 28, (1, 15): 32, (1, 19): 37, (1, 17): 39, (2, 7): 2, (2, 9): 4, (2, 10): 5, (2, 11): 6, (2, 18): 31, (2, 20): 33, (2, 17): 35, (3, 9): 4, (3, 12): 7, (3, 13): 8, (3, 14): 9, (3, 15): 10, (3, 16): 11, (3, 17): 12, (4, 6): 1, (4, 12): 7, (4, 8): 13, (4, 18): 14, (4, 11): 15, (4, 19): 16, (4, 17): 17, (5, 10): 5, (5, 8): 13, (5, 16): 18, (5, 15): 19, (5, 17): 20, (6, 8): 3, (6, 13): 8, (6, 11): 21, (6, 20): 22, (6, 17): 23, (7, 14): 9, (7, 11): 21, (7, 10): 24, (7, 19): 25, (7, 17): 26, (8, 11): 6, (8, 15): 10, (8, 17): 27, (9, 18): 14, (9, 16): 18, (9, 20): 22, (9, 17): 27, (9, 14): 28, (10, 16): 11, (10, 11): 15, (10, 17): 29, (11, 17): 12, (12, 15): 19, (12, 19): 25, (12, 17): 29, (12, 13): 30, (12, 18): 31, (13, 17): 26, (13, 15): 32, (13, 20): 33, (14, 19): 16, (14, 17): 23, (14, 16): 34, (15, 17): 35, (16, 17): 17, (18, 17): 20, (18, 20): 36, (18, 19): 37, (19, 17): 38, (20, 17): 39}\n"
     ]
    }
   ],
   "source": [
    "uid_dict = {}\n",
    "sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "from reaction_network import gtostr\n",
    "\n",
    "print(\"Species present in the Reaction Network: \")\n",
    "print(\"%3s  %2s  %2s\" %(\"Index\",\"--\",'Species'))\n",
    "for n in rn.network.nodes():\n",
    "    print(\"%3d  %4s  %-6s\" %(n,\"--\",gtostr(rn.network.nodes[n]['struct'])))\n",
    "    for k,v in rn.network[n].items():\n",
    "        uid = v['uid']\n",
    "        r1 = set(gtostr(rn.network.nodes[n]['struct']))\n",
    "        p = set(gtostr(rn.network.nodes[k]['struct']))\n",
    "        r2 = p-r1\n",
    "        reactants = (r1,r2)\n",
    "        uid_dict[(n,k)] = uid\n",
    "\n",
    "print()\n",
    "print(\"Total Number of Reactions: \",rn._rxn_count)\n",
    "print(\"Total Number of Species: \",len(rn.network.nodes()))\n",
    "        \n",
    "# Dictionary that stores source,destination of an edge and maps it to its unique id\n",
    "#Key : (First Reactant, Product)\n",
    "#Value : (Reaction_id)\n",
    "print()\n",
    "print(uid_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the initial parameter values \n",
    "The next step is to define the initial conditions for the simulation. The initial concentrations are specified from the input file. However, the initial value of the association rates can be specified either through the input file \n",
    "\n",
    "From the user_input file, currently the code only allows 1 value to be read (from default_assoc parameter).\n",
    "\n",
    "To set starting rates to different values the next code block takes in a list/array of all rxn rates and updates them in the reaction network object.\n",
    "\n",
    "For a hetero-trimer there are 6 reaction rates.\n",
    "Also defines the Vectorized Rxn Net class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of new_kon 40\n",
      "{'k_on': tensor(0.1000, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 0}\n",
      "{'k_on': tensor(0.1480, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 1}\n",
      "{'k_on': tensor(0.2366, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 24}\n",
      "{'k_on': tensor(0.5525, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 30}\n",
      "{'k_on': tensor(0.9467, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 34}\n",
      "{'k_on': tensor(0.4287, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 36}\n",
      "{'k_on': tensor(0.6752, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 38}\n",
      "{'k_on': tensor(0.1000, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 0}\n",
      "{'k_on': tensor(0.2191, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 2}\n",
      "{'k_on': tensor(0.2387, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 3}\n",
      "{'k_on': tensor(1.1619, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 28}\n",
      "{'k_on': tensor(1.8908, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 32}\n",
      "{'k_on': tensor(0.2217, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 37}\n",
      "{'k_on': tensor(1.1926, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 39}\n",
      "{'k_on': tensor(0.2191, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 2}\n",
      "{'k_on': tensor(0.1048, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 4}\n",
      "{'k_on': tensor(0.2126, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 5}\n",
      "{'k_on': tensor(0.4376, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 6}\n",
      "{'k_on': tensor(0.5038, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 31}\n",
      "{'k_on': tensor(1.1851, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 33}\n",
      "{'k_on': tensor(1.3388, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 35}\n",
      "{'k_on': tensor(0.1048, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 4}\n",
      "{'k_on': tensor(0.4949, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 7}\n",
      "{'k_on': tensor(0.1790, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 8}\n",
      "{'k_on': tensor(0.5554, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 9}\n",
      "{'k_on': tensor(0.7786, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 10}\n",
      "{'k_on': tensor(0.6515, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 11}\n",
      "{'k_on': tensor(0.8573, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 12}\n",
      "{'k_on': tensor(0.1480, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 1}\n",
      "{'k_on': tensor(0.4949, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 7}\n",
      "{'k_on': tensor(0.4802, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 13}\n",
      "{'k_on': tensor(0.1848, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 14}\n",
      "{'k_on': tensor(1.0872, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 15}\n",
      "{'k_on': tensor(0.9202, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 16}\n",
      "{'k_on': tensor(1.3586, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 17}\n",
      "{'k_on': tensor(0.2126, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 5}\n",
      "{'k_on': tensor(0.4802, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 13}\n",
      "{'k_on': tensor(0.5269, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 18}\n",
      "{'k_on': tensor(0.2052, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 19}\n",
      "{'k_on': tensor(1.5345, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 20}\n",
      "{'k_on': tensor(0.2387, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 3}\n",
      "{'k_on': tensor(0.1790, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 8}\n",
      "{'k_on': tensor(0.6783, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 21}\n",
      "{'k_on': tensor(0.6093, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 22}\n",
      "{'k_on': tensor(1.0876, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 23}\n",
      "{'k_on': tensor(0.5554, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 9}\n",
      "{'k_on': tensor(0.6783, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 21}\n",
      "{'k_on': tensor(0.2366, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 24}\n",
      "{'k_on': tensor(1.0974, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 25}\n",
      "{'k_on': tensor(0.5216, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 26}\n",
      "{'k_on': tensor(0.4376, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 6}\n",
      "{'k_on': tensor(0.7786, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 10}\n",
      "{'k_on': tensor(1.7048, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 27}\n",
      "{'k_on': tensor(0.1848, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 14}\n",
      "{'k_on': tensor(0.5269, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 18}\n",
      "{'k_on': tensor(0.6093, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 22}\n",
      "{'k_on': tensor(1.7048, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 27}\n",
      "{'k_on': tensor(1.1619, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 28}\n",
      "{'k_on': tensor(0.6515, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 11}\n",
      "{'k_on': tensor(1.0872, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 15}\n",
      "{'k_on': tensor(0.9229, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 29}\n",
      "{'k_on': tensor(0.8573, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 12}\n",
      "{'k_on': tensor(0.2052, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 19}\n",
      "{'k_on': tensor(1.0974, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 25}\n",
      "{'k_on': tensor(0.9229, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 29}\n",
      "{'k_on': tensor(0.5525, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 30}\n",
      "{'k_on': tensor(0.5038, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 31}\n",
      "{'k_on': tensor(0.5216, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 26}\n",
      "{'k_on': tensor(1.8908, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 32}\n",
      "{'k_on': tensor(1.1851, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 33}\n",
      "{'k_on': tensor(0.9202, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 16}\n",
      "{'k_on': tensor(1.0876, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 23}\n",
      "{'k_on': tensor(0.9467, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 34}\n",
      "{'k_on': tensor(1.3388, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 35}\n",
      "{'k_on': tensor(1.3586, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 17}\n",
      "{'k_on': tensor(1.5345, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 20}\n",
      "{'k_on': tensor(0.4287, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 36}\n",
      "{'k_on': tensor(0.2217, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-20.], dtype=torch.float64), 'uid': 37}\n",
      "{'k_on': tensor(0.6752, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 38}\n",
      "{'k_on': tensor(1.1926, dtype=torch.float64, grad_fn=<SelectBackward>), 'k_off': None, 'lcf': 1, 'rxn_score': tensor([-40.], dtype=torch.float64), 'uid': 39}\n",
      "Reaction rates:  tensor([0.1000, 0.1480, 0.2191, 0.2387, 0.1048, 0.2126, 0.4376, 0.4949, 0.1790,\n",
      "        0.5554, 0.7786, 0.6515, 0.8573, 0.4802, 0.1848, 1.0872, 0.9202, 1.3586,\n",
      "        0.5269, 0.2052, 1.5345, 0.6783, 0.6093, 1.0876, 0.2366, 1.0974, 0.5216,\n",
      "        1.7048, 1.1619, 0.9229, 0.5525, 0.5038, 1.8908, 1.1851, 0.9467, 1.3388,\n",
      "        0.4287, 0.2217, 0.6752, 1.1926], dtype=torch.float64,\n",
      "       grad_fn=<CopySlices>)\n",
      "dGs:  tensor([-20., -20., -20., -20., -20., -20., -20., -20., -20., -20., -20., -20.,\n",
      "        -40., -20., -20., -20., -20., -40., -20., -20., -40., -20., -20., -40.,\n",
      "        -20., -20., -40., -40., -20., -40., -20., -20., -20., -20., -20., -40.,\n",
      "        -20., -20., -40., -40.], dtype=torch.float64)\n",
      "Species Concentrations:  tensor([100., 100., 100., 100., 100.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "       dtype=torch.float64)\n",
      "Shifting to device:  cpu\n",
      "Parameter containing:\n",
      "tensor([0.1000, 0.1480, 0.2191, 0.2387, 0.1048, 0.2126, 0.4376, 0.4949, 0.1790,\n",
      "        0.5554, 0.7786, 0.6515, 0.8573, 0.4802, 0.1848, 1.0872, 0.9202, 1.3586,\n",
      "        0.5269, 0.2052, 1.5345, 0.6783, 0.6093, 1.0876, 0.2366, 1.0974, 0.5216,\n",
      "        1.7048, 1.1619, 0.9229, 0.5525, 0.5038, 1.8908, 1.1851, 0.9467, 1.3388,\n",
      "        0.4287, 0.2217, 0.6752, 1.1926], dtype=torch.float64,\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#Define an empty torch tensor with length equal to number of reactions\n",
    "new_kon = torch.zeros([rn._rxn_count], requires_grad=True).double()\n",
    "\n",
    "#Set the initial value of the association rates\n",
    "#Note that this code sets all association rates at the same value\n",
    "\n",
    "#To set individual rates to different values, we need to create an list/array with different values.\n",
    "length = rn._rxn_count\n",
    "min_val = 0.1\n",
    "max_val = 3.0\n",
    "init_val = []\n",
    "\n",
    "for i in range(length):\n",
    "    # Linearly interpolate the current maximum from min_val up to max_val\n",
    "    current_max = min_val + (i / (length - 1)) * (max_val - min_val)\n",
    "    # Draw a random float uniformly between min_val and current_max\n",
    "    val = np.random.uniform(min_val, current_max)\n",
    "    init_val.append(val)\n",
    "\n",
    "#Else we could assign all initial values to be equal to 1; performs bad for lower indeces\n",
    "#init_val = 1\n",
    "\n",
    "\n",
    "new_kon = new_kon + Tensor(init_val)\n",
    "#for init_val = 1\n",
    "#new_kon = new_kon + Tensor([init_val])\n",
    "print(\"Len of new_kon\", len(new_kon))\n",
    "update_kon_dict = {}\n",
    "for edge in rn.network.edges:\n",
    "    update_kon_dict[edge] = new_kon[uid_dict[edge]]\n",
    "\n",
    "nx.set_edge_attributes(rn.network,update_kon_dict,'k_on')\n",
    "for edge in rn.network.edges:\n",
    "    print(rn.network.get_edge_data(edge[0],edge[1]))\n",
    "\n",
    "vec_rn = VectorizedRxnNet(rn, dev='cpu')\n",
    "print(vec_rn.kon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Species present in the Reaction Network: \n",
      "Index  --  Species\n",
      "  0    --  A     \n",
      "  1    --  B     \n",
      "  2    --  C     \n",
      "  3    --  D     \n",
      "  4    --  E     \n",
      "  5    --  AB    \n",
      "  6    --  AE    \n",
      "  7    --  BC    \n",
      "  8    --  ABE   \n",
      "  9    --  CD    \n",
      " 10    --  ABC   \n",
      " 11    --  ABCE  \n",
      " 12    --  DE    \n",
      " 13    --  ADE   \n",
      " 14    --  BCD   \n",
      " 15    --  ABDE  \n",
      " 16    --  ABCD  \n",
      " 17    --  ABCDE \n",
      " 18    --  CDE   \n",
      " 19    --  BCDE  \n",
      " 20    --  ACDE  \n",
      "\n",
      "Initial Binding Rates: \n",
      "Reaction        Id           kon\n",
      "\n",
      "A   +   B       0 \t    0.10\n",
      "C   +   D       4 \t    0.10\n",
      "A   +   E       1 \t    0.15\n",
      "D   +  AE       8 \t    0.18\n",
      "E   +  DC      14 \t    0.18\n",
      "AB  +  ED      19 \t    0.21\n",
      "C   +  AB       5 \t    0.21\n",
      "B   +   C       2 \t    0.22\n",
      "B   + EDC      37 \t    0.22\n",
      "A   +  BC      24 \t    0.24\n",
      "B   +  AE       3 \t    0.24\n",
      "A   + EDC      36 \t    0.43\n",
      "C   + AEB       6 \t    0.44\n",
      "E   +  AB      13 \t    0.48\n",
      "D   +   E       7 \t    0.49\n",
      "C   +  ED      31 \t    0.50\n",
      "BC  + AED      26 \t    0.52\n",
      "AB  +  DC      18 \t    0.53\n",
      "A   +  ED      30 \t    0.55\n",
      "D   +  BC       9 \t    0.56\n",
      "AE  +  DC      22 \t    0.61\n",
      "D   + ABC      11 \t    0.65\n",
      "A   +EBDC      38 \t    0.68\n",
      "AE  +  BC      21 \t    0.68\n",
      "D   + AEB      10 \t    0.78\n",
      "D   +AEBC      12 \t    0.86\n",
      "E   + BDC      16 \t    0.92\n",
      "ABC +  ED      29 \t    0.92\n",
      "A   + BDC      34 \t    0.95\n",
      "E   + ABC      15 \t    1.09\n",
      "AE  + BDC      23 \t    1.09\n",
      "BC  +  ED      25 \t    1.10\n",
      "B   +  DC      28 \t    1.16\n",
      "C   + AED      33 \t    1.19\n",
      "B   +AEDC      39 \t    1.19\n",
      "C   +AEBD      35 \t    1.34\n",
      "E   +ABDC      17 \t    1.36\n",
      "AB  + EDC      20 \t    1.53\n",
      "AEB +  DC      27 \t    1.70\n",
      "B   + AED      32 \t    1.89\n"
     ]
    }
   ],
   "source": [
    "uid_dict = {}\n",
    "sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "from reaction_network import gtostr\n",
    "\n",
    "print(\"Species present in the Reaction Network: \")\n",
    "print(\"%3s  %2s  %2s\" %(\"Index\",\"--\",'Species'))\n",
    "\n",
    "for n in rn.network.nodes():\n",
    "    #print(n)\n",
    "    #print(rn.network.nodes()[n])\n",
    "    print(\"%3d  %4s  %-6s\" %(n,\"--\",gtostr(rn.network.nodes[n]['struct'])))\n",
    "    for k,v in rn.network[n].items():\n",
    "        uid = v['uid']\n",
    "        r1 = set(gtostr(rn.network.nodes[n]['struct']))\n",
    "        p = set(gtostr(rn.network.nodes[k]['struct']))\n",
    "        r2 = p-r1\n",
    "        reactants = (\"\".join(list(r1)),\"\".join(list(r2)))\n",
    "        uid_val = {'reactants':reactants,'kon':v['k_on'],'score':v['rxn_score'],'koff':v['k_off'],'uid':uid}\n",
    "        if uid not in uid_dict.keys():\n",
    "            uid_dict[uid] = uid_val\n",
    "\n",
    "print()\n",
    "print(\"Initial Binding Rates: \")\n",
    "\n",
    "ind_sort = np.argsort(vec_rn.kon.detach().numpy())\n",
    "print(\"%-16s%-3s %12s\" %(\"Reaction\",\"Id\",\"kon\"))\n",
    "print()\n",
    "for i in ind_sort:\n",
    "    print(\"%-4s%1s%4s %7d \\t%8.2f\" %(uid_dict[i]['reactants'][0],\"+\",uid_dict[i]['reactants'][1],uid_dict[i]['uid'],vec_rn.kon[i].item()))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Using the optimizer ##\n",
    "\n",
    "### Define an instance of the optimizer class\n",
    "#### Input Arguments:\n",
    "\n",
    "reaction_network : Input the vectorized rxn network\n",
    "\n",
    "sim_runtime: The runtime of the kinetic simulation. Needs to be same as the time over the experimental reaction data.\n",
    "\n",
    "optim_iterations: No. of iterations to run the optimization. Can start at low values(100) and increase depending upon memory usage.\n",
    "\n",
    "learning_rate = The size of the gradient descent step for updating parameter values. Needs to be atleast (1e-3-1e-1)* min{parameter value}. If learning rate is too high, it can take a longer step and sometimes lead to negative value of parameters which is unphysical. Requires some trial runs to find the best value. \n",
    "\n",
    "device: cpu or gpu\n",
    "\n",
    "method: Choose which pytorch based optimized to use for gradient descent - Adam or RMSprop\n",
    "\n",
    "mom: Only for RMSprop method. Use momentum term during gradient descent. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.utils.benchmark as benchmark\n",
    "import time as time_mod\n",
    "\n",
    "t1 = time_mod.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vec_rn.reset(reset_params=True)\n",
    "optim = Optimizer(reaction_network=vec_rn,\n",
    "                  sim_runtime=1,\n",
    "                  optim_iterations=100,\n",
    "                  learning_rate=1e-2,\n",
    "                  device='cpu',method=\"RMSprop\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the optimization method\n",
    "\n",
    "#### Input arguments\n",
    "\n",
    "conc_scale: Controls the conc step at each iteration. Since the numerical integration is not performed over fixed time steps but over fixed conc. steps. For e.g. for a value of 1uM, at each iteration step a total of app. 1uM is reacted (includes all species). Can be run using the default value. A general rule is use conc_scale = 0.01 * Max_yield\n",
    "\n",
    "conc_thresh: This can be used to periodically decrease the conc_scale parameter. After each iteration if the conc_scale is greater than the conc_thresh, then the conc_scale is decreased by mod_factor. Can be run using the default value. \n",
    "\n",
    "mod_bool: This argument is necessary to fix the mass balance criteria. Sometimes if the conc_scale is large, then the simulation can lead to a higher consumption of a particular species which is very low in conc, and create more of this species out of nothing. Default value:True\n",
    "\n",
    "max_thresh: Max. allowed values of parameters being updated. Beyond this maximum a penalty is imposed on the cost function. (Regularization)\n",
    "\n",
    "max_yield: It is a control variable that is used to store the updated parameter values over all iterations for further analysis. The parameter values are stored only if the current yield exceed this max_yield. \n",
    "\n",
    "yield_species: Yield of the species being optimized(node index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reaction Parameters before optimization: \n",
      "[Parameter containing:\n",
      "tensor([0.1000, 0.1480, 0.2191, 0.2387, 0.1048, 0.2126, 0.4376, 0.4949, 0.1790,\n",
      "        0.5554, 0.7786, 0.6515, 0.8573, 0.4802, 0.1848, 1.0872, 0.9202, 1.3586,\n",
      "        0.5269, 0.2052, 1.5345, 0.6783, 0.6093, 1.0876, 0.2366, 1.0974, 0.5216,\n",
      "        1.7048, 1.1619, 0.9229, 0.5525, 0.5038, 1.8908, 1.1851, 0.9467, 1.3388,\n",
      "        0.4287, 0.2217, 0.6752, 1.1926], dtype=torch.float64,\n",
      "       requires_grad=True)]\n",
      "Optimizer State: <bound method Optimizer.state_dict of RMSprop (\n",
      "Parameter Group 0\n",
      "    alpha: 0.99\n",
      "    centered: False\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    momentum: 0\n",
      "    weight_decay: 0\n",
      ")>\n",
      "Using CPU\n",
      "Yield on sim. iteration 0 was 82.7%.\n",
      "current params: tensor([0.1000, 0.1480, 0.2191, 0.2387, 0.1048, 0.2126, 0.4376, 0.4949, 0.1790,\n",
      "        0.5554, 0.7786, 0.6515, 0.8573, 0.4802, 0.1848, 1.0872, 0.9202, 1.3586,\n",
      "        0.5269, 0.2052, 1.5345, 0.6783, 0.6093, 1.0876, 0.2366, 1.0974, 0.5216,\n",
      "        1.7048, 1.1619, 0.9229, 0.5525, 0.5038, 1.8908, 1.1851, 0.9467, 1.3388,\n",
      "        0.4287, 0.2217, 0.6752, 1.1926], dtype=torch.float64)\n",
      "tensor(0.8273, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 1 was 94.3%.\n",
      "current params: tensor([5.4461e-08, 4.7982e-02, 1.1906e-01, 1.3874e-01, 4.7954e-03, 1.1255e-01,\n",
      "        3.3761e-01, 3.9487e-01, 7.9031e-02, 4.5544e-01, 6.7860e-01, 7.5147e-01,\n",
      "        9.5726e-01, 3.8016e-01, 2.8479e-01, 1.1872e+00, 1.0202e+00, 1.4586e+00,\n",
      "        6.2694e-01, 3.0524e-01, 1.6345e+00, 7.7828e-01, 7.0929e-01, 1.1876e+00,\n",
      "        1.3665e-01, 1.1974e+00, 6.2160e-01, 1.8048e+00, 1.2619e+00, 1.0229e+00,\n",
      "        4.5247e-01, 4.0384e-01, 1.9908e+00, 1.2851e+00, 8.4672e-01, 1.4388e+00,\n",
      "        3.2867e-01, 1.2168e-01, 7.7519e-01, 1.2926e+00], dtype=torch.float64)\n",
      "tensor(0.9435, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 2 was 92.3%.\n",
      "current params: tensor([0.1000, 0.1480, 0.0233, 0.0483, 0.1048, 0.1978, 0.4330, 0.3006, 0.1790,\n",
      "        0.3637, 0.7780, 0.8411, 1.0200, 0.4801, 0.3725, 1.2651, 1.1050, 1.5228,\n",
      "        0.7036, 0.4011, 1.6345, 0.8780, 0.7896, 1.2202, 0.0436, 1.0981, 0.6828,\n",
      "        1.8064, 1.3575, 1.0645, 0.3589, 0.3114, 1.8952, 1.2457, 0.7961, 1.5169,\n",
      "        0.2318, 0.0236, 0.8545, 1.3787], dtype=torch.float64)\n",
      "tensor(0.9240, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 3 was 92.8%.\n",
      "current params: tensor([0.1716, 0.1459, 0.1231, 0.1483, 0.1008, 0.1074, 0.3492, 0.2449, 0.1778,\n",
      "        0.3732, 0.7583, 0.9073, 1.0772, 0.4333, 0.4085, 1.3318, 1.1655, 1.5773,\n",
      "        0.7581, 0.4060, 1.7206, 0.9703, 0.8325, 1.2968, 0.1436, 1.1949, 0.6939,\n",
      "        1.8639, 1.3246, 1.1068, 0.3034, 0.2359, 1.8403, 1.2025, 0.7493, 1.5787,\n",
      "        0.1740, 0.1235, 0.8862, 1.4301], dtype=torch.float64)\n",
      "tensor(0.9286, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 4 was 93.9%.\n",
      "current params: tensor([0.1697, 0.1440, 0.1199, 0.1476, 0.0975, 0.0248, 0.2594, 0.1939, 0.1771,\n",
      "        0.3276, 0.6677, 0.9599, 1.1395, 0.3495, 0.4371, 1.3842, 1.1907, 1.6275,\n",
      "        0.7996, 0.3547, 1.7800, 0.9950, 0.8778, 1.3479, 0.1433, 1.2090, 0.7273,\n",
      "        1.9360, 1.3341, 1.1474, 0.2637, 0.1857, 1.8961, 1.2558, 0.7015, 1.6249,\n",
      "        0.1568, 0.1233, 0.9310, 1.4563], dtype=torch.float64)\n",
      "tensor(0.9392, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 5 was 93.8%.\n",
      "current params: tensor([0.1678, 0.1418, 0.1163, 0.1470, 0.1686, 0.1248, 0.1771, 0.1387, 0.1762,\n",
      "        0.2763, 0.5839, 1.0189, 1.1898, 0.2665, 0.4556, 1.4362, 1.1970, 1.6712,\n",
      "        0.8321, 0.2825, 1.8321, 1.0144, 0.9179, 1.3937, 0.1430, 1.2172, 0.7566,\n",
      "        2.0009, 1.3401, 1.1690, 0.2265, 0.1345, 1.9572, 1.3017, 0.6634, 1.6671,\n",
      "        0.1469, 0.1231, 0.9730, 1.4805], dtype=torch.float64)\n",
      "tensor(0.9381, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 6 was 94.3%.\n",
      "current params: tensor([0.1659, 0.1394, 0.1131, 0.1464, 0.1660, 0.1237, 0.1227, 0.0886, 0.1755,\n",
      "        0.2334, 0.5273, 1.0558, 1.2328, 0.2035, 0.4322, 1.4736, 1.1558, 1.7222,\n",
      "        0.8556, 0.2374, 1.8730, 1.0391, 0.9475, 1.4409, 0.1427, 1.2373, 0.7800,\n",
      "        2.0682, 1.3026, 1.2028, 0.1949, 0.0972, 2.0189, 1.3852, 0.6024, 1.6965,\n",
      "        0.1377, 0.1230, 1.0142, 1.5021], dtype=torch.float64)\n",
      "tensor(0.9439, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 7 was 94.3%.\n",
      "current params: tensor([0.1639, 0.1364, 0.1098, 0.1457, 0.1630, 0.1224, 0.0720, 0.1885, 0.1746,\n",
      "        0.1828, 0.4692, 1.0866, 1.2746, 0.1360, 0.3864, 1.5053, 1.0942, 1.7702,\n",
      "        0.8710, 0.1992, 1.9086, 1.0537, 0.9734, 1.4845, 0.1423, 1.2678, 0.8006,\n",
      "        2.1277, 1.2578, 1.2297, 0.1710, 0.1971, 2.0780, 1.4590, 0.5493, 1.7222,\n",
      "        0.1324, 0.1229, 1.0523, 1.5225], dtype=torch.float64)\n",
      "tensor(0.9431, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 8 was 94.5%.\n",
      "current params: tensor([0.1618, 0.1339, 0.1063, 0.1451, 0.1604, 0.1210, 0.1720, 0.1860, 0.1739,\n",
      "        0.1495, 0.4323, 1.1202, 1.3057, 0.0872, 0.3667, 1.5386, 1.0668, 1.8081,\n",
      "        0.8902, 0.1368, 1.9545, 1.0837, 1.0026, 1.5214, 0.1419, 1.2610, 0.8197,\n",
      "        2.1700, 1.2220, 1.2686, 0.1306, 0.1959, 2.1267, 1.5182, 0.5012, 1.7452,\n",
      "        0.1181, 0.1227, 1.0875, 1.5435], dtype=torch.float64)\n",
      "tensor(0.9457, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 9 was 94.5%.\n",
      "current params: tensor([0.1595, 0.1311, 0.1026, 0.1445, 0.1576, 0.1195, 0.1718, 0.1835, 0.1732,\n",
      "        0.1179, 0.4140, 1.1519, 1.3381, 0.1872, 0.3421, 1.5694, 1.0363, 1.8438,\n",
      "        0.9048, 0.0704, 1.9993, 1.1152, 1.0302, 1.5558, 0.1416, 1.2427, 0.8360,\n",
      "        2.2019, 1.1833, 1.3085, 0.0876, 0.1944, 2.1736, 1.5756, 0.4565, 1.7640,\n",
      "        0.1038, 0.1225, 1.1210, 1.5634], dtype=torch.float64)\n",
      "tensor(0.9450, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 10 was 95.1%.\n",
      "current params: tensor([0.1570, 0.1283, 0.0986, 0.1438, 0.1549, 0.1181, 0.1714, 0.1810, 0.1725,\n",
      "        0.0876, 0.3759, 1.1849, 1.3696, 0.1865, 0.3220, 1.6012, 1.0119, 1.8751,\n",
      "        0.9239, 0.1704, 2.0399, 1.1489, 1.0584, 1.5881, 0.1412, 1.2111, 0.8492,\n",
      "        2.2436, 1.1442, 1.3464, 0.1876, 0.1926, 2.2198, 1.6332, 0.4081, 1.7813,\n",
      "        0.0846, 0.1222, 1.1529, 1.5814], dtype=torch.float64)\n",
      "tensor(0.9516, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 11 was 94.8%.\n",
      "current params: tensor([0.1546, 0.1254, 0.1714, 0.1432, 0.1520, 0.1168, 0.1711, 0.1783, 0.1718,\n",
      "        0.1876, 0.3368, 1.2170, 1.3998, 0.1858, 0.2950, 1.6316, 0.9803, 1.9036,\n",
      "        0.9435, 0.1701, 2.0735, 1.1767, 1.0858, 1.6174, 0.1408, 1.1945, 0.8678,\n",
      "        2.2830, 1.1059, 1.3743, 0.1865, 0.1913, 2.2559, 1.6731, 0.3669, 1.8022,\n",
      "        0.1846, 0.1219, 1.1809, 1.6015], dtype=torch.float64)\n",
      "tensor(0.9487, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 12 was 95.0%.\n",
      "current params: tensor([0.1526, 0.1224, 0.1694, 0.1428, 0.1495, 0.1157, 0.1708, 0.1756, 0.1713,\n",
      "        0.1870, 0.3017, 1.2472, 1.4315, 0.1850, 0.2670, 1.6598, 0.9293, 1.9310,\n",
      "        0.9628, 0.1699, 2.0986, 1.1866, 1.1148, 1.6457, 0.1401, 1.1485, 0.8925,\n",
      "        2.3181, 1.0829, 1.3999, 0.1855, 0.1903, 2.2943, 1.7152, 0.3181, 1.8195,\n",
      "        0.1844, 0.1218, 1.2103, 1.6202], dtype=torch.float64)\n",
      "tensor(0.9502, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 13 was 95.0%.\n",
      "current params: tensor([0.1504, 0.1193, 0.1674, 0.1424, 0.1470, 0.1145, 0.1705, 0.1727, 0.1707,\n",
      "        0.1864, 0.2659, 1.2767, 1.4616, 0.1843, 0.2373, 1.6871, 0.8754, 1.9569,\n",
      "        0.9817, 0.1696, 2.1225, 1.1987, 1.1431, 1.6737, 0.1394, 1.1031, 0.9164,\n",
      "        2.3525, 1.0590, 1.4245, 0.1845, 0.1892, 2.3307, 1.7548, 0.2685, 1.8363,\n",
      "        0.1843, 0.1217, 1.2382, 1.6387], dtype=torch.float64)\n",
      "tensor(0.9508, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 14 was 95.1%.\n",
      "current params: tensor([0.1483, 0.1160, 0.1653, 0.1420, 0.1443, 0.1134, 0.1701, 0.1697, 0.1702,\n",
      "        0.1857, 0.2294, 1.3056, 1.4904, 0.1834, 0.2058, 1.7135, 0.8196, 1.9814,\n",
      "        1.0002, 0.1693, 2.1451, 1.2133, 1.1710, 1.7016, 0.1387, 1.0585, 0.9396,\n",
      "        2.3864, 1.0343, 1.4483, 0.1834, 0.1881, 2.3653, 1.7924, 0.2179, 1.8525,\n",
      "        0.1841, 0.1217, 1.2647, 1.6569], dtype=torch.float64)\n",
      "tensor(0.9519, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 15 was 95.1%.\n",
      "current params: tensor([0.1460, 0.1126, 0.1632, 0.1417, 0.1416, 0.1122, 0.1698, 0.1666, 0.1697,\n",
      "        0.1851, 0.1922, 1.3339, 1.5181, 0.1826, 0.1726, 1.7392, 0.7625, 2.0045,\n",
      "        1.0181, 0.1690, 2.1667, 1.2307, 1.1983, 1.7295, 0.1379, 1.0147, 0.9621,\n",
      "        2.4202, 1.0086, 1.4712, 0.1823, 0.1869, 2.3985, 1.8280, 0.1663, 1.8680,\n",
      "        0.1839, 0.1216, 1.2901, 1.6749], dtype=torch.float64)\n",
      "tensor(0.9515, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 16 was 95.1%.\n",
      "current params: tensor([0.1437, 0.1089, 0.1610, 0.1414, 0.1389, 0.1110, 0.1694, 0.1634, 0.1692,\n",
      "        0.1843, 0.1545, 1.3616, 1.5448, 0.1817, 0.1378, 1.7641, 0.7044, 2.0263,\n",
      "        1.0355, 0.1687, 2.1871, 1.2512, 1.2252, 1.7574, 0.1371, 0.9719, 0.9842,\n",
      "        2.4538, 0.9820, 1.4935, 0.1811, 0.1858, 2.4303, 1.8620, 0.1135, 1.8830,\n",
      "        0.1837, 0.1215, 1.3145, 1.6927], dtype=torch.float64)\n",
      "tensor(0.9514, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 17 was 95.0%.\n",
      "current params: tensor([0.1413, 0.1051, 0.1588, 0.1410, 0.1360, 0.1098, 0.1689, 0.1600, 0.1688,\n",
      "        0.1836, 0.1160, 1.3887, 1.5707, 0.1808, 0.1013, 1.7882, 0.6456, 2.0467,\n",
      "        1.0523, 0.1684, 2.2065, 1.2750, 1.2517, 1.7856, 0.1362, 0.9300, 1.0058,\n",
      "        2.4875, 0.9546, 1.5151, 0.1799, 0.1845, 2.4610, 1.8945, 0.0595, 1.8974,\n",
      "        0.1836, 0.1215, 1.3381, 1.7102], dtype=torch.float64)\n",
      "tensor(0.9504, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 18 was 95.0%.\n",
      "current params: tensor([0.1388, 0.1010, 0.1565, 0.1408, 0.1331, 0.1086, 0.1684, 0.1565, 0.1684,\n",
      "        0.1829, 0.0767, 1.4153, 1.5959, 0.1798, 0.0632, 1.8117, 0.5856, 2.0654,\n",
      "        1.0684, 0.1681, 2.2247, 1.3021, 1.2779, 1.8142, 0.1353, 0.8890, 1.0271,\n",
      "        2.5215, 0.9263, 1.5363, 0.1786, 0.1833, 2.4907, 1.9256, 0.1595, 1.9111,\n",
      "        0.1834, 0.1215, 1.3612, 1.7275], dtype=torch.float64)\n",
      "tensor(0.9505, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 19 was 95.3%.\n",
      "current params: tensor([0.1363, 0.0968, 0.1542, 0.1405, 0.1301, 0.1074, 0.1679, 0.1527, 0.1679,\n",
      "        0.1821, 0.1767, 1.4413, 1.6206, 0.1788, 0.1632, 1.8348, 0.5362, 2.0879,\n",
      "        1.0838, 0.1678, 2.2419, 1.3295, 1.3034, 1.8399, 0.1344, 0.8481, 1.0477,\n",
      "        2.5556, 0.8960, 1.5568, 0.1772, 0.1820, 2.5195, 1.9560, 0.1590, 1.9241,\n",
      "        0.1833, 0.1214, 1.3819, 1.7445], dtype=torch.float64)\n",
      "tensor(0.9532, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 20 was 95.4%.\n",
      "current params: tensor([0.1335, 0.1703, 0.1518, 0.1402, 0.1270, 0.1062, 0.1675, 0.1489, 0.1675,\n",
      "        0.1812, 0.1764, 1.4672, 1.6440, 0.1778, 0.1630, 1.8573, 0.4904, 2.1094,\n",
      "        1.1017, 0.1675, 2.2612, 1.3563, 1.3291, 1.8641, 0.1334, 0.8080, 1.0680,\n",
      "        2.5833, 0.8689, 1.5765, 0.1759, 0.1807, 2.5472, 1.9860, 0.1584, 1.9385,\n",
      "        0.1831, 0.1214, 1.4013, 1.7615], dtype=torch.float64)\n",
      "tensor(0.9543, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 21 was 95.4%.\n",
      "current params: tensor([0.1310, 0.1679, 0.1492, 0.1396, 0.1239, 0.1050, 0.1670, 0.1455, 0.1668,\n",
      "        0.1805, 0.1760, 1.4933, 1.6700, 0.1770, 0.1629, 1.8819, 0.4610, 2.1277,\n",
      "        1.1220, 0.1673, 2.2788, 1.3505, 1.3497, 1.8959, 0.1326, 0.7783, 1.0867,\n",
      "        2.6099, 0.8444, 1.5939, 0.1747, 0.1793, 2.5732, 2.0140, 0.1580, 1.9533,\n",
      "        0.1829, 0.1213, 1.4160, 1.7806], dtype=torch.float64)\n",
      "tensor(0.9545, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 22 was 95.4%.\n",
      "current params: tensor([0.1284, 0.1655, 0.1466, 0.1390, 0.1206, 0.1038, 0.1665, 0.1420, 0.1660,\n",
      "        0.1797, 0.1756, 1.5188, 1.6952, 0.1762, 0.1628, 1.9060, 0.4317, 2.1458,\n",
      "        1.1425, 0.1670, 2.2963, 1.3440, 1.3700, 1.9262, 0.1318, 0.7485, 1.1050,\n",
      "        2.6355, 0.8199, 1.6108, 0.1734, 0.1780, 2.5986, 2.0412, 0.1576, 1.9680,\n",
      "        0.1828, 0.1212, 1.4301, 1.7994], dtype=torch.float64)\n",
      "tensor(0.9547, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 23 was 95.4%.\n",
      "current params: tensor([0.1257, 0.1629, 0.1438, 0.1384, 0.1172, 0.1026, 0.1660, 0.1383, 0.1652,\n",
      "        0.1789, 0.1752, 1.5439, 1.7195, 0.1754, 0.1627, 1.9295, 0.4024, 2.1638,\n",
      "        1.1630, 0.1667, 2.3135, 1.3366, 1.3902, 1.9554, 0.1309, 0.7186, 1.1230,\n",
      "        2.6604, 0.7953, 1.6272, 0.1722, 0.1766, 2.6233, 2.0679, 0.1572, 1.9827,\n",
      "        0.1826, 0.1211, 1.4436, 1.8181], dtype=torch.float64)\n",
      "tensor(0.9548, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 24 was 95.4%.\n",
      "current params: tensor([0.1229, 0.1602, 0.1410, 0.1377, 0.1138, 0.1014, 0.1655, 0.1345, 0.1644,\n",
      "        0.1780, 0.1748, 1.5685, 1.7432, 0.1745, 0.1625, 1.9525, 0.3731, 2.1816,\n",
      "        1.1835, 0.1665, 2.3304, 1.3284, 1.4102, 1.9835, 0.1301, 0.6887, 1.1408,\n",
      "        2.6846, 0.7707, 1.6432, 0.1709, 0.1752, 2.6475, 2.0940, 0.1567, 1.9972,\n",
      "        0.1824, 0.1210, 1.4566, 1.8365], dtype=torch.float64)\n",
      "tensor(0.9549, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 25 was 95.5%.\n",
      "current params: tensor([0.1200, 0.1575, 0.1380, 0.1371, 0.1102, 0.1001, 0.1650, 0.1305, 0.1636,\n",
      "        0.1772, 0.1744, 1.5927, 1.7662, 0.1737, 0.1624, 1.9751, 0.3437, 2.1994,\n",
      "        1.2041, 0.1662, 2.3471, 1.3192, 1.4301, 2.0107, 0.1292, 0.6588, 1.1583,\n",
      "        2.7082, 0.7461, 1.6588, 0.1696, 0.1738, 2.6711, 2.1196, 0.1563, 2.0118,\n",
      "        0.1822, 0.1209, 1.4690, 1.8548], dtype=torch.float64)\n",
      "tensor(0.9550, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 26 was 95.4%.\n",
      "current params: tensor([0.1170, 0.1546, 0.1350, 0.1364, 0.1065, 0.0989, 0.1645, 0.1264, 0.1628,\n",
      "        0.1762, 0.1740, 1.6164, 1.7887, 0.1728, 0.1623, 1.9973, 0.3143, 2.2170,\n",
      "        1.2248, 0.1659, 2.3636, 1.3091, 1.4499, 2.0371, 0.1282, 0.6291, 1.1755,\n",
      "        2.7312, 0.7216, 1.6739, 0.1682, 0.1723, 2.6942, 2.1446, 0.1558, 2.0262,\n",
      "        0.1820, 0.1208, 1.4808, 1.8730], dtype=torch.float64)\n",
      "tensor(0.9546, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 27 was 95.5%.\n",
      "current params: tensor([0.1139, 0.1515, 0.1318, 0.1358, 0.1027, 0.1734, 0.1640, 0.1221, 0.1619,\n",
      "        0.1753, 0.1736, 1.6397, 1.8106, 0.1719, 0.1621, 2.0191, 0.2847, 2.2346,\n",
      "        1.2455, 0.1657, 2.3798, 1.2980, 1.4695, 2.0628, 0.1272, 0.5996, 1.1925,\n",
      "        2.7536, 0.6971, 1.6887, 0.1668, 0.1708, 2.7169, 2.1692, 0.1553, 2.0407,\n",
      "        0.1818, 0.1207, 1.4920, 1.8909], dtype=torch.float64)\n",
      "tensor(0.9553, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 28 was 95.4%.\n",
      "current params: tensor([0.1105, 0.1484, 0.1285, 0.1350, 0.0989, 0.1728, 0.1635, 0.1177, 0.1611,\n",
      "        0.1743, 0.1732, 1.6596, 1.8330, 0.1711, 0.1620, 2.0382, 0.2558, 2.2529,\n",
      "        1.2687, 0.1655, 2.3945, 1.2862, 1.4891, 2.0878, 0.1261, 0.5714, 1.2093,\n",
      "        2.7745, 0.6721, 1.7067, 0.1654, 0.1694, 2.7389, 2.1939, 0.1548, 2.0546,\n",
      "        0.1816, 0.1205, 1.5026, 1.9086], dtype=torch.float64)\n",
      "tensor(0.9549, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 29 was 95.5%.\n",
      "current params: tensor([0.1070, 0.1451, 0.1251, 0.1343, 0.1607, 0.1721, 0.1631, 0.1131, 0.1602,\n",
      "        0.1733, 0.1728, 1.6793, 1.8549, 0.1703, 0.1618, 2.0572, 0.2267, 2.2711,\n",
      "        1.2917, 0.1654, 2.4091, 1.2735, 1.5085, 2.1122, 0.1250, 0.5434, 1.2257,\n",
      "        2.7949, 0.6472, 1.7241, 0.1640, 0.1679, 2.7604, 2.2182, 0.1542, 2.0685,\n",
      "        0.1813, 0.1204, 1.5126, 1.9262], dtype=torch.float64)\n",
      "tensor(0.9558, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 30 was 95.5%.\n",
      "current params: tensor([0.1034, 0.1416, 0.1221, 0.1337, 0.1583, 0.1716, 0.1628, 0.1089, 0.1596,\n",
      "        0.1725, 0.1726, 1.7015, 1.8730, 0.1696, 0.1615, 2.0764, 0.1862, 2.2910,\n",
      "        1.3087, 0.1652, 2.4227, 1.2754, 1.5235, 2.1391, 0.1239, 0.5190, 1.2408,\n",
      "        2.8191, 0.5996, 1.7391, 0.1625, 0.1667, 2.7821, 2.2441, 0.1534, 2.0804,\n",
      "        0.1810, 0.1202, 1.5226, 1.9446], dtype=torch.float64)\n",
      "tensor(0.9554, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 31 was 95.4%.\n",
      "current params: tensor([0.0997, 0.1379, 0.1190, 0.1330, 0.1557, 0.1710, 0.1625, 0.1045, 0.1589,\n",
      "        0.1716, 0.1723, 1.7234, 1.8909, 0.1688, 0.1611, 2.0953, 0.1462, 2.3110,\n",
      "        1.3253, 0.1651, 2.4362, 1.2778, 1.5382, 2.1654, 0.1227, 0.4947, 1.2557,\n",
      "        2.8431, 0.5533, 1.7535, 0.1610, 0.1655, 2.8034, 2.2696, 0.1525, 2.0922,\n",
      "        0.1807, 0.1200, 1.5317, 1.9630], dtype=torch.float64)\n",
      "tensor(0.9549, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 32 was 95.3%.\n",
      "current params: tensor([0.1632, 0.1339, 0.1158, 0.1324, 0.1531, 0.1705, 0.1623, 0.0999, 0.1582,\n",
      "        0.1707, 0.1721, 1.7451, 1.9085, 0.1681, 0.1607, 2.1140, 0.1063, 2.3310,\n",
      "        1.3417, 0.1650, 2.4496, 1.2809, 1.5525, 2.1910, 0.1215, 0.4709, 1.2703,\n",
      "        2.8668, 0.5080, 1.7675, 0.1594, 0.1643, 2.8243, 2.2947, 0.1516, 2.1040,\n",
      "        0.1803, 0.1198, 1.5400, 1.9815], dtype=torch.float64)\n",
      "tensor(0.9537, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 33 was 95.0%.\n",
      "current params: tensor([0.1608, 0.1303, 0.1129, 0.1319, 0.1504, 0.1697, 0.1619, 0.1741, 0.1576,\n",
      "        0.1698, 0.1718, 1.7647, 1.9261, 0.1669, 0.1603, 2.1309, 0.0696, 2.3538,\n",
      "        1.3486, 0.1648, 2.4651, 1.2950, 1.5682, 2.2131, 0.1205, 0.4495, 1.2834,\n",
      "        2.8933, 0.4745, 1.7829, 0.1581, 0.1630, 2.8467, 2.3194, 0.1508, 2.1155,\n",
      "        0.1802, 0.1197, 1.5465, 1.9976], dtype=torch.float64)\n",
      "tensor(0.9507, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 34 was 95.2%.\n",
      "current params: tensor([0.1583, 0.1273, 0.1099, 0.1314, 0.1481, 0.1689, 0.1615, 0.1718, 0.1572,\n",
      "        0.1692, 0.1716, 1.7857, 1.9412, 0.1659, 0.1600, 2.1491, 0.1696, 2.3738,\n",
      "        1.3591, 0.1643, 2.4833, 1.3193, 1.5859, 2.2320, 0.1195, 0.4108, 1.2982,\n",
      "        2.9151, 0.4456, 1.8054, 0.1560, 0.1611, 2.8669, 2.3410, 0.1500, 2.1285,\n",
      "        0.1799, 0.1196, 1.5533, 2.0142], dtype=torch.float64)\n",
      "tensor(0.9526, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 35 was 95.2%.\n",
      "current params: tensor([0.1558, 0.1242, 0.1069, 0.1309, 0.1457, 0.1680, 0.1612, 0.1695, 0.1567,\n",
      "        0.1685, 0.1714, 1.8062, 1.9563, 0.1650, 0.1597, 2.1674, 0.1692, 2.3930,\n",
      "        1.3691, 0.1639, 2.5013, 1.3417, 1.6032, 2.2484, 0.1184, 0.3729, 1.3127,\n",
      "        2.9366, 0.4156, 1.8275, 0.1539, 0.1591, 2.8867, 2.3624, 0.1493, 2.1413,\n",
      "        0.1796, 0.1194, 1.5610, 2.0308], dtype=torch.float64)\n",
      "tensor(0.9528, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 36 was 95.2%.\n",
      "current params: tensor([0.1531, 0.1211, 0.1037, 0.1305, 0.1433, 0.1672, 0.1609, 0.1671, 0.1563,\n",
      "        0.1678, 0.1712, 1.8266, 1.9712, 0.1640, 0.1594, 2.1856, 0.1689, 2.4119,\n",
      "        1.3788, 0.1635, 2.5192, 1.3641, 1.6203, 2.2641, 0.1174, 0.3355, 1.3271,\n",
      "        2.9579, 0.3852, 1.8492, 0.1516, 0.1571, 2.9062, 2.3836, 0.1487, 2.1542,\n",
      "        0.1792, 0.1192, 1.5685, 2.0473], dtype=torch.float64)\n",
      "tensor(0.9524, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 37 was 95.1%.\n",
      "current params: tensor([0.1503, 0.1179, 0.1004, 0.1300, 0.1408, 0.1664, 0.1605, 0.1647, 0.1558,\n",
      "        0.1671, 0.1711, 1.8468, 1.9860, 0.1631, 0.1591, 2.2036, 0.1685, 2.4306,\n",
      "        1.3881, 0.1630, 2.5369, 1.3867, 1.6371, 2.2790, 0.1163, 0.2986, 1.3415,\n",
      "        2.9789, 0.3543, 1.8707, 0.1493, 0.1550, 2.9254, 2.4045, 0.1481, 2.1671,\n",
      "        0.1789, 0.1190, 1.5756, 2.0637], dtype=torch.float64)\n",
      "tensor(0.9520, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 38 was 95.2%.\n",
      "current params: tensor([0.1474, 0.1145, 0.0970, 0.1295, 0.1383, 0.1655, 0.1602, 0.1622, 0.1554,\n",
      "        0.1663, 0.1709, 1.8668, 2.0008, 0.1621, 0.1587, 2.2215, 0.1682, 2.4492,\n",
      "        1.3971, 0.1625, 2.5546, 1.4094, 1.6538, 2.2932, 0.1151, 0.2621, 1.3559,\n",
      "        2.9998, 0.3228, 1.8919, 0.1469, 0.1528, 2.9442, 2.4251, 0.1474, 2.1799,\n",
      "        0.1786, 0.1188, 1.5824, 2.0802], dtype=torch.float64)\n",
      "tensor(0.9521, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 39 was 94.7%.\n",
      "current params: tensor([0.1444, 0.1111, 0.1606, 0.1290, 0.1357, 0.1647, 0.1599, 0.1597, 0.1550,\n",
      "        0.1656, 0.1707, 1.8866, 2.0154, 0.1612, 0.1584, 2.2392, 0.1679, 2.4676,\n",
      "        1.4056, 0.1621, 2.5722, 1.4322, 1.6702, 2.3066, 0.1139, 0.2258, 1.3702,\n",
      "        3.0205, 0.2907, 1.9128, 0.1444, 0.1506, 2.9627, 2.4454, 0.1468, 2.1928,\n",
      "        0.1782, 0.1186, 1.5889, 2.0967], dtype=torch.float64)\n",
      "tensor(0.9479, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 40 was 94.6%.\n",
      "current params: tensor([0.1417, 0.1074, 0.1585, 0.1287, 0.1335, 0.1640, 0.1597, 0.1571, 0.1546,\n",
      "        0.1644, 0.1706, 1.9057, 2.0319, 0.1602, 0.1580, 2.2558, 0.1674, 2.4858,\n",
      "        1.4161, 0.1616, 2.5874, 1.4384, 1.6884, 2.3190, 0.1119, 0.1717, 1.3928,\n",
      "        3.0390, 0.2664, 1.9346, 0.1418, 0.1487, 2.9819, 2.4664, 0.1460, 2.2044,\n",
      "        0.1779, 0.1184, 1.5961, 2.1111], dtype=torch.float64)\n",
      "tensor(0.9460, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 41 was 94.3%.\n",
      "current params: tensor([0.1389, 0.1037, 0.1564, 0.1284, 0.1313, 0.1634, 0.1595, 0.1545, 0.1543,\n",
      "        0.1631, 0.1705, 1.9246, 2.0482, 0.1593, 0.1576, 2.2723, 0.1670, 2.5039,\n",
      "        1.4266, 0.1612, 2.6026, 1.4441, 1.7065, 2.3309, 0.1097, 0.1191, 1.4163,\n",
      "        3.0574, 0.2418, 1.9568, 0.1390, 0.1467, 3.0007, 2.4870, 0.1452, 2.2160,\n",
      "        0.1775, 0.1183, 1.6028, 2.1256], dtype=torch.float64)\n",
      "tensor(0.9434, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 42 was 94.0%.\n",
      "current params: tensor([0.1361, 0.0998, 0.1543, 0.1281, 0.1290, 0.1627, 0.1594, 0.1519, 0.1540,\n",
      "        0.1618, 0.1704, 1.9433, 2.0645, 0.1584, 0.1572, 2.2885, 0.1666, 2.5219,\n",
      "        1.4370, 0.1607, 2.6178, 1.4493, 1.7246, 2.3422, 0.1074, 0.0673, 1.4412,\n",
      "        3.0756, 0.2166, 1.9796, 0.1359, 0.1447, 3.0191, 2.5072, 0.1443, 2.2277,\n",
      "        0.1771, 0.1182, 1.6086, 2.1401], dtype=torch.float64)\n",
      "tensor(0.9403, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 43 was 94.9%.\n",
      "current params: tensor([0.1331, 0.1627, 0.1521, 0.1278, 0.1268, 0.1621, 0.1592, 0.1492, 0.1537,\n",
      "        0.1604, 0.1703, 1.9618, 2.0806, 0.1574, 0.1568, 2.3045, 0.1662, 2.5399,\n",
      "        1.4472, 0.1602, 2.6330, 1.4541, 1.7427, 2.3530, 0.1047, 0.1673, 1.4677,\n",
      "        3.0937, 0.1910, 2.0032, 0.1327, 0.1426, 3.0372, 2.5271, 0.1434, 2.2394,\n",
      "        0.1767, 0.1180, 1.6136, 2.1546], dtype=torch.float64)\n",
      "tensor(0.9498, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 44 was 95.0%.\n",
      "current params: tensor([0.1303, 0.1602, 0.1497, 0.1272, 0.1243, 0.1615, 0.1590, 0.1466, 0.1531,\n",
      "        0.1591, 0.1701, 1.9807, 2.0984, 0.1567, 0.1565, 2.3225, 0.1660, 2.5548,\n",
      "        1.4596, 0.1598, 2.6467, 1.4292, 1.7560, 2.3656, 0.1029, 0.1667, 1.4855,\n",
      "        3.1116, 0.1665, 2.0213, 0.1301, 0.1405, 3.0554, 2.5474, 0.1429, 2.2509,\n",
      "        0.1763, 0.1178, 1.6189, 2.1707], dtype=torch.float64)\n",
      "tensor(0.9504, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 45 was 95.0%.\n",
      "current params: tensor([0.1274, 0.1576, 0.1472, 0.1266, 0.1217, 0.1608, 0.1588, 0.1441, 0.1525,\n",
      "        0.1579, 0.1700, 1.9994, 2.1158, 0.1559, 0.1562, 2.3404, 0.1658, 2.5695,\n",
      "        1.4719, 0.1594, 2.6603, 1.4032, 1.7690, 2.3777, 0.1010, 0.1660, 1.5029,\n",
      "        3.1294, 0.1415, 2.0390, 0.1274, 0.1384, 3.0734, 2.5674, 0.1425, 2.2623,\n",
      "        0.1760, 0.1177, 1.6243, 2.1868], dtype=torch.float64)\n",
      "tensor(0.9505, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 46 was 95.0%.\n",
      "current params: tensor([0.1243, 0.1550, 0.1446, 0.1259, 0.1191, 0.1601, 0.1585, 0.1414, 0.1518,\n",
      "        0.1566, 0.1698, 2.0179, 2.1330, 0.1551, 0.1558, 2.3581, 0.1656, 2.5842,\n",
      "        1.4841, 0.1590, 2.6737, 1.3760, 1.7816, 2.3893, 0.0991, 0.1653, 1.5200,\n",
      "        3.1471, 0.1159, 2.0562, 0.1247, 0.1362, 3.0913, 2.5873, 0.1420, 2.2736,\n",
      "        0.1757, 0.1175, 1.6297, 2.2030], dtype=torch.float64)\n",
      "tensor(0.9506, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 47 was 95.3%.\n",
      "current params: tensor([0.1212, 0.1523, 0.1419, 0.1252, 0.1164, 0.1594, 0.1583, 0.1387, 0.1511,\n",
      "        0.1552, 0.1697, 2.0363, 2.1498, 0.1543, 0.1555, 2.3758, 0.1654, 2.5987,\n",
      "        1.4961, 0.1586, 2.6870, 1.3478, 1.7939, 2.4004, 0.1765, 0.1646, 1.5367,\n",
      "        3.1648, 0.0898, 2.0731, 0.1219, 0.1340, 3.1090, 2.6071, 0.1416, 2.2849,\n",
      "        0.1753, 0.1173, 1.6350, 2.2191], dtype=torch.float64)\n",
      "tensor(0.9531, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 48 was 95.1%.\n",
      "current params: tensor([0.1179, 0.1496, 0.1391, 0.1244, 0.1136, 0.1587, 0.1581, 0.1359, 0.1504,\n",
      "        0.1540, 0.1695, 2.0522, 2.1666, 0.1535, 0.1551, 2.3914, 0.1654, 2.6140,\n",
      "        1.5081, 0.1582, 2.7001, 1.3278, 1.8054, 2.4104, 0.1754, 0.1641, 1.5504,\n",
      "        3.1823, 0.1898, 2.0941, 0.1193, 0.1318, 3.1266, 2.6269, 0.1413, 2.2960,\n",
      "        0.1750, 0.1171, 1.6399, 2.2351], dtype=torch.float64)\n",
      "tensor(0.9520, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 49 was 95.1%.\n",
      "current params: tensor([0.1146, 0.1469, 0.1362, 0.1237, 0.1107, 0.1580, 0.1578, 0.1331, 0.1497,\n",
      "        0.1528, 0.1694, 2.0679, 2.1830, 0.1528, 0.1548, 2.4070, 0.1652, 2.6289,\n",
      "        1.5230, 0.1578, 2.7129, 1.3095, 1.8187, 2.4227, 0.1743, 0.1635, 1.5638,\n",
      "        3.1983, 0.1894, 2.1146, 0.1166, 0.1294, 3.1443, 2.6464, 0.1409, 2.3070,\n",
      "        0.1746, 0.1168, 1.6450, 2.2502], dtype=torch.float64)\n",
      "tensor(0.9519, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 50 was 95.1%.\n",
      "current params: tensor([0.1113, 0.1441, 0.1332, 0.1229, 0.1076, 0.1573, 0.1575, 0.1302, 0.1490,\n",
      "        0.1516, 0.1692, 2.0836, 2.1992, 0.1520, 0.1545, 2.4224, 0.1650, 2.6437,\n",
      "        1.5383, 0.1575, 2.7255, 1.2909, 1.8321, 2.4350, 0.1732, 0.1630, 1.5769,\n",
      "        3.2141, 0.1891, 2.1346, 0.1138, 0.1270, 3.1618, 2.6658, 0.1405, 2.3179,\n",
      "        0.1743, 0.1166, 1.6501, 2.2652], dtype=torch.float64)\n",
      "tensor(0.9518, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 51 was 95.1%.\n",
      "current params: tensor([0.1078, 0.1412, 0.1301, 0.1221, 0.1045, 0.1566, 0.1573, 0.1272, 0.1482,\n",
      "        0.1504, 0.1690, 2.0991, 2.2152, 0.1512, 0.1542, 2.4378, 0.1648, 2.6583,\n",
      "        1.5541, 0.1571, 2.7379, 1.2718, 1.8455, 2.4472, 0.1721, 0.1624, 1.5897,\n",
      "        3.2296, 0.1888, 2.1541, 0.1110, 0.1245, 3.1793, 2.6850, 0.1401, 2.3289,\n",
      "        0.1739, 0.1164, 1.6553, 2.2800], dtype=torch.float64)\n",
      "tensor(0.9517, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 52 was 95.1%.\n",
      "current params: tensor([0.1041, 0.1382, 0.1268, 0.1212, 0.1013, 0.1558, 0.1570, 0.1240, 0.1474,\n",
      "        0.1491, 0.1688, 2.1146, 2.2310, 0.1504, 0.1538, 2.4531, 0.1647, 2.6728,\n",
      "        1.5702, 0.1567, 2.7502, 1.2523, 1.8590, 2.4595, 0.1710, 0.1619, 1.6022,\n",
      "        3.2449, 0.1885, 2.1732, 0.1082, 0.1220, 3.1965, 2.7040, 0.1396, 2.3398,\n",
      "        0.1736, 0.1162, 1.6605, 2.2946], dtype=torch.float64)\n",
      "tensor(0.9512, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 53 was 95.1%.\n",
      "current params: tensor([0.1004, 0.1351, 0.1234, 0.1204, 0.0980, 0.1551, 0.1567, 0.1208, 0.1466,\n",
      "        0.1478, 0.1686, 2.1299, 2.2465, 0.1496, 0.1535, 2.4683, 0.1645, 2.6871,\n",
      "        1.5868, 0.1563, 2.7623, 1.2325, 1.8726, 2.4718, 0.1698, 0.1613, 1.6144,\n",
      "        3.2599, 0.1881, 2.1919, 0.1052, 0.1194, 3.2137, 2.7228, 0.1392, 2.3507,\n",
      "        0.1732, 0.1159, 1.6658, 2.3092], dtype=torch.float64)\n",
      "tensor(0.9512, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 54 was 95.1%.\n",
      "current params: tensor([0.0965, 0.1319, 0.1199, 0.1195, 0.1550, 0.1544, 0.1563, 0.1175, 0.1457,\n",
      "        0.1464, 0.1684, 2.1451, 2.2619, 0.1488, 0.1532, 2.4835, 0.1643, 2.7012,\n",
      "        1.6039, 0.1560, 2.7742, 1.2124, 1.8862, 2.4840, 0.1686, 0.1607, 1.6263,\n",
      "        3.2747, 0.1878, 2.2102, 0.1022, 0.1168, 3.2307, 2.7415, 0.1388, 2.3617,\n",
      "        0.1729, 0.1157, 1.6711, 2.3236], dtype=torch.float64)\n",
      "tensor(0.9519, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 55 was 95.2%.\n",
      "current params: tensor([0.1537, 0.1285, 0.1167, 0.1187, 0.1527, 0.1538, 0.1562, 0.1144, 0.1451,\n",
      "        0.1453, 0.1684, 2.1619, 2.2749, 0.1481, 0.1526, 2.4987, 0.1640, 2.7166,\n",
      "        1.6145, 0.1557, 2.7855, 1.2043, 1.8951, 2.4966, 0.1674, 0.1602, 1.6373,\n",
      "        3.2914, 0.1871, 2.2268, 0.0992, 0.1145, 3.2477, 2.7612, 0.1382, 2.3711,\n",
      "        0.1724, 0.1154, 1.6767, 2.3397], dtype=torch.float64)\n",
      "tensor(0.9526, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 56 was 95.5%.\n",
      "current params: tensor([0.1511, 0.1254, 0.1137, 0.1180, 0.1503, 0.1529, 0.1560, 0.1112, 0.1444,\n",
      "        0.1442, 0.1682, 2.1777, 2.2877, 0.1469, 0.1520, 2.5135, 0.1638, 2.7343,\n",
      "        1.6170, 0.1552, 2.7984, 1.2024, 1.9064, 2.5078, 0.1664, 0.1597, 1.6469,\n",
      "        3.3093, 0.1867, 2.2435, 0.1770, 0.1121, 3.2659, 2.7808, 0.1378, 2.3805,\n",
      "        0.1722, 0.1152, 1.6815, 2.3533], dtype=torch.float64)\n",
      "tensor(0.9556, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 57 was 95.5%.\n",
      "current params: tensor([0.1485, 0.1223, 0.1109, 0.1174, 0.1477, 0.1519, 0.1557, 0.1078, 0.1438,\n",
      "        0.1430, 0.1681, 2.1935, 2.3001, 0.1457, 0.1515, 2.5282, 0.1636, 2.7515,\n",
      "        1.6196, 0.1548, 2.8100, 1.2029, 1.9179, 2.5188, 0.1654, 0.1593, 1.6595,\n",
      "        3.3268, 0.1862, 2.2567, 0.1757, 0.1102, 3.2823, 2.7980, 0.1374, 2.3911,\n",
      "        0.1720, 0.1152, 1.6859, 2.3679], dtype=torch.float64)\n",
      "tensor(0.9560, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 58 was 95.5%.\n",
      "current params: tensor([0.1458, 0.1191, 0.1079, 0.1167, 0.1451, 0.1510, 0.1555, 0.1043, 0.1431,\n",
      "        0.1418, 0.1679, 2.2092, 2.3125, 0.1444, 0.1509, 2.5428, 0.1633, 2.7686,\n",
      "        1.6222, 0.1544, 2.8216, 1.2043, 1.9294, 2.5297, 0.1645, 0.1589, 1.6717,\n",
      "        3.3441, 0.1857, 2.2697, 0.1743, 0.1083, 3.2986, 2.8151, 0.1369, 2.4016,\n",
      "        0.1718, 0.1151, 1.6903, 2.3822], dtype=torch.float64)\n",
      "tensor(0.9554, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 59 was 95.5%.\n",
      "current params: tensor([0.1431, 0.1157, 0.1048, 0.1161, 0.1424, 0.1500, 0.1553, 0.1006, 0.1424,\n",
      "        0.1406, 0.1677, 2.2248, 2.3248, 0.1431, 0.1503, 2.5574, 0.1630, 2.7856,\n",
      "        1.6247, 0.1540, 2.8331, 1.2069, 1.9410, 2.5405, 0.1635, 0.1585, 1.6836,\n",
      "        3.3613, 0.1853, 2.2823, 0.1730, 0.1063, 3.3148, 2.8322, 0.1365, 2.4121,\n",
      "        0.1717, 0.1150, 1.6948, 2.3964], dtype=torch.float64)\n",
      "tensor(0.9554, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 60 was 95.5%.\n",
      "current params: tensor([0.1402, 0.1122, 0.1016, 0.1154, 0.1395, 0.1490, 0.1550, 0.0968, 0.1417,\n",
      "        0.1394, 0.1676, 2.2403, 2.3369, 0.1418, 0.1496, 2.5718, 0.1627, 2.8024,\n",
      "        1.6273, 0.1536, 2.8444, 1.2107, 1.9527, 2.5513, 0.1625, 0.1582, 1.6952,\n",
      "        3.3783, 0.1848, 2.2946, 0.1716, 0.1043, 3.3310, 2.8492, 0.1361, 2.4226,\n",
      "        0.1715, 0.1149, 1.6992, 2.4105], dtype=torch.float64)\n",
      "tensor(0.9554, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 61 was 95.4%.\n",
      "current params: tensor([0.1373, 0.1086, 0.0983, 0.1148, 0.1366, 0.1480, 0.1548, 0.1612, 0.1410,\n",
      "        0.1382, 0.1674, 2.2557, 2.3491, 0.1404, 0.1490, 2.5861, 0.1624, 2.8191,\n",
      "        1.6297, 0.1532, 2.8557, 1.2159, 1.9645, 2.5621, 0.1615, 0.1579, 1.7064,\n",
      "        3.3951, 0.1842, 2.3066, 0.1703, 0.1023, 3.3471, 2.8661, 0.1356, 2.4330,\n",
      "        0.1713, 0.1148, 1.7037, 2.4245], dtype=torch.float64)\n",
      "tensor(0.9545, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 62 was 95.2%.\n",
      "current params: tensor([0.1344, 0.1055, 0.1559, 0.1142, 0.1341, 0.1470, 0.1546, 0.1589, 0.1405,\n",
      "        0.1372, 0.1673, 2.2720, 2.3597, 0.1393, 0.1485, 2.6013, 0.1622, 2.8339,\n",
      "        1.6357, 0.1526, 2.8682, 1.2279, 1.9785, 2.5714, 0.1605, 0.1573, 1.7198,\n",
      "        3.4094, 0.1838, 2.3236, 0.1683, 0.0995, 3.3615, 2.8809, 0.1352, 2.4451,\n",
      "        0.1711, 0.1147, 1.7086, 2.4386], dtype=torch.float64)\n",
      "tensor(0.9523, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 63 was 95.4%.\n",
      "current params: tensor([0.1317, 0.1023, 0.1538, 0.1138, 0.1320, 0.1463, 0.1544, 0.1566, 0.1401,\n",
      "        0.1357, 0.1672, 2.2874, 2.3719, 0.1383, 0.1481, 2.6154, 0.1619, 2.8485,\n",
      "        1.6445, 0.1520, 2.8790, 1.2279, 1.9942, 2.5806, 0.1591, 0.1565, 1.7391,\n",
      "        3.4222, 0.1834, 2.3418, 0.1664, 0.1787, 3.3766, 2.8963, 0.1346, 2.4560,\n",
      "        0.1709, 0.1146, 1.7143, 2.4511], dtype=torch.float64)\n",
      "tensor(0.9544, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 64 was 95.4%.\n",
      "current params: tensor([0.1290, 0.0989, 0.1517, 0.1134, 0.1298, 0.1456, 0.1543, 0.1542, 0.1397,\n",
      "        0.1342, 0.1672, 2.3027, 2.3839, 0.1373, 0.1476, 2.6293, 0.1616, 2.8627,\n",
      "        1.6554, 0.1517, 2.8930, 1.2278, 2.0099, 2.5899, 0.1575, 0.1558, 1.7565,\n",
      "        3.4346, 0.1831, 2.3572, 0.1646, 0.1775, 3.3920, 2.9130, 0.1340, 2.4661,\n",
      "        0.1704, 0.1144, 1.7200, 2.4635], dtype=torch.float64)\n",
      "tensor(0.9545, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 65 was 95.6%.\n",
      "current params: tensor([0.1262, 0.1560, 0.1496, 0.1130, 0.1276, 0.1449, 0.1542, 0.1518, 0.1393,\n",
      "        0.1327, 0.1671, 2.3179, 2.3958, 0.1363, 0.1471, 2.6431, 0.1613, 2.8768,\n",
      "        1.6665, 0.1513, 2.9069, 1.2287, 2.0257, 2.5991, 0.1560, 0.1551, 1.7736,\n",
      "        3.4470, 0.1827, 2.3724, 0.1628, 0.1763, 3.4073, 2.9295, 0.1333, 2.4761,\n",
      "        0.1700, 0.1142, 1.7258, 2.4758], dtype=torch.float64)\n",
      "tensor(0.9565, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 66 was 95.6%.\n",
      "current params: tensor([0.1235, 0.1537, 0.1474, 0.1122, 0.1252, 0.1442, 0.1541, 0.1495, 0.1385,\n",
      "        0.1312, 0.1670, 2.3334, 2.4089, 0.1354, 0.1468, 2.6584, 0.1611, 2.8888,\n",
      "        1.6810, 0.1510, 2.9198, 1.2008, 2.0380, 2.6099, 0.1547, 0.1545, 1.7882,\n",
      "        3.4591, 0.1823, 2.3858, 0.1612, 0.1750, 3.4226, 2.9460, 0.1330, 2.4860,\n",
      "        0.1696, 0.1140, 1.7306, 2.4894], dtype=torch.float64)\n",
      "tensor(0.9566, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 67 was 95.6%.\n",
      "current params: tensor([0.1208, 0.1513, 0.1450, 0.1114, 0.1228, 0.1435, 0.1539, 0.1472, 0.1377,\n",
      "        0.1297, 0.1669, 2.3488, 2.4219, 0.1346, 0.1464, 2.6737, 0.1610, 2.9008,\n",
      "        1.6958, 0.1506, 2.9326, 1.1725, 2.0503, 2.6207, 0.1534, 0.1539, 1.8026,\n",
      "        3.4711, 0.1820, 2.3989, 0.1596, 0.1737, 3.4378, 2.9624, 0.1326, 2.4960,\n",
      "        0.1692, 0.1138, 1.7354, 2.5030], dtype=torch.float64)\n",
      "tensor(0.9567, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 68 was 95.6%.\n",
      "current params: tensor([0.1180, 0.1488, 0.1427, 0.1106, 0.1203, 0.1428, 0.1537, 0.1448, 0.1369,\n",
      "        0.1282, 0.1667, 2.3641, 2.4348, 0.1337, 0.1461, 2.6889, 0.1608, 2.9127,\n",
      "        1.7109, 0.1503, 2.9453, 1.1439, 2.0626, 2.6316, 0.1520, 0.1532, 1.8169,\n",
      "        3.4830, 0.1817, 2.4119, 0.1580, 0.1724, 3.4529, 2.9786, 0.1322, 2.5059,\n",
      "        0.1688, 0.1135, 1.7403, 2.5166], dtype=torch.float64)\n",
      "tensor(0.9564, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 69 was 95.6%.\n",
      "current params: tensor([0.1151, 0.1463, 0.1402, 0.1097, 0.1177, 0.1421, 0.1536, 0.1423, 0.1361,\n",
      "        0.1266, 0.1666, 2.3793, 2.4475, 0.1329, 0.1457, 2.7040, 0.1607, 2.9245,\n",
      "        1.7264, 0.1500, 2.9579, 1.1149, 2.0749, 2.6424, 0.1507, 0.1526, 1.8310,\n",
      "        3.4948, 0.1813, 2.4247, 0.1563, 0.1711, 3.4680, 2.9948, 0.1319, 2.5159,\n",
      "        0.1684, 0.1133, 1.7452, 2.5301], dtype=torch.float64)\n",
      "tensor(0.9565, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 70 was 95.6%.\n",
      "current params: tensor([0.1122, 0.1437, 0.1377, 0.1089, 0.1151, 0.1414, 0.1534, 0.1398, 0.1352,\n",
      "        0.1250, 0.1664, 2.3944, 2.4600, 0.1320, 0.1454, 2.7190, 0.1605, 2.9362,\n",
      "        1.7422, 0.1496, 2.9703, 1.0857, 2.0873, 2.6533, 0.1493, 0.1520, 1.8450,\n",
      "        3.5064, 0.1810, 2.4374, 0.1546, 0.1698, 3.4830, 3.0109, 0.1315, 2.5258,\n",
      "        0.1680, 0.1130, 1.7502, 2.5435], dtype=torch.float64)\n",
      "tensor(0.9562, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 71 was 95.5%.\n",
      "current params: tensor([0.1091, 0.1410, 0.1351, 0.1080, 0.1124, 0.1407, 0.1532, 0.1372, 0.1343,\n",
      "        0.1233, 0.1663, 2.4094, 2.4724, 0.1312, 0.1450, 2.7339, 0.1604, 2.9479,\n",
      "        1.7583, 0.1493, 2.9827, 1.0562, 2.0997, 2.6642, 0.1478, 0.1513, 1.8589,\n",
      "        3.5179, 0.1806, 2.4499, 0.1529, 0.1684, 3.4978, 3.0269, 0.1311, 2.5357,\n",
      "        0.1675, 0.1127, 1.7552, 2.5569], dtype=torch.float64)\n",
      "tensor(0.9559, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 72 was 95.5%.\n",
      "current params: tensor([0.1060, 0.1383, 0.1325, 0.1071, 0.1096, 0.1400, 0.1530, 0.1345, 0.1334,\n",
      "        0.1216, 0.1661, 2.4243, 2.4847, 0.1303, 0.1446, 2.7487, 0.1602, 2.9595,\n",
      "        1.7748, 0.1490, 2.9949, 1.0265, 2.1121, 2.6752, 0.1464, 0.1507, 1.8726,\n",
      "        3.5293, 0.1803, 2.4622, 0.1512, 0.1670, 3.5126, 3.0428, 0.1307, 2.5457,\n",
      "        0.1671, 0.1124, 1.7603, 2.5702], dtype=torch.float64)\n",
      "tensor(0.9556, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 73 was 95.5%.\n",
      "current params: tensor([0.1028, 0.1355, 0.1297, 0.1061, 0.1067, 0.1393, 0.1528, 0.1317, 0.1324,\n",
      "        0.1199, 0.1660, 2.4391, 2.4969, 0.1294, 0.1443, 2.7635, 0.1601, 2.9710,\n",
      "        1.7916, 0.1487, 3.0071, 0.9967, 2.1245, 2.6861, 0.1449, 0.1500, 1.8861,\n",
      "        3.5406, 0.1799, 2.4743, 0.1494, 0.1656, 3.5274, 3.0586, 0.1304, 2.5557,\n",
      "        0.1666, 0.1122, 1.7654, 2.5834], dtype=torch.float64)\n",
      "tensor(0.9553, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 74 was 95.5%.\n",
      "current params: tensor([0.0994, 0.1325, 0.1269, 0.1051, 0.1037, 0.1385, 0.1526, 0.1288, 0.1314,\n",
      "        0.1180, 0.1658, 2.4538, 2.5089, 0.1285, 0.1439, 2.7782, 0.1599, 2.9824,\n",
      "        1.8087, 0.1484, 3.0191, 0.9666, 2.1370, 2.6971, 0.1433, 0.1494, 1.8996,\n",
      "        3.5517, 0.1796, 2.4863, 0.1477, 0.1642, 3.5420, 3.0744, 0.1300, 2.5656,\n",
      "        0.1662, 0.1119, 1.7706, 2.5967], dtype=torch.float64)\n",
      "tensor(0.9550, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 75 was 95.4%.\n",
      "current params: tensor([0.1528, 0.1295, 0.1240, 0.1041, 0.1007, 0.1378, 0.1524, 0.1259, 0.1304,\n",
      "        0.1162, 0.1656, 2.4684, 2.5208, 0.1277, 0.1436, 2.7928, 0.1597, 2.9937,\n",
      "        1.8261, 0.1481, 3.0310, 0.9364, 2.1494, 2.7082, 0.1417, 0.1487, 1.9129,\n",
      "        3.5627, 0.1792, 2.4981, 0.1458, 0.1628, 3.5566, 3.0900, 0.1296, 2.5756,\n",
      "        0.1657, 0.1115, 1.7758, 2.6099], dtype=torch.float64)\n",
      "tensor(0.9546, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 76 was 95.4%.\n",
      "current params: tensor([0.1505, 0.1268, 0.1213, 0.1033, 0.0974, 0.1366, 0.1520, 0.1228, 0.1294,\n",
      "        0.1143, 0.1653, 2.4823, 2.5325, 0.1262, 0.1433, 2.8070, 0.1596, 3.0066,\n",
      "        1.8388, 0.1476, 3.0445, 0.9110, 2.1637, 2.7181, 0.1404, 0.1481, 1.9250,\n",
      "        3.5741, 0.1790, 2.5102, 0.1443, 0.1613, 3.5721, 3.1056, 0.1293, 2.5855,\n",
      "        0.1655, 0.1114, 1.7804, 2.6213], dtype=torch.float64)\n",
      "tensor(0.9547, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 77 was 95.6%.\n",
      "current params: tensor([0.1481, 0.1240, 0.1185, 0.1024, 0.1509, 0.1354, 0.1516, 0.1197, 0.1283,\n",
      "        0.1123, 0.1650, 2.4961, 2.5441, 0.1248, 0.1430, 2.8211, 0.1595, 3.0195,\n",
      "        1.8519, 0.1470, 3.0579, 0.8852, 2.1780, 2.7281, 0.1390, 0.1474, 1.9369,\n",
      "        3.5854, 0.1788, 2.5222, 0.1426, 0.1597, 3.5876, 3.1211, 0.1291, 2.5955,\n",
      "        0.1652, 0.1113, 1.7850, 2.6327], dtype=torch.float64)\n",
      "tensor(0.9563, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 78 was 95.6%.\n",
      "current params: tensor([0.1456, 0.1211, 0.1159, 0.1016, 0.1485, 0.1344, 0.1515, 0.1167, 0.1275,\n",
      "        0.1106, 0.1649, 2.5110, 2.5540, 0.1235, 0.1424, 2.8354, 0.1592, 3.0339,\n",
      "        1.8574, 0.1466, 3.0704, 0.8690, 2.1887, 2.7382, 0.1376, 0.1468, 1.9482,\n",
      "        3.5984, 0.1783, 2.5329, 0.1410, 0.1584, 3.6031, 3.1374, 0.1287, 2.6042,\n",
      "        0.1649, 0.1111, 1.7897, 2.6452], dtype=torch.float64)\n",
      "tensor(0.9560, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 79 was 95.5%.\n",
      "current params: tensor([0.1430, 0.1181, 0.1133, 0.1009, 0.1461, 0.1335, 0.1513, 0.1137, 0.1266,\n",
      "        0.1089, 0.1648, 2.5258, 2.5638, 0.1221, 0.1418, 2.8496, 0.1590, 3.0482,\n",
      "        1.8630, 0.1461, 3.0829, 0.8529, 2.1995, 2.7483, 0.1361, 0.1461, 1.9594,\n",
      "        3.6113, 0.1778, 2.5436, 0.1394, 0.1571, 3.6184, 3.1536, 0.1283, 2.6128,\n",
      "        0.1646, 0.1110, 1.7945, 2.6576], dtype=torch.float64)\n",
      "tensor(0.9557, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 80 was 95.5%.\n",
      "current params: tensor([0.1404, 0.1149, 0.1106, 0.1001, 0.1436, 0.1325, 0.1512, 0.1106, 0.1258,\n",
      "        0.1071, 0.1646, 2.5405, 2.5735, 0.1208, 0.1412, 2.8637, 0.1588, 3.0625,\n",
      "        1.8687, 0.1457, 3.0953, 0.8369, 2.2103, 2.7584, 0.1346, 0.1455, 1.9704,\n",
      "        3.6241, 0.1773, 2.5540, 0.1377, 0.1557, 3.6337, 3.1698, 0.1279, 2.6214,\n",
      "        0.1643, 0.1108, 1.7993, 2.6700], dtype=torch.float64)\n",
      "tensor(0.9554, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 81 was 95.5%.\n",
      "current params: tensor([0.1376, 0.1117, 0.1078, 0.0993, 0.1410, 0.1315, 0.1510, 0.1073, 0.1249,\n",
      "        0.1052, 0.1645, 2.5551, 2.5832, 0.1194, 0.1406, 2.8777, 0.1586, 3.0768,\n",
      "        1.8744, 0.1453, 3.1076, 0.8211, 2.2211, 2.7684, 0.1331, 0.1449, 1.9813,\n",
      "        3.6369, 0.1768, 2.5642, 0.1360, 0.1544, 3.6489, 3.1858, 0.1275, 2.6301,\n",
      "        0.1640, 0.1107, 1.8041, 2.6823], dtype=torch.float64)\n",
      "tensor(0.9552, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 82 was 95.4%.\n",
      "current params: tensor([0.1348, 0.1083, 0.1049, 0.1827, 0.1383, 0.1305, 0.1508, 0.1039, 0.1240,\n",
      "        0.1033, 0.1644, 2.5697, 2.5928, 0.1179, 0.1399, 2.8917, 0.1583, 3.0910,\n",
      "        1.8801, 0.1449, 3.1198, 0.8056, 2.2320, 2.7784, 0.1316, 0.1442, 1.9920,\n",
      "        3.6496, 0.1763, 2.5743, 0.1344, 0.1530, 3.6641, 3.2018, 0.1271, 2.6387,\n",
      "        0.1637, 0.1105, 1.8090, 2.6946], dtype=torch.float64)\n",
      "tensor(0.9540, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 83 was 95.3%.\n",
      "current params: tensor([0.1319, 0.1048, 0.1020, 0.1823, 0.1355, 0.1294, 0.1505, 0.1004, 0.1232,\n",
      "        0.1014, 0.1641, 2.5841, 2.6022, 0.1165, 0.1393, 2.9055, 0.1581, 3.1050,\n",
      "        1.8865, 0.1444, 3.1320, 0.7954, 2.2443, 2.7879, 0.1300, 0.1436, 2.0025,\n",
      "        3.6651, 0.1758, 2.5842, 0.1326, 0.1516, 3.6794, 3.2178, 0.1267, 2.6474,\n",
      "        0.1634, 0.1103, 1.8140, 2.7061], dtype=torch.float64)\n",
      "tensor(0.9534, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 84 was 95.2%.\n",
      "current params: tensor([0.1290, 0.1012, 0.0990, 0.1819, 0.1326, 0.1284, 0.1503, 0.0967, 0.1224,\n",
      "        0.0994, 0.1638, 2.5985, 2.6116, 0.1150, 0.1386, 2.9193, 0.1578, 3.1191,\n",
      "        1.8930, 0.1440, 3.1440, 0.7858, 2.2566, 2.7974, 0.1283, 0.1430, 2.0129,\n",
      "        3.6806, 0.1753, 2.5939, 0.1309, 0.1502, 3.6946, 3.2337, 0.1262, 2.6561,\n",
      "        0.1631, 0.1102, 1.8190, 2.7176], dtype=torch.float64)\n",
      "tensor(0.9529, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 85 was 95.0%.\n",
      "current params: tensor([0.1259, 0.0974, 0.1530, 0.1815, 0.1297, 0.1273, 0.1500, 0.1549, 0.1216,\n",
      "        0.1813, 0.1636, 2.6127, 2.6209, 0.1134, 0.1379, 2.9330, 0.1575, 3.1331,\n",
      "        1.8994, 0.1436, 3.1560, 0.7768, 2.2690, 2.8067, 0.1267, 0.1423, 2.0230,\n",
      "        3.6959, 0.1748, 2.6034, 0.1292, 0.1488, 3.7097, 3.2496, 0.1257, 2.6648,\n",
      "        0.1627, 0.1100, 1.8241, 2.7290], dtype=torch.float64)\n",
      "tensor(0.9507, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 86 was 95.2%.\n",
      "current params: tensor([0.1232, 0.1505, 0.1509, 0.1812, 0.1275, 0.1265, 0.1498, 0.1527, 0.1211,\n",
      "        0.1802, 0.1635, 2.6276, 2.6295, 0.1122, 0.1374, 2.9468, 0.1570, 3.1457,\n",
      "        1.9118, 0.1431, 3.1680, 0.7716, 2.2847, 2.8177, 0.1246, 0.1413, 2.0378,\n",
      "        3.7075, 0.1744, 2.6170, 0.1267, 0.1472, 3.7242, 3.2646, 0.1248, 2.6737,\n",
      "        0.1623, 0.1098, 1.8306, 2.7393], dtype=torch.float64)\n",
      "tensor(0.9526, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 87 was 95.2%.\n",
      "current params: tensor([0.1207, 0.1482, 0.1488, 0.1808, 0.1253, 0.1256, 0.1495, 0.1506, 0.1203,\n",
      "        0.1791, 0.1633, 2.6426, 2.6390, 0.1113, 0.1371, 2.9615, 0.1567, 3.1564,\n",
      "        1.9272, 0.1425, 3.1792, 0.7457, 2.2977, 2.8306, 0.1228, 0.1402, 2.0509,\n",
      "        3.7197, 0.1741, 2.6293, 0.1245, 0.1455, 3.7389, 3.2796, 0.1242, 2.6827,\n",
      "        0.1619, 0.1097, 1.8362, 2.7507], dtype=torch.float64)\n",
      "tensor(0.9526, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 88 was 95.2%.\n",
      "current params: tensor([0.1181, 0.1458, 0.1467, 0.1803, 0.1229, 0.1248, 0.1492, 0.1485, 0.1194,\n",
      "        0.1780, 0.1630, 2.6576, 2.6483, 0.1103, 0.1367, 2.9762, 0.1565, 3.1670,\n",
      "        1.9429, 0.1420, 3.1904, 0.7195, 2.3107, 2.8436, 0.1210, 0.1392, 2.0638,\n",
      "        3.7319, 0.1738, 2.6414, 0.1222, 0.1437, 3.7534, 3.2946, 0.1235, 2.6917,\n",
      "        0.1615, 0.1095, 1.8418, 2.7620], dtype=torch.float64)\n",
      "tensor(0.9521, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 89 was 95.2%.\n",
      "current params: tensor([0.1155, 0.1434, 0.1445, 0.1798, 0.1206, 0.1239, 0.1488, 0.1464, 0.1185,\n",
      "        0.1769, 0.1628, 2.6724, 2.6576, 0.1093, 0.1364, 2.9909, 0.1562, 3.1777,\n",
      "        1.9588, 0.1415, 3.2015, 0.6931, 2.3238, 2.8566, 0.1192, 0.1382, 2.0767,\n",
      "        3.7439, 0.1735, 2.6535, 0.1200, 0.1420, 3.7679, 3.3095, 0.1229, 2.7006,\n",
      "        0.1611, 0.1093, 1.8474, 2.7734], dtype=torch.float64)\n",
      "tensor(0.9520, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 90 was 95.1%.\n",
      "current params: tensor([0.1127, 0.1409, 0.1422, 0.1793, 0.1181, 0.1231, 0.1485, 0.1442, 0.1176,\n",
      "        0.1757, 0.1626, 2.6871, 2.6667, 0.1083, 0.1361, 3.0055, 0.1559, 3.1882,\n",
      "        1.9748, 0.1410, 3.2125, 0.6665, 2.3368, 2.8697, 0.1173, 0.1371, 2.0894,\n",
      "        3.7559, 0.1732, 2.6655, 0.1176, 0.1402, 3.7824, 3.3244, 0.1223, 2.7096,\n",
      "        0.1607, 0.1092, 1.8531, 2.7847], dtype=torch.float64)\n",
      "tensor(0.9515, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 91 was 95.1%.\n",
      "current params: tensor([0.1099, 0.1384, 0.1399, 0.1789, 0.1156, 0.1222, 0.1482, 0.1419, 0.1166,\n",
      "        0.1745, 0.1623, 2.7018, 2.6758, 0.1072, 0.1357, 3.0200, 0.1557, 3.1987,\n",
      "        1.9911, 0.1404, 3.2235, 0.6397, 2.3499, 2.8828, 0.1153, 0.1360, 2.1021,\n",
      "        3.7677, 0.1729, 2.6773, 0.1152, 0.1383, 3.7967, 3.3392, 0.1216, 2.7185,\n",
      "        0.1603, 0.1090, 1.8589, 2.7959], dtype=torch.float64)\n",
      "tensor(0.9510, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 92 was 95.0%.\n",
      "current params: tensor([0.1071, 0.1357, 0.1375, 0.1783, 0.1130, 0.1214, 0.1478, 0.1395, 0.1157,\n",
      "        0.1733, 0.1620, 2.7165, 2.6847, 0.1062, 0.1354, 3.0344, 0.1554, 3.2092,\n",
      "        2.0076, 0.1399, 3.2344, 0.6128, 2.3630, 2.8959, 0.1133, 0.1349, 2.1146,\n",
      "        3.7795, 0.1726, 2.6891, 0.1128, 0.1365, 3.8111, 3.3540, 0.1209, 2.7275,\n",
      "        0.1599, 0.1088, 1.8648, 2.8072], dtype=torch.float64)\n",
      "tensor(0.9506, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 93 was 95.0%.\n",
      "current params: tensor([0.1041, 0.1330, 0.1351, 0.1778, 0.1104, 0.1205, 0.1474, 0.1371, 0.1147,\n",
      "        0.1720, 0.1618, 2.7310, 2.6935, 0.1052, 0.1351, 3.0488, 0.1551, 3.2196,\n",
      "        2.0243, 0.1394, 3.2453, 0.5857, 2.3762, 2.9092, 0.1112, 0.1337, 2.1271,\n",
      "        3.7912, 0.1723, 2.7008, 0.1103, 0.1345, 3.8253, 3.3687, 0.1203, 2.7365,\n",
      "        0.1595, 0.1086, 1.8707, 2.8184], dtype=torch.float64)\n",
      "tensor(0.9501, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 94 was 94.9%.\n",
      "current params: tensor([0.1010, 0.1303, 0.1325, 0.1773, 0.1077, 0.1196, 0.1471, 0.1347, 0.1137,\n",
      "        0.1707, 0.1615, 2.7455, 2.7022, 0.1042, 0.1347, 3.0632, 0.1548, 3.2300,\n",
      "        2.0412, 0.1388, 3.2560, 0.5585, 2.3893, 2.9224, 0.1091, 0.1325, 2.1394,\n",
      "        3.8028, 0.1720, 2.7124, 0.1077, 0.1326, 3.8395, 3.3834, 0.1196, 2.7455,\n",
      "        0.1590, 0.1084, 1.8767, 2.8295], dtype=torch.float64)\n",
      "tensor(0.9497, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 95 was 94.9%.\n",
      "current params: tensor([0.0979, 0.1274, 0.1299, 0.1767, 0.1049, 0.1187, 0.1467, 0.1321, 0.1126,\n",
      "        0.1694, 0.1612, 2.7599, 2.7108, 0.1031, 0.1344, 3.0775, 0.1545, 3.2403,\n",
      "        2.0584, 0.1383, 3.2668, 0.5312, 2.4025, 2.9358, 0.1069, 0.1313, 2.1517,\n",
      "        3.8143, 0.1717, 2.7239, 0.1051, 0.1306, 3.8537, 3.3981, 0.1189, 2.7544,\n",
      "        0.1586, 0.1082, 1.8827, 2.8407], dtype=torch.float64)\n",
      "tensor(0.9493, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 96 was 94.8%.\n",
      "current params: tensor([0.1486, 0.1245, 0.1272, 0.1762, 0.1020, 0.1178, 0.1463, 0.1295, 0.1116,\n",
      "        0.1680, 0.1609, 2.7743, 2.7193, 0.1021, 0.1341, 3.0917, 0.1542, 3.2505,\n",
      "        2.0757, 0.1378, 3.2774, 0.5037, 2.4158, 2.9492, 0.1046, 0.1301, 2.1639,\n",
      "        3.8258, 0.1713, 2.7353, 0.1024, 0.1285, 3.8678, 3.4127, 0.1182, 2.7634,\n",
      "        0.1581, 0.1079, 1.8889, 2.8518], dtype=torch.float64)\n",
      "tensor(0.9486, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 97 was 94.8%.\n",
      "current params: tensor([0.1463, 0.1218, 0.1248, 0.1757, 0.0989, 0.1164, 0.1457, 0.1268, 0.1105,\n",
      "        0.1666, 0.1605, 2.7881, 2.7277, 0.1004, 0.1338, 3.1057, 0.1539, 3.2622,\n",
      "        2.0889, 0.1369, 3.2893, 0.4798, 2.4303, 2.9617, 0.1027, 0.1289, 2.1747,\n",
      "        3.8369, 0.1711, 2.7469, 0.1000, 0.1264, 3.8826, 3.4273, 0.1176, 2.7726,\n",
      "        0.1579, 0.1079, 1.8944, 2.8614], dtype=torch.float64)\n",
      "tensor(0.9481, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 98 was 94.9%.\n",
      "current params: tensor([0.1440, 0.1190, 0.1222, 0.1752, 0.1498, 0.1149, 0.1452, 0.1240, 0.1093,\n",
      "        0.1652, 0.1600, 2.8020, 2.7361, 0.0987, 0.1335, 3.1196, 0.1536, 3.2738,\n",
      "        2.1022, 0.1359, 3.3011, 0.4556, 2.4449, 2.9742, 0.1007, 0.1276, 2.1855,\n",
      "        3.8479, 0.1709, 2.7584, 0.0976, 0.1241, 3.8974, 3.4418, 0.1170, 2.7817,\n",
      "        0.1577, 0.1079, 1.9000, 2.8710], dtype=torch.float64)\n",
      "tensor(0.9499, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "Using CPU\n",
      "Yield on sim. iteration 99 was 95.1%.\n",
      "optimization complete\n",
      "Final params: tensor([0.1440, 0.1190, 0.1222, 0.1752, 0.1498, 0.1149, 0.1452, 0.1240, 0.1093,\n",
      "        0.1652, 0.1600, 2.8020, 2.7361, 0.0987, 0.1335, 3.1196, 0.1536, 3.2738,\n",
      "        2.1022, 0.1359, 3.3011, 0.4556, 2.4449, 2.9742, 0.1007, 0.1276, 2.1855,\n",
      "        3.8479, 0.1709, 2.7584, 0.0976, 0.1241, 3.8974, 3.4418, 0.1170, 2.7817,\n",
      "        0.1577, 0.1079, 1.9000, 2.8710], dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<KineticAssembly_AD.vectorized_rxn_net.VectorizedRxnNet at 0x1698dccb278>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optim.rn.update_reaction_net(rn)\n",
    "optim.optimize(conc_scale=1e-1,conc_thresh=1e-1,mod_bool=True,mod_factor=10,max_thresh=1e8,max_yield=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = time_mod.perf_counter()\n",
    "print(\"Time taken for complete analysis: %.4f\" %(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track the yield over optim iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optim.plot_yield()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all the parameter values\n",
    "\n",
    "### This can be stored in a file for later analysis or used to find the best parameter value depending upon a condition. For e.g. the values that give a minimum error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yields= []\n",
    "final_params=[]\n",
    "\n",
    "final_t50 = []\n",
    "final_t85 = []\n",
    "final_t95 = []\n",
    "final_t99 = []\n",
    "\n",
    "for i in range(len(optim.final_yields)):\n",
    "    yields.append(optim.final_yields[i])\n",
    "    final_params.append(optim.final_solns[i])\n",
    "    \n",
    "    #Storing the different time points it reaches a particular yield threshold\n",
    "    if optim.final_t85[i] == -1:\n",
    "        final_t85.append(1) \n",
    "    else:\n",
    "        final_t85.append(optim.final_t85[i]) \n",
    "    if optim.final_t95[i] == -1:\n",
    "        final_t95.append(1)\n",
    "    else:\n",
    "        final_t95.append(optim.final_t95[i])\n",
    "\n",
    "\n",
    "final_yield_arr = np.array(yields)\n",
    "final_param_arr = np.array(final_params)\n",
    "final_t85 = np.array(final_t85)\n",
    "final_t95 = np.array(final_t95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the ratio of ktri vs kdim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mask_r = final_yield_arr > 0.1\n",
    "\n",
    "#Calculate the ratio\n",
    "ratio = final_param_arr[:,5]/final_param_arr[:,0]\n",
    "\n",
    "#Normalize the time scale (t = t*conc*max_rate)\n",
    "conc=vec_rn.initial_copies[0].item()\n",
    "scale_time = final_t95[mask_r]*conc*np.max(final_param_arr[mask_r],axis=1)\n",
    "#Calculate the y_per_time\n",
    "y_per_time = 0.95/scale_time\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "ax.plot(ratio,y_per_time,linestyle='',marker='o')\n",
    "ax.set_ylabel(\"Efficiency\",fontsize=20)\n",
    "ax.set_xlabel(\"Ratio\",fontsize=20)\n",
    "ax.tick_params(labelsize=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_indx = np.argmax(y_per_time)\n",
    "max_ratio = ratio[max_indx]\n",
    "max_rates = final_param_arr[max_indx]\n",
    "print(\"Ratio with maximum efficiency: \",max_ratio)\n",
    "\n",
    "reaction_rates = np.zeros(rn._rxn_count)\n",
    "counter=0\n",
    "for cls,uids in vec_rn.rxn_class.items():\n",
    "    for rid in uids:\n",
    "        reaction_rates[rid]=max_rates[counter]\n",
    "    counter+=1\n",
    "\n",
    "print(\"Optimal Rates: \",list(reaction_rates))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
